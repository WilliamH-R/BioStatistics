---
output: html_document
editor_options: 
  chunk_output_type: inline

params:
  percentage_train: 0.80
  n_folds: 10
  grid_size: 100
  penalty_lower: -10
  penalty_higher: 1
---
Set seed and load packages.
```{r}
#| output: FALSE
#| code-fold: true
#| code-summary: "Show the code"

set.seed(1337)

library("tidymodels")
tidymodels::tidymodels_prefer()
```

Load data.
```{r}
#| output: FALSE
#| code-fold: true
#| code-summary: "Show the code"

count_matrix_clr <- readr::read_rds("https://github.com/WilliamH-R/BioStatistics/raw/main/data/count_matrix/count_matrix_clr.rds") |> 
  select(-"NA")

meta <- read.csv(file = "data/metadata.txt") |> 
  as_tibble() |>
  select(Run, chem_administration, ETHNICITY, geo_loc_name,
         Host_age, host_body_mass_index, Host_disease, host_phenotype, host_sex) |> 
  rename(Sample = Run,
         Treatment = chem_administration,
         Ethnicity = ETHNICITY,
         Location = geo_loc_name,
         Age = Host_age,
         BMI = host_body_mass_index,
         Disease_severity = Host_disease,
         EDSS = host_phenotype,
         Sex = host_sex) |>
  mutate(Patient_status = case_when(Disease_severity == "1HealthyControl" ~ "Healthy",
                                    TRUE ~ "MS"),
         EDSS = as.factor(EDSS),
         EDSS = case_when(is.na(EDSS) & Disease_severity == "1HealthyControl" ~ "-1",
                          is.na(EDSS) & Disease_severity != "1HealthyControl" ~ "Unknown",
                          TRUE ~ EDSS),
         EDSS = as.factor(EDSS))
```

# Modelling

## Data wrangling
The data is joined with the metadata as the `Patient_status` is used as the outcome variable. Only the columns that are needed for the model are kept. Notice `inner_join()` is used such that only rows for which both the count matrix and metadata are available are kept.

```{r}
count_matrix_clr <- count_matrix_clr |>
  inner_join(meta,
             by = "Sample") |>
  select(-c(Sample, Treatment, Ethnicity, Location,
            Age, BMI, Disease_severity, EDSS, Sex)) |>
  relocate(Patient_status)
```


## Cross-Validation
The data is split into a training and testing set using cross-validation to avoid overfitting. This is especially needed, as some hyperparameters needs to be tuned.

The data is split:
```{r}
count_matrix_clr_split <- initial_split(count_matrix_clr,
                                        prop = params$percentage_train,
                                        strata = Patient_status)
count_matrix_clr_train <- training(count_matrix_clr_split)
count_matrix_clr_test <- testing(count_matrix_clr_split)
```

The CV object is created:
```{r}
count_matrix_clr_folds <- vfold_cv(count_matrix_clr_train,
                                   v = params$n_folds)
count_matrix_clr_folds
```

## Recipes
The recipes contain two pieces of important information. The first is the formula that describes the relationship between the variables and the outcome. In this case, which genera to use for predicting disease. The second is the pre-processing steps that are applied to the data before the model is built. This could include steps such as scaling, log-transform or imputing missing values. Here, the data is already clr-transformed.

Two recipes are created. One without any pre-processing steps, and one where Principal Component Analysis (PCA) is applied to the data. In both cases, all variables is used to try and predict the outcome `Patient_status`.

```{r}
stand_recipe <- recipe(Patient_status ~ .,
                       data = count_matrix_clr_train)

pca_recipe <- stand_recipe |> 
  step_pca(all_predictors(),
           num_comp = 7)

pls_recipe <- stand_recipe |>
  step_pls(all_predictors(),
           outcome = "Patient_status",
           num_comp = 7)
```

## Model Specifications
To model the data, a specification is needed. This includes the model to use, the engine to use, the mode of prediction and choosing hyperparameters. Since the outcome is binary, a logistic regression model is used. The model is later tuned using a grid search over the hyperparameters `penalty` and `mixture`. Models with exclusively Ridge and Lasso regularization are built, as well as a model with a mixture of both.

```{r}
svm_spec <- svm_rbf(cost = tune(),
                    rbf_sigma = tune(),
                    margin = tune()) |>
  set_engine("kernlab") |>
  set_mode("classification")
```

## Change Hyperparameter Values
The hyperparameters `penalty` and `mixture` are extracted from the model specification. This is done to see the possible values that can be used in the grid search.

For the mixture, values between $0.05$ and $1$ is used which seem to be decent values.

```{r}
# log_spec |> 
#   extract_parameter_set_dials() |> 
#   extract_parameter_dials("mixture")
```

For the penalty, values between $10^-10$ and $1$ are used pr default. The range is increased to $5*10^1$. The updated range is saved in a new object, but the information is not added to the models until the workflow is created further down. As seen from the output, the information do not pertain to a specific model, but simply contain ranges for different hyperparameters.

```{r}
svm_param_ranges <- svm_spec
# log_param_ranges <- log_spec |> 
#   extract_parameter_set_dials() |>
#   update(penalty = penalty(c(params$penalty_lower,
#                              params$penalty_higher)))
# 
# log_penalty_param_ranges <- log_lasso_spec |> 
#   extract_parameter_set_dials() |>
#   update(penalty = penalty(c(params$penalty_lower,
#                              params$penalty_higher)))
# 
# log_param_ranges
# 
# log_penalty_param_ranges
```

## Create the Workflow Set
The recipes are combined with the model specifications to create a workflow set. The workflow set is used to fit the models to the data and evaluate the models. Notice here it is called a workflow set, as it contains multiple workflows. Each row of the workflow set is a workflow of its own which further contain the recipe and model.

A workflow set for all combinations of recipes and models is created:
  
```{r}
workflow_set <- workflow_set(
  preproc = list(stand = stand_recipe,
                 pca = pca_recipe,
                 pls = pls_recipe),
  models = list(svm = svm_spec)
)

workflow_set
```

The parameter objects which contain the hyperparameter ranges are added to the workflow:
  
```{r}
# workflow_set <- workflow_set |>
#   option_add(id = "stand_svm",
#              param_info = svm_param_ranges) |>
#   option_add(id = "pca_svm",
#              param_info = svm_param_ranges)
# workflow_set
```

## Tune hyperparameters

All workflows in the workflow set contain hyperparameters. These are tuned with a search grid using the function `tune_grid()`. As `workflow_set` contain multiple workflows, the `tune_grid()` function can be mapped over the workflows to tune all hyperparameters using `workflow_map()`, a `purrr`-like map function.

Several settings for tuning the grid exists. This includes e.g. whether or not to parallelize, what output to save and how verbose the output should be, i.e. how much should be printed to std.out. The settings are set to:
  
```{r}
grid_settings <-
  control_grid(
    save_pred = TRUE,
    parallel_over = "everything",
    save_workflow = TRUE,
    extract = function(x) x
  )
```

The grid search is performed for a grid of size 10. This means that 10 different combinations of the hyperparameters are tried for each model.

```{r}
grid_results <- workflow_set |>
  workflow_map(
    fn = "tune_grid",
    seed = 1337,
    resamples = count_matrix_clr_folds,
    grid = params$grid_size,
    control = grid_settings
  )
```

`tidymodels` comes with a lot of convenience functions. One of these is `autoplot()` that can, among other things, plot the results of the grid search. The best result of each workflow is selected based on the AUC.

```{r}
autoplot(grid_results,
         id = "pls_svm",
         metric = "roc_auc")
```

The issue with wrappers, e.g. the convenience function `autoplot()`, is that they do not always provide the flexibility needed.

```{r}
grid_results |>
  collect_metrics() |>
  filter(.metric == "roc_auc") |> 
  group_by(wflow_id) |>
  arrange(desc(mean)) |>
  mutate(rank = row_number()) |>
  ungroup() |>
  ggplot(aes(x = rank,
             y = mean,
             col = wflow_id)) +
  geom_point() +
  geom_errorbar(aes(ymin = mean - std_err,
                    ymax = mean + std_err)) +
  theme(legend.position = "none") +
  labs(x = "Rank",
       y = "AUC") +
  facet_wrap(~ wflow_id,
             ncol = 2, scales = "free_y")
```

The hyperparameters resulting in the best AUC is extracted for each workflow. Note that the value of mixture is an order of magnitude different for `stand_log` and `pca_log`. In the PCA space, the model emphasizes Lasso over Ridge. This is likely due to the PCA reducing the number of variables, which in turn seems to reduce the need for setting coefficients to 0 (Lasso regularization).


Note that the value of mixture is close to zero for both Elastic Net models, which means that the model emphasizes Ridge over Lasso. Note also that the penalty is much lower for the workflow with PCA pre-processing. This is likely due to the PCA reducing the number of variables, which in turn seems to reduce the need for regularization.

```{r}
workflow_ids <- grid_results |>
  pull(wflow_id)


tuning_params_result <- workflow_ids |>
  purrr::map(~{grid_results |>
      extract_workflow_set_result(.x) |>
      select_best(metric = "roc_auc") |> 
      mutate(wflow_id = .x)
  }) |>
  bind_rows() |>
  select(wflow_id, cost, rbf_sigma, margin)

tuning_params_result
```

## Finalize workflow
As the hyperparameters have been tuned, the final workflow can be created. This is done by adding the hyperparameters to the workflow. When the hyperparameters have been added, the workflows can be fitted.

```{r}
workflow_set_final <- workflow_ids |>
  purrr::map(~{
    grid_results |>
      extract_workflow(.x) |> 
      finalize_workflow(tuning_params_result |> 
                          filter(wflow_id == .x))
  })
names(workflow_set_final) <- workflow_ids
```

## Fit and Predict for the Final Workflow
With the hyperparameters set, the workflows can be fit to the data, and the models can be evaluated. The `last_fit()` function is used to fit the model to the data. The `split` argument is set to the cross-validation object created earlier. This is done to avoid overfitting.

```{r}
workflow_set_fit <- workflow_ids |>
  purrr::map(~{
    workflow_set_final[[.x]] |>
      last_fit(split = count_matrix_clr_split)
  })
names(workflow_set_fit) <- workflow_ids
```

## Predict and visualize performance
For each of the fitted models, the predictions are collected and the performance is evaluated. One option is to look at the confusion matrices, e.g.: 
  
```{r}
workflow_set_fit$stand_svm |>
  collect_predictions()|> 
  conf_mat(truth = Patient_status,
           estimate = .pred_class)
```

Another more visual approach is to plot the ROC-curve. This is done for all models in the workflow set. First, the specificity and sensitivity is calculated for each model with a convenience function `roc_curve()`

```{r}
roc_auc_results <- workflow_ids |>
  map( ~ {
    workflow_set_fit[[.x]] |>
      collect_predictions() |>
      roc_curve(truth = Patient_status,
                .pred_MS,
                event_level = "second") |>
      mutate(wflow_id = .x)
  }) |> bind_rows()
```

Finally, the ROC-curve is plotted stratified by the workflow ID. The ROC-curve is a plot of the sensitivity against (1 - specificity). Sensitivity is the proportion of true positives out of all positives, i.e. true positive rate. In this case, the proportion of correctly classified MS patients out of all MS patients. Specificity is the proportion of true negatives out of all negatives. In this case, the proportion of correctly classified healthy patients out of all healthy patients. Since the x-axis plots (1 - specificity), it is actually the proportion of false positives out of all negatives, i.e. the false positive rate.

```{r}
roc_auc_results |>
  ggplot(aes(x = 1 - specificity,
             y = sensitivity,
             col = wflow_id)) + 
  geom_path(lwd = 1,
            alpha = 0.6) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  scale_color_viridis_d()
```
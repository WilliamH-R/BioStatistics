Set seed and load packages.
```{r}
#| output: FALSE
#| code-fold: true
#| code-summary: "Show the code"

set.seed(1337)

library("tidymodels")
tidymodels::tidymodels_prefer()
library("plotly")
```

Load data.
```{r}
#| output: FALSE
#| code-fold: true
#| code-summary: "Show the code"

count_matrix_clr <- readr::read_rds("https://github.com/WilliamH-R/BioStatistics/raw/main/data/count_matrix/count_matrix_clr.rds") |> 
  select(-"NA")

meta <- read.csv(file = "data/metadata.txt") |> 
  as_tibble() |>
  select(Run, chem_administration, ETHNICITY, geo_loc_name,
         Host_age, host_body_mass_index, Host_disease, host_phenotype, host_sex) |> 
  rename(Sample = Run,
         Treatment = chem_administration,
         Ethnicity = ETHNICITY,
         Location = geo_loc_name,
         Age = Host_age,
         BMI = host_body_mass_index,
         Disease_severity = Host_disease,
         EDSS = host_phenotype,
         Sex = host_sex) |>
  mutate(Patient_status = case_when(Disease_severity == "1HealthyControl" ~ "Healthy",
                                    TRUE ~ "MS"),
         EDSS = as.factor(EDSS),
         EDSS = case_when(is.na(EDSS) & Disease_severity == "1HealthyControl" ~ "-1",
                          is.na(EDSS) & Disease_severity != "1HealthyControl" ~ "Unknown",
                          TRUE ~ EDSS),
         EDSS = as.factor(EDSS))
```

# Principal Component Analysis

When having more than 3 variables, it is not possible to visualize them in a single plot. To overcome this, Principal Component Analysis (PCA) can reduce the dimensionality of the data to lower dimensions (e.g. 2 or 3) while preserving the variance of the data. Each Principal Component (PC) is a linear combination of the original variables, and can now be considered as a variable on its own replacing the originals. A data set with many variables, e.g. the `r ncol(count_matrix_clr)` variables in the count matrix, can be visualised with a 2D plot. Loosing that many dimensions is almost bound to loose some information, but a PCA optimizes towards retaining as much as possible.

To exemplify and showcasing the method and the underlying math, we will use the `mtcars` data set as introduced in Chapter 7. For this example, two dimensions `wt` (weight) and `mpg` (miles pr. gallon) are used. The points are coloured by `cyl` (cylinders) to show how the data is grouped in the 2D space. The goal is to retain the information, but reduce the dimensions to a single line that best describes the data. This line is called the first Principal Component (PC1).

```{r}
mtcars |> 
  ggplot(aes(x = wt,
             y = mpg,
             col = cyl)) +
  geom_point(size = 3) +
  labs(title = "Miles pr Gallon vs Weight",
       x = "Weight (1000 lbs)",
       y = "Miles pr Gallon",
       colour = "Cylinders") +
  scale_colour_viridis_c()
```

The data is centered, i.e. the mean of each variable is subtracted from each observation within that variable.

$$
X_{i,j \;centered} = X_{i,j} - \mu_{j}
$$

Where `X` is the data matrix, `i` is the observation and `j` is the variable. By subtracting the mean, all data observations are centered around the origin. This makes some mathematical operations easier, such as rotating the coordinate system. Centering the data do not affect the correlation between observations. The resulting plot is then:

```{r}
mtcars |> 
  ggplot(aes(x = wt - mean(wt),
             y = mpg - mean(mpg),
             col = cyl - mean(cyl))) +
  geom_point(size = 3) +
  labs(title = "Miles pr Gallon vs Weight",
       x = "Weight (1000 lbs)",
       y = "Miles pr Gallon",
       colour = "Cylinders") +
  scale_colour_viridis_c()
```

A line is to be fit to the data. This line should maximize the variance of the data, i.e. maximize the sum of squared distances from the origin to the points projected on to the line.

Stuff to do:

- Describe what it does in words. The data is centered, meaning the mean of each variable is subtracted from the data. This makes calculations easier, as we now need to fit an OLS-like model to the data. The line has to go through the origin, as the data is centered. It is not OLS, but it is fitting a straight line. For OLS, the criterion is to minimize the sum of squared residuals. For PCA, the criterion is to maximize the variance of the data, i.e. maximize the sum of squared distances. The distances in this case, is from the origin to the positions on the line where the poins are projected on to. The line which maximizes the sum of squared distances, and thereby also the variance, is PC1.


- The line that maximizes the sum of squared distances can be described by a linear combination of the original variables, i.e. how many steps a long each axis we need to take to reach the line.

- A 1 unit long vector that describes the best fit line, PC1, is called the singular vector or eigenvector. The values of the eigenvector are called the loadings. The loadings are the weights of the original variables in the linear combination that describes the line.

- The average of the sum of squared distances is the eigenvalue

- The square root of the sum og squared distances is the singular value

- Finding PC2: The line that is orthogonal to PC1 and goes through the origin. 

- After finding all PCs, rotate such that PC1 is horisontal, i.e. is the x-axis. Project each point on to the new axes.

- The eigenvalues are the variance explained, and the sum of the eigenvalues is the total variance. The proportion of the variance explained by each PC is the eigenvalue divided by the total variance. The plot that shows the proportion of the variance explained by each PC is called a scree plot.

- DESCRIBE THE MATH.

- Plot a simple 2D plot, e.g. something from mtcars where you can colour by something that makes some easy to spot clusters. Then apply PCA to the data and plot the PC1

- Do PCA for the full count matrix and plot the first two PCs. Colour by the different variables in the metadata.

The simple math: https://www.youtube.com/watch?v=S51bTyIwxFs

Set seed and load packages.
```{r}
#| output: false
#| code-fold: true
#| code-summary: "Show the code"

set.seed(1337)

library("tidymodels")
tidymodels::tidymodels_prefer()
library("vegan")
```

Load data.
```{r}
#| output: false
#| code-fold: true
#| code-summary: "Show the code"

count_matrix <- readr::read_rds("https://github.com/WilliamH-R/BioStatistics/raw/main/data/count_matrix/count_matrix.rds") |> 
  select(-"NA")

meta <- read.csv(file = "data/metadata.txt") |> 
  as_tibble() |>
  select(Run, chem_administration, ETHNICITY, geo_loc_name,
         Host_age, host_body_mass_index, Host_disease, host_phenotype, host_sex) |> 
  rename(Sample = Run,
         Treatment = chem_administration,
         Ethnicity = ETHNICITY,
         Location = geo_loc_name,
         Age = Host_age,
         BMI = host_body_mass_index,
         Disease_severity = Host_disease,
         EDSS = host_phenotype,
         Sex = host_sex) |>
  mutate(Patient_status = case_when(Disease_severity == "1HealthyControl" ~ "Healthy",
                                    TRUE ~ "MS"),
         EDSS = as.factor(EDSS),
         EDSS = case_when(is.na(EDSS) & Disease_severity == "1HealthyControl" ~ "-1",
                          is.na(EDSS) & Disease_severity != "1HealthyControl" ~ "Unknown",
                          TRUE ~ EDSS),
         EDSS = as.factor(EDSS))
```

# Multiple testing correction

The p-value is the probability of observing a test statistic as extreme as the one observed, given that the null hypothesis is true. Essentially, a value denoting the probability that the observed data was drawn from the population, if the null hypothesis is true. If multiple samples are tested, the probability of observing a significant result by chance increases which is exactly why correcting for multiple testing is important.

For a simple example, the odds of getting a six with one die is $1/6$. If multiple dice are thrown, the odds of getting a six increases If multiple tests are performed, the odds of getting a significant result by chance increases. This is why adjusting for multiple testing to avoid false positives is needed.

To setup a scenario where multiple testing becomes relevant, the `Age` and `BMI` columns are split into groups. The Wilcoxon Rank Sum test is then performed for each subset comparing healthy vs MS patients. The number of data points affect the power of a test, i.e. the chances of finding a statistically significant difference, if one exist, as it reduces randomness in the data. Therefore, the number of data points in each group should be similar. The groups are defined as follows and the distribution is:

```{r}
#| warning: false

meta <- meta |> 
  mutate(Age_group = case_when(Age >= 30 & Age < 36.6 ~ "30-36.6",
                               Age >= 36.6 & Age < 41 ~ "36.6-41",
                               Age >= 41 & Age < 45 ~ "41-45",
                               Age >= 45 & Age < 48.6 ~ "45-48.6",
                               Age >= 48.6 & Age < 51.2 ~ "48.6-51.2",
                               Age >= 51.2 & Age < 53 ~ "51.2-53",
                               Age >= 53 & Age < 55.7 ~ "53-55.7",
                               Age >= 55.7 & Age < 59.3 ~ "55.7-59.3",
                               Age >= 59.3 & Age < 63.2 ~ "59.3-63.2",
                               Age >= 63.2 ~ "63.2+"),
         BMI_group = case_when(BMI >= 17 & BMI < 21.7 ~ "17-21.7",
                               BMI >= 21.7 & BMI < 23.77 ~ "21.7-23.77",
                               BMI >= 23.77 & BMI < 25.77 ~ "23.77-25.77",
                               BMI >= 25.77 & BMI < 28.51 ~ "25.77-28.51",
                               BMI >= 28.51 & BMI < 33.31 ~ "28.51-33.31",
                               BMI >= 33.31 ~ "33.31+"))
meta |> 
  ggplot(aes(x = Age_group,
             fill = Patient_status)) +
  geom_histogram(alpha = 0.7,
                 stat = "count") +
  labs(title = "Age distribution",
       x = "Age group",
       y = "Count")

meta |> 
  drop_na(BMI) |> 
  ggplot(aes(x = BMI_group,
             fill = Patient_status)) +
  geom_histogram(alpha = 0.7,
                 stat = "count") +
  labs(title = "BMI distribution",
       x = "BMI group",
       y = "Count")
```
The Shannon Index is then calculated. As mentioned in @sec-shannon_index, it is a within sample metric, so which subset a sample belongs to is irrelevant for the calculation. Afterwards, the Wilcoxon Rank Sum test is performed for each subset and gathered:

```{r}
shannon <- count_matrix |>
  column_to_rownames(var = "Sample") |>
  diversity(index = "shannon") |> 
  as_tibble(rownames = "Sample") |> 
  rename(Shannon = value) |> 
  left_join(meta,
            by = "Sample")

p_value_per_age_group <- shannon |> 
  group_by(Age_group) |> 
  summarise(p_value = wilcox.test(Shannon ~ Patient_status)$p.value) |> 
  pivot_longer(cols = Age_group,
               names_to = "Group Name",
               values_to = "Group Value")

p_value_per_BMI_group <- shannon |>
  drop_na(BMI) |> 
  group_by(BMI_group) |> 
  summarise(p_value = wilcox.test(Shannon ~ Patient_status)$p.value) |> 
  pivot_longer(cols = BMI_group,
               names_to = "Group Name",
               values_to = "Group Value")

p_values_per_group <- bind_rows(p_value_per_age_group,
                                p_value_per_BMI_group) |> 
  select(`Group Name`, `Group Value`, p_value)
p_values_per_group
```

With a significance level of 0.05, a total of four tests are significant.

```{r}
alpha <- 0.05

p_values_per_group |> 
  filter(p_value < alpha)
```

The probability of not finding a false positive is $1 - \alpha$. When performing multiple tests, the probability of not finding a false positive increases with $(1 - \alpha)^{m}$, where $m$ is number of tests. Since the probability of finding a false positive and not finding one equals 1, the probability of finding a false positive in $m$ tests is $1 - (1- \alpha)^{m}$. 

With 16 tests performed, and a significance level of 0.05, the probability of finding at least on false positive is:

$$
P(false\; positive) = 1 - (1 - \alpha)^{m} = 1 - (1 - 0.05)^{16} = 0.56
$$

The probability of finding at least on false positive is also referred to as the Family-Wise Error Rate (FWER).

A way to correct for multiple testing is needed to avoid these high probabilities of false positives. There are several methods for correcting for multiple testing including common methods such as Bonferroni correction and the Benjamini-Hochberg Procedure


## FWER correction (Bonferroni)
The Bonferroni correction is a conservative method, which means that it is less likely to find significant results. To apply Bonferroni correction, the significance level is divided by the number of tests, which would then also affect the FWER (the probability of finding at least one false positive among the significant results). The Bonferroni adjusted significance level is then:

```{r}
alpha_bonferroni <- alpha / nrow(p_values_per_group)
alpha_bonferroni
```

Using this, we actually find no significant results:

```{r}
p_values_per_group |> 
  filter(p_value < alpha_bonferroni)
```

The FWER becomes:

$$
FWER = 1 - (1 - \frac{\alpha}{m})^{m} = 1 - (1 - \frac{0.05}{16})^{16} = 0.049
$$

The FWER is reduced from 56% to 4.9%, close to the original significance level of 5%. It is then expected to find fewer false positives, also called Type I errors. However, the chance of finding false negatives also increases (referred to as Type II errors). In this case, finding false negatives means accepting the null hypothesis when it was actually false, i.e. not finding a significant result when one exists. In some cases, using a very conservative method is preferred, especially when the cost of a false positive is high.

## False Discovery Rate correction (Benjamini-Hochberg)
Another way to control for multiple testing is through the False Discovery Rate (FDR). FDR is the proportion of false positives among the significant results:

$$
FDR = \frac{FP}{FP + TP}
$$

Where $FP = False Positive$ and $TP = True Positive$. FDR is then an expression for the proportion of false positives among the significant results.

In some cases, a less conservative method than the Bonferroni correction is preferred allowing for more significant hits. This comes at a cost of an increase in false positives. The Benjamini-Hochberg Procedure is such a method - a type of False Discovery Rate (FDR) correction.

Two ways to implement the method exists. Either the significance level is adjusted, or the p-values are adjusted, both giving the same significant hits. The latter is the most common way, but both are introduced.

In general, each p-value is compared to a moving significance level instead of compared to a static value of $\alpha$, e.g. $0.05$. The moving significance level is dependent on the rank of the p-value and the number of tests. The lowest p-value gets rank 1, the second lowest rank 2, and so on. With a higher rank, the significance level increases, allowing for more significant hits.

### Adjusting the significance level
All p-values are sorted in ascending order and ranked as mentioned above. A critical value for each rank is calculated by:

$$
\text{Critical Value} = \frac{k}{m} \cdot \alpha
$$

Where $k$ is the rank of the considered p-value, $m$ is the number of tests and $\alpha$ is the significance level. The largest value of $k$ where the p-value is less than the critical value is found:

$$
\max_{k} \left( p_{(k)} \leq Critical \; Value \right) = \max_{k} \left( p_{(k)} \leq \frac{k}{m} \cdot \alpha \right)
$$

All p-values with a rank less than or equal to the largest $k$ are then considered significant. Note that is is irrelevant if a p-value of lower rank than the rank found from the above, is larger than the critical value. It is still considered significant.

As for the Bonferroni correction, no significant results are found:
```{r}
p_values_per_group |> 
  arrange(p_value) |> 
  mutate(rank = row_number(),
         critical_value = (rank / nrow(p_values_per_group)) * alpha,
         significant = p_value <= critical_value)
```

### Adjusting the p-values
The p-values are again sorted and ranked exactly as before. Apply the following formula to each p-value as an intermediary step:

$$
p_{k,adj} = p_{k} \cdot \frac{m}{k}
$$

Where $m$ is the number of tests, $k$ is the rank of the considered p-value and $p_{k}$ is the p-value of said rank. The adjusted p-values can then be found.

Starting with the highest numbered rank, i.e. largest p-value, the adjusted p-value is simply the intermediary value found from the above. For the second highest rank, the adjusted p-value is the minimum of the intermediary value and the adjusted p-value of previously calculated rank. This is repeated for all ranks. The adjusted p-values are then compared to the significance level $\alpha$, and all p-values less than the significance level are considered significant. Once again, no significant results are found:

```{r}
p_values_per_group |> 
  mutate(p_value_adjusted = p.adjust(p = p_value,
                                     method = "BH"),
         significant = p_value_adjusted < 0.05)
```

# Session Info

```{r}
sessioninfo::session_info()
```
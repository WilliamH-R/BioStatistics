Set seed and load packages.
```{r}
#| output: false
#| code-fold: true
#| code-summary: "Show the code"

set.seed(1337)

library("tidymodels")
tidymodels::tidymodels_prefer()
library("tidyposterior")
library("rstanarm")
```

Load data.
```{r}
#| output: false
#| code-fold: true
#| code-summary: "Show the code"

data("iris")
iris <- iris |>
  tibble::as_tibble() |> 
  filter(Species != "setosa") |> 
  droplevels()

iris_split <- initial_split(iris,
                            strata = Species,
                            prop = 0.90)
iris_train <- training(iris_split)
iris_test <- testing(iris_split)
```


# Creating Multiple Models
## Cross-validation
The reasonings for, and theoretical aspects of, cross-validation (CV) are expected to be already known. In the `tidymodels` universe, CV is setup as:
```{r}
iris_folds <- vfold_cv(iris_train,
                       v = 10)
iris_folds
```



## Recipes
When it comes to modelling, creating and comparing multiple models is essential. This is easily achieved through the use of `recipe()` from the `recipes` package. A recipe is a collection of both a fomula and preprocessing steps. Here preprocessing steps is not applied, but could be: log-transforming data, creating dummy variables (e.g. one hot encoding), sum up low occurring categories to handle class imbalance and dimensionality reduction (see @sec-preprocessing). A series of recipes are creating and combined:

```{r}
SL_rec <- recipe(Species ~ Sepal.Length,
                 data = iris_train)

SLW_rec <- recipe(Species ~ Sepal.Length + Sepal.Width,
                  data = iris_train)

SLW_int_rec <- recipe(Species ~ Sepal.Length + Sepal.Width,
                      data = iris_train) |>
  step_interact(~ Sepal.Length:Sepal.Width)

PL_rec <- recipe(Species ~ Petal.Length,
                 data = iris_train)

PLW_rec <- recipe(Species ~ Petal.Length + Petal.Width,
                  data = iris_train)

PLW_int_rec <- recipe(Species ~ Petal.Length + Petal.Width,
                      data = iris_train) |>
  step_interact(~ Petal.Length:Petal.Width)

recipe_list <- list(SL = SL_rec,
                    SLW = SLW_rec,
                    SLW_int = SLW_int_rec,
                    PL = PL_rec,
                    PLW = PLW_rec,
                    PLW_int = PLW_int_rec)

lg_models <- workflow_set(preproc = recipe_list,
                          models = list(logistic = logistic_reg()),
                          cross = FALSE)
```

Each of the models are fitted with a `purr`-like workflow function:
```{r}
#| warning: false

# To save the predicted values and used workflows
keep_pred <- control_resamples(save_pred = TRUE,
                               save_workflow = TRUE)


lg_models <- lg_models |> 
  workflow_map("fit_resamples",
               resamples = iris_folds,
               control = keep_pred,
               seed = 1337)
lg_models
```

## Evaluation Metrics Across Models
Two simple evaluation metrics for logistic regressions are the accuracy and Area Under the Curve (AUC). It is implicit that it is area under the Receiver Operating Characteristic (ROC) curve. To view these two evaluation metrics (the column `mean`):
```{r}
collect_metrics(lg_models) |> 
  select(-c(.config, preproc, .estimator)) |> 
  filter(.metric == "accuracy") |> 
  arrange(desc(mean))

collect_metrics(lg_models) |> 
  select(-c(.config, preproc, .estimator)) |> 
  filter(.metric == "roc_auc") |> 
  arrange(desc(mean))
```

To illustrate the two metrics:
```{r}
lg_models |> 
  autoplot(metric = "accuracy") +
  geom_label(aes(label = wflow_id)) +
  theme(legend.position = "none") +
  xlim(c(0.5, 6.5)) +
  ggtitle("Accuracy Stratified on Model") +
  theme(text=element_text(size=13))
  
lg_models |> 
  collect_predictions() |> 
  group_by(wflow_id) |> 
  roc_curve(truth = Species,
            .pred_versicolor) |> 
  autoplot() +
  ggtitle("ROC Rurve Stratified on Model") +
  theme(text=element_text(size=12))
```

# Session Info

```{r}
sessioninfo::session_info()
```
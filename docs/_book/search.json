[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BioStatistics",
    "section": "",
    "text": "Preface\nWrite some introduction to the book and its purpose.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "tidy00.html",
    "href": "tidy00.html",
    "title": "Tidy Modelling with R",
    "section": "",
    "text": "Some introduction to the book.",
    "crumbs": [
      "Tidy Modelling with R"
    ]
  },
  {
    "objectID": "tidy01.html",
    "href": "tidy01.html",
    "title": "1  Introduction to Tidy Modelling",
    "section": "",
    "text": "1.1 Introdution to dataset\nFor simplicity, the iris dataset is used since the structure is simple, and readers should be familiar with it. As a quick reminder, the dataset consists of five variables (columns) and 150 observations (rows). The first four variables are doubles describing the size of the flower, used as predictors. The last variable is a factor indicating the species of flower, used as the outcome. To make this a binary logistic classification problem, and not nominal, the versicolor species have been excluded.\ndim(iris)\n\n[1] 100   5\n\niris |&gt; \n  slice_head(n = 5) |&gt;\n  str()\n\ntibble [5 × 5] (S3: tbl_df/tbl/data.frame)\n $ Sepal.Length: num [1:5] 7 6.4 6.9 5.5 6.5\n $ Sepal.Width : num [1:5] 3.2 3.2 3.1 2.3 2.8\n $ Petal.Length: num [1:5] 4.7 4.5 4.9 4 4.6\n $ Petal.Width : num [1:5] 1.4 1.5 1.5 1.3 1.5\n $ Species     : Factor w/ 2 levels \"versicolor\",\"virginica\": 1 1 1 1 1",
    "crumbs": [
      "Tidy Modelling with R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Tidy Modelling</span>"
    ]
  },
  {
    "objectID": "tidy01.html#training--and-test-set",
    "href": "tidy01.html#training--and-test-set",
    "title": "1  Introduction to Tidy Modelling",
    "section": "1.2 Training- and test set",
    "text": "1.2 Training- and test set\nAs is customary when modeling, the dataset is split into a training- and testing set. We need to check for class imbalance in case some outcomes (here species) are more likely in the dataset. As seen from Figure 1.1, it is not the case.\n\niris |&gt; \n  ggplot(aes(x = Species,\n             fill = Species)) +\n  geom_bar()\n\n\n\n\n\n\n\nFigure 1.1: Count of each species in the iris dataset\n\n\n\n\n\nIf we saw a class imbalance, we would have used the below code for splitting the data. The prop argument indicates that 90% of the data is used for training and 20% for testing.\n\niris_split &lt;- initial_split(iris,\n                            prop = 0.90,\n                            strata = Species)\niris_train &lt;- training(iris_split)\niris_test &lt;- testing(iris_split)\n\nAs this is not the case, we use the below very similar code chunk.\n\niris_split &lt;- initial_split(iris, prop = 0.90)\niris_train &lt;- training(iris_split)\niris_test &lt;- testing(iris_split)",
    "crumbs": [
      "Tidy Modelling with R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Tidy Modelling</span>"
    ]
  },
  {
    "objectID": "tidy01.html#fitting-a-model",
    "href": "tidy01.html#fitting-a-model",
    "title": "1  Introduction to Tidy Modelling",
    "section": "1.3 Fitting a model",
    "text": "1.3 Fitting a model\nThe parsnip package standardize creating models. When learning a new technique, here parsnip, it is a good idea to apply well known theory. Therefore, a linear logistic regression model is used to predict the species of a flower given the dimensions of the sepal and the petal.\n\nlg_model &lt;- logistic_reg()\n\nDifferent packages contain different ways of applying a linear logistic regression model, and therefore different ways of supplying input, but the parsnip package standardize it. We choose which engine to use - i.e. which package.\n\n# Show available packages for the model\nshow_engines(\"logistic_reg\")\n\n# A tibble: 7 × 2\n  engine    mode          \n  &lt;chr&gt;     &lt;chr&gt;         \n1 glm       classification\n2 glmnet    classification\n3 LiblineaR classification\n4 spark     classification\n5 keras     classification\n6 stan      classification\n7 brulee    classification\n\nlg_model &lt;- lg_model |&gt; \n  set_engine(\"glm\")\n\nAs the goal is to predict flower species, it is necessary to specify how the model should be fit, e.g. what are the predictors and what is the outcome. This can be done via the formula syntax. From the p-values we see that some predictors are not statistically meaningful. This is ignored for now, but note that this metric is important.\n\n\n\nTable 1.1\n\n\nlg_fit &lt;- lg_model |&gt;\n  fit(Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width,\n      data = iris_train)\n\nlg_fit |&gt; tidy()\n\n# A tibble: 5 × 5\n  term         estimate std.error statistic p.value\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)    -41.6      24.6      -1.69  0.0912\n2 Sepal.Length    -2.35      2.33     -1.01  0.314 \n3 Sepal.Width     -6.94      4.49     -1.54  0.123 \n4 Petal.Length     9.55      4.86      1.97  0.0493\n5 Petal.Width     17.2       9.31      1.84  0.0653",
    "crumbs": [
      "Tidy Modelling with R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Tidy Modelling</span>"
    ]
  },
  {
    "objectID": "tidy01.html#prediction",
    "href": "tidy01.html#prediction",
    "title": "1  Introduction to Tidy Modelling",
    "section": "1.4 Prediction",
    "text": "1.4 Prediction\nMaking predictions is also standardized and requires the function predict(). The model seem to predict correctly in all cases.\n\niris_test |&gt; \n  select(Species) |&gt; \n  bind_cols(predict(lg_fit,\n                    new_data = iris_test))\n\n# A tibble: 10 × 2\n   Species    .pred_class\n   &lt;fct&gt;      &lt;fct&gt;      \n 1 versicolor versicolor \n 2 versicolor versicolor \n 3 versicolor versicolor \n 4 versicolor versicolor \n 5 versicolor versicolor \n 6 versicolor versicolor \n 7 virginica  virginica  \n 8 virginica  virginica  \n 9 virginica  virginica  \n10 virginica  virginica",
    "crumbs": [
      "Tidy Modelling with R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Tidy Modelling</span>"
    ]
  },
  {
    "objectID": "tidy02.html",
    "href": "tidy02.html",
    "title": "2  Creating multiple models",
    "section": "",
    "text": "2.1 Cross-validation\nThe reasonings for, and theoretical aspects of, cross-validation (CV) are expected to be already known. In the tidymodels universe, CV is setup as:\niris_folds &lt;- vfold_cv(iris_train,\n                       v = 10)\niris_folds\n\n#  10-fold cross-validation \n# A tibble: 10 × 2\n   splits         id    \n   &lt;list&gt;         &lt;chr&gt; \n 1 &lt;split [81/9]&gt; Fold01\n 2 &lt;split [81/9]&gt; Fold02\n 3 &lt;split [81/9]&gt; Fold03\n 4 &lt;split [81/9]&gt; Fold04\n 5 &lt;split [81/9]&gt; Fold05\n 6 &lt;split [81/9]&gt; Fold06\n 7 &lt;split [81/9]&gt; Fold07\n 8 &lt;split [81/9]&gt; Fold08\n 9 &lt;split [81/9]&gt; Fold09\n10 &lt;split [81/9]&gt; Fold10",
    "crumbs": [
      "Tidy Modelling with R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Creating multiple models</span>"
    ]
  },
  {
    "objectID": "tidy02.html#recipes",
    "href": "tidy02.html#recipes",
    "title": "2  Creating multiple models",
    "section": "2.2 Recipes",
    "text": "2.2 Recipes\nWhen it comes to modelling, creating and comparing multiple models is essential. This is easily achieved through the use of recipe() from the recipes package. A recipe is a collection of both a fomula and preprocessing steps. Here preprocessing steps is not applied, but could be: log-transforming data, creating dummy variables (e.g. one hot encoding) and sum up low occurring categories to handle class imbalance.\n\nSL_rec &lt;- recipe(Species ~ Sepal.Length,\n                 data = iris_train)\n\nSLW_rec &lt;- recipe(Species ~ Sepal.Length + Sepal.Width,\n                  data = iris_train)\n\nSLW_int_rec &lt;- recipe(Species ~ Sepal.Length + Sepal.Width,\n                      data = iris_train) |&gt;\n  step_interact(~ Sepal.Length:Sepal.Width)\n\nPL_rec &lt;- recipe(Species ~ Petal.Length,\n                 data = iris_train)\n\nPLW_rec &lt;- recipe(Species ~ Petal.Length + Petal.Width,\n                  data = iris_train)\n\nPLW_int_rec &lt;- recipe(Species ~ Petal.Length + Petal.Width,\n                      data = iris_train) |&gt;\n  step_interact(~ Petal.Length:Petal.Width)\n\nrecipe_list &lt;- list(SL = SL_rec,\n                    SLW = SLW_rec,\n                    SLW_int = SLW_int_rec,\n                    PL = PL_rec,\n                    PLW = PLW_rec,\n                    PLW_int = PLW_int_rec)\n\nlg_models &lt;- workflow_set(preproc = recipe_list,\n                          models = list(logistic = logistic_reg()),\n                          cross = FALSE)\n\nEach of the v folds are fitted with a purr-like workflow function:\n\n# To save the predicted values and used workflows\nkeep_pred &lt;- control_resamples(save_pred = TRUE, save_workflow = TRUE)\n\n\nlg_models &lt;- lg_models |&gt; \n  workflow_map(\"fit_resamples\",\n               resamples = iris_folds,\n               control = keep_pred,\n               seed = 1337)\n\n→ A | warning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\n\nThere were issues with some computations   A: x1\nThere were issues with some computations   A: x1\n\n\n\n\nlg_models\n\n# A workflow set/tibble: 6 × 4\n  wflow_id         info             option    result   \n  &lt;chr&gt;            &lt;list&gt;           &lt;list&gt;    &lt;list&gt;   \n1 SL_logistic      &lt;tibble [1 × 4]&gt; &lt;opts[2]&gt; &lt;rsmp[+]&gt;\n2 SLW_logistic     &lt;tibble [1 × 4]&gt; &lt;opts[2]&gt; &lt;rsmp[+]&gt;\n3 SLW_int_logistic &lt;tibble [1 × 4]&gt; &lt;opts[2]&gt; &lt;rsmp[+]&gt;\n4 PL_logistic      &lt;tibble [1 × 4]&gt; &lt;opts[2]&gt; &lt;rsmp[+]&gt;\n5 PLW_logistic     &lt;tibble [1 × 4]&gt; &lt;opts[2]&gt; &lt;rsmp[+]&gt;\n6 PLW_int_logistic &lt;tibble [1 × 4]&gt; &lt;opts[2]&gt; &lt;rsmp[+]&gt;",
    "crumbs": [
      "Tidy Modelling with R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Creating multiple models</span>"
    ]
  },
  {
    "objectID": "tidy02.html#evaluation-metrics-across-models",
    "href": "tidy02.html#evaluation-metrics-across-models",
    "title": "2  Creating multiple models",
    "section": "2.3 Evaluation metrics across models",
    "text": "2.3 Evaluation metrics across models\nTwo simple evaluation metrics for logistic regressions are the accuracy and Area Under the Curve (AUC), in this case area under the Receiver Operating Characteristic (ROC) curve. To view these two evaluation metrics:\n\ncollect_metrics(lg_models) |&gt; \n  select(-c(.config, preproc, .estimator)) |&gt; \n  filter(.metric == \"accuracy\") |&gt; \n  arrange(desc(mean))\n\n# A tibble: 6 × 6\n  wflow_id         model        .metric   mean     n std_err\n  &lt;chr&gt;            &lt;chr&gt;        &lt;chr&gt;    &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1 PLW_int_logistic logistic_reg accuracy 0.933    10  0.0246\n2 PL_logistic      logistic_reg accuracy 0.922    10  0.0237\n3 PLW_logistic     logistic_reg accuracy 0.922    10  0.0289\n4 SL_logistic      logistic_reg accuracy 0.733    10  0.0296\n5 SLW_logistic     logistic_reg accuracy 0.722    10  0.0299\n6 SLW_int_logistic logistic_reg accuracy 0.711    10  0.0339\n\ncollect_metrics(lg_models) |&gt; \n  select(-c(.config, preproc, .estimator)) |&gt; \n  filter(.metric == \"roc_auc\") |&gt; \n  arrange(desc(mean))\n\n# A tibble: 6 × 6\n  wflow_id         model        .metric  mean     n std_err\n  &lt;chr&gt;            &lt;chr&gt;        &lt;chr&gt;   &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1 PL_logistic      logistic_reg roc_auc 0.991    10 0.00458\n2 PLW_logistic     logistic_reg roc_auc 0.988    10 0.00825\n3 PLW_int_logistic logistic_reg roc_auc 0.988    10 0.00825\n4 SL_logistic      logistic_reg roc_auc 0.800    10 0.0442 \n5 SLW_logistic     logistic_reg roc_auc 0.787    10 0.0404 \n6 SLW_int_logistic logistic_reg roc_auc 0.776    10 0.0458 \n\n\nTo illustrate the two metrics:\n\nlg_models |&gt; \n  autoplot(metric = \"accuracy\") +\n  geom_label(aes(label = wflow_id)) +\n  theme(legend.position = \"none\") +\n  xlim(c(0.5, 6.5)) +\n  ggtitle(\"Accuracy stratified on model\")\n\n\n\n\n\n\n\nlg_models |&gt; \n  collect_predictions() |&gt; \n  group_by(wflow_id) |&gt; \n  roc_curve(truth = Species,\n            .pred_versicolor) |&gt; \n  autoplot() +\n  ggtitle(\"ROC curve statified on model\")",
    "crumbs": [
      "Tidy Modelling with R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Creating multiple models</span>"
    ]
  },
  {
    "objectID": "tidy02.html#resample-to-resample-component-of-variation-consider-remove",
    "href": "tidy02.html#resample-to-resample-component-of-variation-consider-remove",
    "title": "2  Creating multiple models",
    "section": "2.4 Resample-to-resample component of variation (consider remove)",
    "text": "2.4 Resample-to-resample component of variation (consider remove)\nAll the models are tested on the same v-folds. In some cases, different models tends to perform well on the same folds - this effect is called a resample-to-resample component of variation. We can numerically investigate these correlations by correlating each model estimates with eachother:\n\nlg_models |&gt; \n  collect_metrics(summarize = FALSE) |&gt; \n  filter(.metric == \"accuracy\") |&gt; \n  select(wflow_id, .estimate, id) |&gt; \n  pivot_wider(id_cols = \"id\",\n              names_from = \"wflow_id\",\n              values_from = \".estimate\") |&gt;\n  select(-id) |&gt; \n  corrr::correlate(quiet = TRUE)\n\n# A tibble: 6 × 7\n  term        SL_logistic SLW_logistic SLW_int_logistic PL_logistic PLW_logistic\n  &lt;chr&gt;             &lt;dbl&gt;        &lt;dbl&gt;            &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;\n1 SL_logistic     NA            0.930            0.627      -0.547        0.0320\n2 SLW_logist…      0.930       NA                0.812      -0.291       -0.0794\n3 SLW_int_lo…      0.627        0.812           NA           0.136        0.112 \n4 PL_logistic     -0.547       -0.291            0.136      NA           -0.180 \n5 PLW_logist…      0.0320      -0.0794           0.112      -0.180       NA     \n6 PLW_int_lo…     -0.0754      -0.187            0.0658     -0.0471       0.927 \n# ℹ 1 more variable: PLW_int_logistic &lt;dbl&gt;\n\n\nThe correlation illustrated:\n\nlg_models |&gt; \n  collect_metrics(summarize = FALSE) |&gt; \n  filter(.metric == \"accuracy\") |&gt; \n  mutate(wflow_id = reorder(wflow_id,\n                            .estimate)) |&gt; \n  ggplot(aes(x = wflow_id,\n             y = .estimate,\n             group = id,\n             color = id)) + \n  geom_line(alpha = .5,\n            linewidth = 1.25) + \n  theme(legend.position = \"none\")",
    "crumbs": [
      "Tidy Modelling with R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Creating multiple models</span>"
    ]
  },
  {
    "objectID": "tidy03.html",
    "href": "tidy03.html",
    "title": "3  Tuning hyperparameters and overfitting",
    "section": "",
    "text": "3.1 What is a hyperparameter?\nA classical and simple example of a hyperparameter is the number of neighbors (k) in a k-nearest neighbors (KNN) algorithm. This is a hyperparameter as it is not estimated during model fitting, but is specified a priori making it impossible to optimize during parameter estimation.",
    "crumbs": [
      "Tidy Modelling with R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Tuning hyperparameters and overfitting</span>"
    ]
  },
  {
    "objectID": "tidy03.html#set-up-tuning",
    "href": "tidy03.html#set-up-tuning",
    "title": "3  Tuning hyperparameters and overfitting",
    "section": "3.2 Set up tuning",
    "text": "3.2 Set up tuning\nIn the tidymodels universe, hyperparameters are marked for tuning in the specifications for a model. To examplify, both the number of nearest neighbors and a range of weight functions are tuned.\n\nknn_spec &lt;- nearest_neighbor(neighbors = tune(),\n                             weight_func = tune()) |&gt; \n  set_engine(engine = \"kknn\",\n             trace = 0) |&gt; \n  set_mode(\"classification\")\n\nSecondly, the recipe is set up. As no preprocessing is applied (e.g. log-transformation) it is quite simple.\n\nknn_rec &lt;- recipe(Species ~ ., # Use all other columns as predictors\n                  data = iris)\n\nThe specs and recipe is then combined into a workflow:\n\nknn_wflow &lt;- workflow() |&gt; \n  add_model(knn_spec) |&gt; \n  add_recipe(knn_rec)\n\nIt is possible to inspect which hyperparameters are being tuned, check which values that are tested and change those values. This is done through the use of the dials package.\n\n# Check hyperparameters\nknn_spec |&gt; extract_parameter_set_dials()\n\nCollection of 2 parameters for tuning\n\n  identifier        type    object\n   neighbors   neighbors nparam[+]\n weight_func weight_func dparam[+]\n\n# Check values tested\nknn_spec |&gt; extract_parameter_set_dials() |&gt; \n  extract_parameter_dials(\"weight_func\")\n\nDistance Weighting Function  (qualitative)\n\n\n10 possible values include:\n\n\n'rectangular', 'triangular', 'epanechnikov', 'biweight', 'triweight', 'cos', ... \n\n# Change values, save in new object\nknn_params &lt;- knn_spec |&gt; extract_parameter_set_dials() |&gt;\n  update(weight_func = weight_func(c(\"cos\", \"inv\", \"gaussian\")),\n         neighbors = neighbors(c(1, 15)))\n\n# Check that it is updated\nknn_params |&gt;\n  extract_parameter_dials(\"weight_func\")\n\nDistance Weighting Function  (qualitative)\n\n\n3 possible values include:\n\n\n'cos', 'inv' and 'gaussian' \n\nknn_params |&gt;\n  extract_parameter_dials(\"neighbors\")\n\n# Nearest Neighbors (quantitative)\nRange: [1, 15]\n\n\nDifferent grid_* functions exist to combine the hyperparameters, e.g. grid_random() and grid_regular(). As exemplified below, grid_regular() combines the parameters in all possible ways dependent on the number of levels chosen.\n\ngrid_regular(knn_params,\n             levels = 4)\n\n# A tibble: 12 × 2\n   neighbors weight_func\n       &lt;int&gt; &lt;chr&gt;      \n 1         1 cos        \n 2         5 cos        \n 3        10 cos        \n 4        15 cos        \n 5         1 inv        \n 6         5 inv        \n 7        10 inv        \n 8        15 inv        \n 9         1 gaussian   \n10         5 gaussian   \n11        10 gaussian   \n12        15 gaussian",
    "crumbs": [
      "Tidy Modelling with R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Tuning hyperparameters and overfitting</span>"
    ]
  },
  {
    "objectID": "tidy03.html#measure-performance-of-tuning",
    "href": "tidy03.html#measure-performance-of-tuning",
    "title": "3  Tuning hyperparameters and overfitting",
    "section": "3.3 Measure performance of tuning",
    "text": "3.3 Measure performance of tuning\nA metric is needed to measure the performance of the hyperparameters. The ROC curve is used. The regular grid is tuned:\n\n# Performance metric\nroc &lt;- metric_set(roc_auc)\n\n# Tuning\nknn_tune &lt;- knn_wflow |&gt; \n  tune_grid(iris_folds,\n            grid = knn_params |&gt; grid_regular(levels = 4),\n            metrics = roc)\nknn_tune\n\n# Tuning results\n# 10-fold cross-validation \n# A tibble: 10 × 4\n   splits          id     .metrics          .notes          \n   &lt;list&gt;          &lt;chr&gt;  &lt;list&gt;            &lt;list&gt;          \n 1 &lt;split [90/10]&gt; Fold01 &lt;tibble [12 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 2 &lt;split [90/10]&gt; Fold02 &lt;tibble [12 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 3 &lt;split [90/10]&gt; Fold03 &lt;tibble [12 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 4 &lt;split [90/10]&gt; Fold04 &lt;tibble [12 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 5 &lt;split [90/10]&gt; Fold05 &lt;tibble [12 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 6 &lt;split [90/10]&gt; Fold06 &lt;tibble [12 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 7 &lt;split [90/10]&gt; Fold07 &lt;tibble [12 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 8 &lt;split [90/10]&gt; Fold08 &lt;tibble [12 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 9 &lt;split [90/10]&gt; Fold09 &lt;tibble [12 × 6]&gt; &lt;tibble [0 × 3]&gt;\n10 &lt;split [90/10]&gt; Fold10 &lt;tibble [12 × 6]&gt; &lt;tibble [0 × 3]&gt;\n\n\nTo visualize the performance:\n\nknn_tune |&gt; \n  unnest(cols = .metrics) |&gt; \n  select(id, .metric, neighbors, weight_func, .estimate) |&gt;\n  group_by(neighbors, weight_func) |&gt; \n  mutate(estimate_avg = mean(.estimate)) |&gt;\n  ggplot(aes(x = neighbors,\n             y = estimate_avg)) +\n  geom_point() +\n  geom_line() +\n  scale_x_continuous(breaks = c(1, 5, 10, 15)) +\n  facet_wrap(~ weight_func)",
    "crumbs": [
      "Tidy Modelling with R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Tuning hyperparameters and overfitting</span>"
    ]
  },
  {
    "objectID": "tidy03.html#finalize-hyperparameter-selection",
    "href": "tidy03.html#finalize-hyperparameter-selection",
    "title": "3  Tuning hyperparameters and overfitting",
    "section": "3.4 Finalize hyperparameter selection",
    "text": "3.4 Finalize hyperparameter selection\nIt would seem there is no visual difference between the weight functions. For the number of neighbors, the performance is highest for 10 and 15 neighbors. Preferably, the simplest of the two models is chosen.\n\nfinal_hyperparams &lt;- tibble(weight_func = \"gaussian\",\n                            neighbors = 10)\n\nfinal_knn_wflow &lt;- knn_wflow |&gt; \n  finalize_workflow(final_hyperparams)\nfinal_knn_wflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: nearest_neighbor()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nK-Nearest Neighbor Model Specification (classification)\n\nMain Arguments:\n  neighbors = 10\n  weight_func = gaussian\n\nEngine-Specific Arguments:\n  trace = 0\n\nComputational engine: kknn \n\n\nThe model can now be fit to the data and used for prediction.\n\nfinal_knn_fit &lt;- final_knn_wflow |&gt; \n  fit(iris)\nfinal_knn_fit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: nearest_neighbor()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\nkknn::train.kknn(formula = ..y ~ ., data = data, ks = min_rows(10,     data, 5), kernel = ~\"gaussian\", trace = ~0)\n\nType of response variable: nominal\nMinimal misclassification: 0.07\nBest kernel: gaussian\nBest k: 10",
    "crumbs": [
      "Tidy Modelling with R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Tuning hyperparameters and overfitting</span>"
    ]
  },
  {
    "objectID": "stat00.html",
    "href": "stat00.html",
    "title": "Applied bio-statistical methods",
    "section": "",
    "text": "Each of the following chapters delve into an area of statistics. This ranges from biostatistics such as diversity measures and different statistical models such as Ordinary Linear Regression (OLS).\nIn the first part of the book, the iris data set was used to exemplify the usage of the tidymodels package. For the following chapters, a count matrix from a metagenomics study is used instead. The raw data can be obtained from Cox et al.(Cox et al. 2021). The structure of the data is quite similar to the iris data. In this case, each row is a sample from a person, each column is a genus and the values represent the abundance of said genus in said sample. Explaining how the count matrix is acquired from raw sequences is out of scope for this book. Scripts used for the pre-processing exists on the GitHub page from which this book is created, located in the src/ sub-folder.\nA snippet of the data set can be seen here:\n\ncount_matrix &lt;- readRDS(\"data/count_matrix/count_matrix.rds\")\ncount_matrix\n\n# A tibble: 456 × 124\n   Sample      Actinomyces Adlercreutzia Agathobacter Akkermansia Alistipes\n   &lt;chr&gt;             &lt;int&gt;         &lt;int&gt;        &lt;int&gt;       &lt;int&gt;     &lt;int&gt;\n 1 SRR14214860           0            11          190         486       272\n 2 SRR14214861           0            12            0           5      1158\n 3 SRR14214862           0             0            0           0         0\n 4 SRR14214863           4             0          505          94       361\n 5 SRR14214864           0            45          744           3       794\n 6 SRR14214865           0            29          924         933        45\n 7 SRR14214866           0            41         1187         308       145\n 8 SRR14214867           0            45          648         323       193\n 9 SRR14214868           0             0         6973           0       287\n10 SRR14214869           0             0          362         270        30\n# ℹ 446 more rows\n# ℹ 118 more variables: Anaerofustis &lt;int&gt;, Anaerostipes &lt;int&gt;,\n#   Anaerotruncus &lt;int&gt;, Angelakisella &lt;int&gt;, Bacteroides &lt;int&gt;,\n#   Barnesiella &lt;int&gt;, Bifidobacterium &lt;int&gt;, Bilophila &lt;int&gt;, Blautia &lt;int&gt;,\n#   Butyricicoccus &lt;int&gt;, Butyricimonas &lt;int&gt;, `CAG-352` &lt;int&gt;, `CAG-56` &lt;int&gt;,\n#   `Candidatus Soleaferrea` &lt;int&gt;, `Candidatus Stoquefichus` &lt;int&gt;,\n#   Catenibacillus &lt;int&gt;, Catenibacterium &lt;int&gt;, …\n\n\nThe shown snippet is used as test data when exemplifying.\n\ncount_matrix_test &lt;- count_matrix |&gt; \n  slice_head(n = 10) |&gt; \n  select(Sample, Actinomyces,\n         Adlercreutzia, Agathobacter, Akkermansia)\ncount_matrix_test\n\n# A tibble: 10 × 5\n   Sample      Actinomyces Adlercreutzia Agathobacter Akkermansia\n   &lt;chr&gt;             &lt;int&gt;         &lt;int&gt;        &lt;int&gt;       &lt;int&gt;\n 1 SRR14214860           0            11          190         486\n 2 SRR14214861           0            12            0           5\n 3 SRR14214862           0             0            0           0\n 4 SRR14214863           4             0          505          94\n 5 SRR14214864           0            45          744           3\n 6 SRR14214865           0            29          924         933\n 7 SRR14214866           0            41         1187         308\n 8 SRR14214867           0            45          648         323\n 9 SRR14214868           0             0         6973           0\n10 SRR14214869           0             0          362         270\n\n\n\n\n\n\nCox, Laura M., Amir Hadi Maghzi, Shirong Liu, Stephanie K. Tankou, Fyonn H. Dhang, Valerie Willocq, Anya Song, et al. 2021. “Gut Microbiome in Progressive Multiple Sclerosis.” Annals of Neurology 89 (June): 1195–1211. https://doi.org/10.1002/ANA.26084.",
    "crumbs": [
      "Applied bio-statistical methods"
    ]
  },
  {
    "objectID": "stat01.html",
    "href": "stat01.html",
    "title": "4  Diversity measures",
    "section": "",
    "text": "4.1 Alpha Diversity\nGenerally, alpha diversity measures the within-sample diversity. Multiple ways of calculating alpha diversity exists. A crude way is simply to count unique organisms per sample. One could also take the distribution into account or the phylogeny.\nThe R package vegan is widely used to help calculate diversity and is used here.",
    "crumbs": [
      "Applied bio-statistical methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Diversity measures</span>"
    ]
  },
  {
    "objectID": "stat01.html#alpha-diversity",
    "href": "stat01.html#alpha-diversity",
    "title": "4  Diversity measures",
    "section": "",
    "text": "4.1.1 Species Richness\nAs mentioned above, a crude way to measure diversity is to count the number of unique organisms in a given taxon for each sample. That is exactly what the species richness is. Through the vegan package, it can be calculated as follows:\n\nrichness &lt;- count_matrix_test |&gt;\n  column_to_rownames(var = \"Sample\") |&gt; \n  specnumber() |&gt;\n  as_tibble(rownames = \"Sample\") |&gt;\n  rename(Richness = value)\n\ncount_matrix_test |&gt;\n  left_join(richness,\n            by = \"Sample\")\n\n# A tibble: 10 × 6\n   Sample      Actinomyces Adlercreutzia Agathobacter Akkermansia Richness\n   &lt;chr&gt;             &lt;int&gt;         &lt;int&gt;        &lt;int&gt;       &lt;int&gt;    &lt;int&gt;\n 1 SRR14214860           0            11          190         486        3\n 2 SRR14214861           0            12            0           5        2\n 3 SRR14214862           0             0            0           0        0\n 4 SRR14214863           4             0          505          94        3\n 5 SRR14214864           0            45          744           3        3\n 6 SRR14214865           0            29          924         933        3\n 7 SRR14214866           0            41         1187         308        3\n 8 SRR14214867           0            45          648         323        3\n 9 SRR14214868           0             0         6973           0        1\n10 SRR14214869           0             0          362         270        2\n\n\n\n\n4.1.2 Shannon Index\nThe measure is usually referred to as Shannon Index or Shannon Entropy. It is able to take both the richness (described above) and the evenness into account. The evenness refers to how evenly the abundances are distributed among taxa for a species. The richness is considered when summing over all taxonomic units at a specified taxonomic rank. The index is calculated via the following formula:\n\\[\nH' = -\\sum_{i=1}^{R} p_i \\ln(p_i)\n\\]\n\\(R\\) is the number of observed species within a sample, i.e. the Richness. \\(p_{i}\\) is the proportion of the abundance belonging to the \\(i\\)’th species out of the total abundance for a specific sample(Shannon 1948).\nTo illustrate the meaning of the value of the Shannon Index a few examples are calculated. In the first scenario, three species and three samples exists. For the first sample, the proportion of abundances are spread evenly between the three species. For the second and third sample, the proportions are more skewered towards one species. These examples show how the evenness affect the Shannon Index.\n\n\n# A tibble: 3 × 4\n  sample   species_1 species_2 species_3\n  &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 sample_A       0.3       0.3       0.3\n2 sample_B       0.6       0.2       0.2\n3 sample_C       1         0         0  \n\n\nFor each of the three samples, the calculation would be as follows:\n\\[\nH'_{sample_{A}} = -(0.3 \\cdot ln{(0.3)} + 0.3 \\cdot ln{(0.3)} + 0.3 \\cdot ln{(0.3)}) = 1.08\n\\]\n\\[\nH'_{sample_{B}} = -(0.6 \\cdot ln{(0.6)} + 0.2 \\cdot ln{(0.2)} + 0.2 \\cdot ln{(0.2)}) = 0.95\n\\]\n\\[\nH'_{sample_{C}} = -(1 \\cdot ln{(1)}) = 0\n\\]\nWhen the samples are evenly distributed, we observe a higher Shannon Index.\nIn the following example, the number of species are increased to five. Three different samples with different distributions are still investigated.\n\n\n# A tibble: 3 × 6\n  sample   species_1 species_2 species_3 species_4 species_5\n  &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 sample_D       0.2       0.2       0.2       0.2       0.2\n2 sample_E       0.6       0.1       0.1       0.1       0.1\n3 sample_F       1         0         0         0         0  \n\n\n\\[\nH'_{sample_{D}} = 1.61\n\\]\n\\[\nH'_{sample_{E}} = 1.23\n\\]\n\\[\nH'_{sample_{F}} = 0\n\\]\nAs before, the Shannon Index decreases with a more skewered distribution. Both \\(sample_{A}\\) and \\(sample_{D}\\) have even distributions, but the Shannon Index for \\(sample_{D}\\) higher since the richness is higher, i.e. more species with an even distribution.\nThe diversity in a sample is said to be high when the number of species are high, and the distribution is even. As shown above, that would yield a high Shannon Index. So, to conclude, a high Shannon Index suggests high diversity.\nFinally, Shannon Index calculated for the test set. The index does seem to be higher when the genera are more evenly distributed as show in the above.\n\nshannon &lt;- count_matrix_test |&gt;\n  column_to_rownames(var = \"Sample\") |&gt;\n  diversity(index = \"shannon\") |&gt; \n  as_tibble(rownames = \"Sample\") |&gt; \n  rename(Shannon = value)\n\ncount_matrix_test |&gt;\n  left_join(shannon,\n            by = \"Sample\")\n\n# A tibble: 10 × 6\n   Sample      Actinomyces Adlercreutzia Agathobacter Akkermansia Shannon\n   &lt;chr&gt;             &lt;int&gt;         &lt;int&gt;        &lt;int&gt;       &lt;int&gt;   &lt;dbl&gt;\n 1 SRR14214860           0            11          190         486   0.667\n 2 SRR14214861           0            12            0           5   0.606\n 3 SRR14214862           0             0            0           0   0    \n 4 SRR14214863           4             0          505          94   0.472\n 5 SRR14214864           0            45          744           3   0.243\n 6 SRR14214865           0            29          924         933   0.762\n 7 SRR14214866           0            41         1187         308   0.618\n 8 SRR14214867           0            45          648         323   0.789\n 9 SRR14214868           0             0         6973           0   0    \n10 SRR14214869           0             0          362         270   0.683\n\n\n\n\n4.1.3 Faith’s phylogenetic diversity (Faith’s PD)\nFaith’s PD consider phylogenetic distances within a sample. The PD for a sample is calculated as the total sum of branch lengths from a phylogenetic tree, where the tree only contains the organisms found in a given sample (Faith 1992). Imagine a scenario with four organisms, organism a and b are closely related, and so are organism c and d (Figure 4.1).\n\n\n\n\n\n\nFigure 4.1: Phylogenetic tree\n\n\n\nIn the case where all four organisms were found in the same sample, the PD would be calculated as:\n\\[\nPD = 15 + 6 + 4 + 3 + 1 + 2 + 4 = 35\n\\]\nIf organism a was lost, the PD would be decreased with 4 units to a value of 31. A higher PD is thus indicative of a higher diversity.\n\n\n\n\nFaith, Daniel P. 1992. “Conservation Evaluation and Phylogenetic Diversity.” Biological Conservation 61 (January): 1–10. https://doi.org/10.1016/0006-3207(92)91201-3.\n\n\nShannon, C. E. 1948. “A Mathematical Theory of Communication.” Bell System Technical Journal 27: 623–56. https://doi.org/10.1002/J.1538-7305.1948.TB00917.X.",
    "crumbs": [
      "Applied bio-statistical methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Diversity measures</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Cox, Laura M., Amir Hadi Maghzi, Shirong Liu, Stephanie K. Tankou, Fyonn\nH. Dhang, Valerie Willocq, Anya Song, et al. 2021. “Gut Microbiome\nin Progressive Multiple Sclerosis.” Annals of Neurology\n89 (June): 1195–1211. https://doi.org/10.1002/ANA.26084.\n\n\nFaith, Daniel P. 1992. “Conservation Evaluation and Phylogenetic\nDiversity.” Biological Conservation 61 (January): 1–10.\nhttps://doi.org/10.1016/0006-3207(92)91201-3.\n\n\nShannon, C. E. 1948. “A Mathematical Theory of\nCommunication.” Bell System Technical Journal 27:\n623–56. https://doi.org/10.1002/J.1538-7305.1948.TB00917.X.",
    "crumbs": [
      "References"
    ]
  }
]
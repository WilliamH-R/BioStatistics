[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BioStatistics",
    "section": "",
    "text": "Preface\nThis book introduces the concept of Tidy Modelling through the package tidymodels, and is designed to be a practical guide to the tools and techniques that are used in the Tidy Modelling framework. It is aimed at people who are familiar with the basics of R and tidyverse and are interested in learning how to use the tidymodels package to build and evaluate models. Further, a series of statistical concepts often used in Bioinformatics and Biostatistics are introduced in the context of Tidy Modelling.\nTo cover all the topics, the book is divided into three main parts. The first part introduces the tidymodels package and its core principles such as such as data splitting, pre-processing, model building, and evaluation. The second part introduces some statistical concepts that are often used in Bioinformatics and Biostatistics, such as hypothesis testing, linear regression, logistic regression, and diversity measures. The concepts are, where applicable, introduced with a description of the method followed by the math behind it and lastly, visual representation using the tidymodels package.\nThe final part combines the concepts introduced in the first two parts to build and evaluate models for a real-world dataset. A study pertaining to microbiome and disease prediction is used. The raw data are 16S rRNA amplicon reads used to describe multiple sclerosis (MS) (Cox et al. 2021). The samples were pre-processed using the pipeline described in the GitHub repository from which this book is created. After pre-processing, a count matrix was obtained with 456 samples, and the abundance of 124 genera. For each sample, the sex and age of the person is known among other information. All of these variables are used to build several models trying to predict the disease status of the person.\n\n\n\n\nCox, Laura M., Amir Hadi Maghzi, Shirong Liu, Stephanie K. Tankou, Fyonn H. Dhang, Valerie Willocq, Anya Song, et al. 2021. “Gut Microbiome in Progressive Multiple Sclerosis.” Annals of Neurology 89 (June): 1195–1211. https://doi.org/10.1002/ANA.26084.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "tidy00_book_intro.html",
    "href": "tidy00_book_intro.html",
    "title": "Tidy Modelling with R",
    "section": "",
    "text": "R is already a great tool for data analysis and visualization. However, when it comes to building and evaluating models, it can be a bit cumbersome. Different packages have different syntax and conventions, which can make it difficult to switch between them. The tidymodels package aims to solve this problem by providing a consistent and easy-to-use interface for building and evaluating models. The tidymodels package is a collection of packages that work together to make the process of building and evaluating models easier and more consistent. It works excellently as an extension to tidyverse by using the same methodology and syntax. In this chapter, the principles of the tidymodels package is introduced as well as how to create, evaluate, and tune many models at a time.",
    "crumbs": [
      "Tidy Modelling with R"
    ]
  },
  {
    "objectID": "tidy01_intro_package.html",
    "href": "tidy01_intro_package.html",
    "title": "1  Introduction to Tidy Modelling",
    "section": "",
    "text": "1.1 Introdution to dataset\nFor simplicity, the iris dataset is used since the structure is simple, and readers should be familiar with it. As a quick reminder, the dataset consists of five variables (columns) and 150 observations (rows). The first four variables are doubles describing the size of the flower, used as predictors. The last variable is a factor indicating the species of flower, used as the outcome. To make this a binary logistic classification problem, and not nominal, the versicolor species have been excluded.\ndim(iris)\n\n[1] 100   5\n\niris |&gt; \n  slice_head(n = 5) |&gt;\n  str()\n\ntibble [5 × 5] (S3: tbl_df/tbl/data.frame)\n $ Sepal.Length: num [1:5] 7 6.4 6.9 5.5 6.5\n $ Sepal.Width : num [1:5] 3.2 3.2 3.1 2.3 2.8\n $ Petal.Length: num [1:5] 4.7 4.5 4.9 4 4.6\n $ Petal.Width : num [1:5] 1.4 1.5 1.5 1.3 1.5\n $ Species     : Factor w/ 2 levels \"versicolor\",\"virginica\": 1 1 1 1 1",
    "crumbs": [
      "Tidy Modelling with R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Tidy Modelling</span>"
    ]
  },
  {
    "objectID": "tidy01_intro_package.html#training--and-test-set",
    "href": "tidy01_intro_package.html#training--and-test-set",
    "title": "1  Introduction to Tidy Modelling",
    "section": "1.2 Training- and test set",
    "text": "1.2 Training- and test set\nAs is customary when modeling, the dataset is split into a training- and testing set. We need to check for class imbalance in case some outcomes (here species) are more likely in the dataset. As seen from Figure 1.1, it is not the case.\n\niris |&gt; \n  ggplot(aes(x = Species,\n             fill = Species)) +\n  geom_bar()\n\n\n\n\n\n\n\nFigure 1.1: Count of each species in the iris dataset\n\n\n\n\n\nIf we saw a class imbalance, we would have used the below code for splitting the data. The prop argument indicates that 90% of the data is used for training and 20% for testing.\n\niris_split &lt;- initial_split(iris,\n                            prop = 0.90,\n                            strata = Species)\niris_train &lt;- training(iris_split)\niris_test &lt;- testing(iris_split)\n\nAs this is not the case, we use the below very similar code chunk.\n\niris_split &lt;- initial_split(iris, prop = 0.90)\niris_train &lt;- training(iris_split)\niris_test &lt;- testing(iris_split)",
    "crumbs": [
      "Tidy Modelling with R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Tidy Modelling</span>"
    ]
  },
  {
    "objectID": "tidy01_intro_package.html#fitting-a-model",
    "href": "tidy01_intro_package.html#fitting-a-model",
    "title": "1  Introduction to Tidy Modelling",
    "section": "1.3 Fitting a model",
    "text": "1.3 Fitting a model\nThe parsnip package standardize creating models. When learning a new technique, here parsnip, it is a good idea to apply well known theory. Therefore, a linear logistic regression model is used to predict the species of a flower given the dimensions of the sepal and the petal.\n\nlg_model &lt;- logistic_reg()\n\nDifferent packages contain different ways of applying a linear logistic regression model, and therefore different ways of supplying input, but the parsnip package standardize it. We choose which engine to use - i.e. which package.\n\n# Show available packages for the model\nshow_engines(\"logistic_reg\")\n\n# A tibble: 7 × 2\n  engine    mode          \n  &lt;chr&gt;     &lt;chr&gt;         \n1 glm       classification\n2 glmnet    classification\n3 LiblineaR classification\n4 spark     classification\n5 keras     classification\n6 stan      classification\n7 brulee    classification\n\nlg_model &lt;- lg_model |&gt; \n  set_engine(\"glm\")\n\nAs the goal is to predict flower species, it is necessary to specify how the model should be fit, e.g. what are the predictors and what is the outcome. This can be done via the formula syntax. From the p-values we see that some predictors are not statistically meaningful. This is ignored for now, but note that this metric is important.\n\n\n\nTable 1.1\n\n\nlg_fit &lt;- lg_model |&gt;\n  fit(Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width,\n      data = iris_train)\n\nlg_fit |&gt; tidy()\n\n# A tibble: 5 × 5\n  term         estimate std.error statistic p.value\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)    -41.6      24.6      -1.69  0.0912\n2 Sepal.Length    -2.35      2.33     -1.01  0.314 \n3 Sepal.Width     -6.94      4.49     -1.54  0.123 \n4 Petal.Length     9.55      4.86      1.97  0.0493\n5 Petal.Width     17.2       9.31      1.84  0.0653",
    "crumbs": [
      "Tidy Modelling with R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Tidy Modelling</span>"
    ]
  },
  {
    "objectID": "tidy01_intro_package.html#prediction",
    "href": "tidy01_intro_package.html#prediction",
    "title": "1  Introduction to Tidy Modelling",
    "section": "1.4 Prediction",
    "text": "1.4 Prediction\nMaking predictions is also standardized and requires the function predict(). The model seem to predict correctly in all cases.\n\niris_test |&gt; \n  select(Species) |&gt; \n  bind_cols(predict(lg_fit,\n                    new_data = iris_test))\n\n# A tibble: 10 × 2\n   Species    .pred_class\n   &lt;fct&gt;      &lt;fct&gt;      \n 1 versicolor versicolor \n 2 versicolor versicolor \n 3 versicolor versicolor \n 4 versicolor versicolor \n 5 versicolor versicolor \n 6 versicolor versicolor \n 7 virginica  virginica  \n 8 virginica  virginica  \n 9 virginica  virginica  \n10 virginica  virginica",
    "crumbs": [
      "Tidy Modelling with R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Tidy Modelling</span>"
    ]
  },
  {
    "objectID": "tidy02_mult_models.html",
    "href": "tidy02_mult_models.html",
    "title": "2  Creating multiple models",
    "section": "",
    "text": "2.1 Cross-validation\nThe reasonings for, and theoretical aspects of, cross-validation (CV) are expected to be already known. In the tidymodels universe, CV is setup as:\niris_folds &lt;- vfold_cv(iris_train,\n                       v = 10)\niris_folds\n\n#  10-fold cross-validation \n# A tibble: 10 × 2\n   splits         id    \n   &lt;list&gt;         &lt;chr&gt; \n 1 &lt;split [81/9]&gt; Fold01\n 2 &lt;split [81/9]&gt; Fold02\n 3 &lt;split [81/9]&gt; Fold03\n 4 &lt;split [81/9]&gt; Fold04\n 5 &lt;split [81/9]&gt; Fold05\n 6 &lt;split [81/9]&gt; Fold06\n 7 &lt;split [81/9]&gt; Fold07\n 8 &lt;split [81/9]&gt; Fold08\n 9 &lt;split [81/9]&gt; Fold09\n10 &lt;split [81/9]&gt; Fold10",
    "crumbs": [
      "Tidy Modelling with R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Creating multiple models</span>"
    ]
  },
  {
    "objectID": "tidy02_mult_models.html#recipes",
    "href": "tidy02_mult_models.html#recipes",
    "title": "2  Creating multiple models",
    "section": "2.2 Recipes",
    "text": "2.2 Recipes\nWhen it comes to modelling, creating and comparing multiple models is essential. This is easily achieved through the use of recipe() from the recipes package. A recipe is a collection of both a fomula and preprocessing steps. Here preprocessing steps is not applied, but could be: log-transforming data, creating dummy variables (e.g. one hot encoding) and sum up low occurring categories to handle class imbalance.\n\nSL_rec &lt;- recipe(Species ~ Sepal.Length,\n                 data = iris_train)\n\nSLW_rec &lt;- recipe(Species ~ Sepal.Length + Sepal.Width,\n                  data = iris_train)\n\nSLW_int_rec &lt;- recipe(Species ~ Sepal.Length + Sepal.Width,\n                      data = iris_train) |&gt;\n  step_interact(~ Sepal.Length:Sepal.Width)\n\nPL_rec &lt;- recipe(Species ~ Petal.Length,\n                 data = iris_train)\n\nPLW_rec &lt;- recipe(Species ~ Petal.Length + Petal.Width,\n                  data = iris_train)\n\nPLW_int_rec &lt;- recipe(Species ~ Petal.Length + Petal.Width,\n                      data = iris_train) |&gt;\n  step_interact(~ Petal.Length:Petal.Width)\n\nrecipe_list &lt;- list(SL = SL_rec,\n                    SLW = SLW_rec,\n                    SLW_int = SLW_int_rec,\n                    PL = PL_rec,\n                    PLW = PLW_rec,\n                    PLW_int = PLW_int_rec)\n\nlg_models &lt;- workflow_set(preproc = recipe_list,\n                          models = list(logistic = logistic_reg()),\n                          cross = FALSE)\n\nEach of the v folds are fitted with a purr-like workflow function:\n\n# To save the predicted values and used workflows\nkeep_pred &lt;- control_resamples(save_pred = TRUE, save_workflow = TRUE)\n\n\nlg_models &lt;- lg_models |&gt; \n  workflow_map(\"fit_resamples\",\n               resamples = iris_folds,\n               control = keep_pred,\n               seed = 1337)\n\n→ A | warning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\n\nThere were issues with some computations   A: x1\nThere were issues with some computations   A: x1\n\n\n\n\nlg_models\n\n# A workflow set/tibble: 6 × 4\n  wflow_id         info             option    result   \n  &lt;chr&gt;            &lt;list&gt;           &lt;list&gt;    &lt;list&gt;   \n1 SL_logistic      &lt;tibble [1 × 4]&gt; &lt;opts[2]&gt; &lt;rsmp[+]&gt;\n2 SLW_logistic     &lt;tibble [1 × 4]&gt; &lt;opts[2]&gt; &lt;rsmp[+]&gt;\n3 SLW_int_logistic &lt;tibble [1 × 4]&gt; &lt;opts[2]&gt; &lt;rsmp[+]&gt;\n4 PL_logistic      &lt;tibble [1 × 4]&gt; &lt;opts[2]&gt; &lt;rsmp[+]&gt;\n5 PLW_logistic     &lt;tibble [1 × 4]&gt; &lt;opts[2]&gt; &lt;rsmp[+]&gt;\n6 PLW_int_logistic &lt;tibble [1 × 4]&gt; &lt;opts[2]&gt; &lt;rsmp[+]&gt;",
    "crumbs": [
      "Tidy Modelling with R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Creating multiple models</span>"
    ]
  },
  {
    "objectID": "tidy02_mult_models.html#evaluation-metrics-across-models",
    "href": "tidy02_mult_models.html#evaluation-metrics-across-models",
    "title": "2  Creating multiple models",
    "section": "2.3 Evaluation metrics across models",
    "text": "2.3 Evaluation metrics across models\nTwo simple evaluation metrics for logistic regressions are the accuracy and Area Under the Curve (AUC), it is implied that it is area under the Receiver Operating Characteristic (ROC) curve. To view these two evaluation metrics:\n\ncollect_metrics(lg_models) |&gt; \n  select(-c(.config, preproc, .estimator)) |&gt; \n  filter(.metric == \"accuracy\") |&gt; \n  arrange(desc(mean))\n\n# A tibble: 6 × 6\n  wflow_id         model        .metric   mean     n std_err\n  &lt;chr&gt;            &lt;chr&gt;        &lt;chr&gt;    &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1 PLW_int_logistic logistic_reg accuracy 0.933    10  0.0246\n2 PL_logistic      logistic_reg accuracy 0.922    10  0.0237\n3 PLW_logistic     logistic_reg accuracy 0.922    10  0.0289\n4 SL_logistic      logistic_reg accuracy 0.733    10  0.0296\n5 SLW_logistic     logistic_reg accuracy 0.722    10  0.0299\n6 SLW_int_logistic logistic_reg accuracy 0.711    10  0.0339\n\ncollect_metrics(lg_models) |&gt; \n  select(-c(.config, preproc, .estimator)) |&gt; \n  filter(.metric == \"roc_auc\") |&gt; \n  arrange(desc(mean))\n\n# A tibble: 6 × 6\n  wflow_id         model        .metric  mean     n std_err\n  &lt;chr&gt;            &lt;chr&gt;        &lt;chr&gt;   &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1 PL_logistic      logistic_reg roc_auc 0.991    10 0.00458\n2 PLW_logistic     logistic_reg roc_auc 0.988    10 0.00825\n3 PLW_int_logistic logistic_reg roc_auc 0.988    10 0.00825\n4 SL_logistic      logistic_reg roc_auc 0.800    10 0.0442 \n5 SLW_logistic     logistic_reg roc_auc 0.787    10 0.0404 \n6 SLW_int_logistic logistic_reg roc_auc 0.776    10 0.0458 \n\n\nTo illustrate the two metrics:\n\nlg_models |&gt; \n  autoplot(metric = \"accuracy\") +\n  geom_label(aes(label = wflow_id)) +\n  theme(legend.position = \"none\") +\n  xlim(c(0.5, 6.5)) +\n  ggtitle(\"Accuracy stratified on model\")\n\n\n\n\n\n\n\nlg_models |&gt; \n  collect_predictions() |&gt; \n  group_by(wflow_id) |&gt; \n  roc_curve(truth = Species,\n            .pred_versicolor) |&gt; \n  autoplot() +\n  ggtitle(\"ROC curve statified on model\")",
    "crumbs": [
      "Tidy Modelling with R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Creating multiple models</span>"
    ]
  },
  {
    "objectID": "tidy02_mult_models.html#resample-to-resample-component-of-variation-consider-remove",
    "href": "tidy02_mult_models.html#resample-to-resample-component-of-variation-consider-remove",
    "title": "2  Creating multiple models",
    "section": "2.4 Resample-to-resample component of variation (consider remove)",
    "text": "2.4 Resample-to-resample component of variation (consider remove)\nAll the models are tested on the same v-folds. In some cases, different models tends to perform well on the same folds - this effect is called a resample-to-resample component of variation. We can numerically investigate these correlations by correlating each model estimates with eachother:\n\nlg_models |&gt; \n  collect_metrics(summarize = FALSE) |&gt; \n  filter(.metric == \"accuracy\") |&gt; \n  select(wflow_id, .estimate, id) |&gt; \n  pivot_wider(id_cols = \"id\",\n              names_from = \"wflow_id\",\n              values_from = \".estimate\") |&gt;\n  select(-id) |&gt; \n  corrr::correlate(quiet = TRUE)\n\n# A tibble: 6 × 7\n  term        SL_logistic SLW_logistic SLW_int_logistic PL_logistic PLW_logistic\n  &lt;chr&gt;             &lt;dbl&gt;        &lt;dbl&gt;            &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;\n1 SL_logistic     NA            0.930            0.627      -0.547        0.0320\n2 SLW_logist…      0.930       NA                0.812      -0.291       -0.0794\n3 SLW_int_lo…      0.627        0.812           NA           0.136        0.112 \n4 PL_logistic     -0.547       -0.291            0.136      NA           -0.180 \n5 PLW_logist…      0.0320      -0.0794           0.112      -0.180       NA     \n6 PLW_int_lo…     -0.0754      -0.187            0.0658     -0.0471       0.927 \n# ℹ 1 more variable: PLW_int_logistic &lt;dbl&gt;\n\n\nThe correlation illustrated:\n\nlg_models |&gt; \n  collect_metrics(summarize = FALSE) |&gt; \n  filter(.metric == \"accuracy\") |&gt; \n  mutate(wflow_id = reorder(wflow_id,\n                            .estimate)) |&gt; \n  ggplot(aes(x = wflow_id,\n             y = .estimate,\n             group = id,\n             color = id)) + \n  geom_line(alpha = .5,\n            linewidth = 1.25) + \n  theme(legend.position = \"none\")",
    "crumbs": [
      "Tidy Modelling with R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Creating multiple models</span>"
    ]
  },
  {
    "objectID": "tidy03_hyperparam.html",
    "href": "tidy03_hyperparam.html",
    "title": "3  Tuning hyperparameters and overfitting",
    "section": "",
    "text": "3.1 What is a hyperparameter?\nA classical and simple example of a hyperparameter is the number of neighbors (k) in a k-nearest neighbors (KNN) algorithm. This is a hyperparameter as it is not estimated during model fitting, but is specified a priori making it impossible to optimize during parameter estimation.",
    "crumbs": [
      "Tidy Modelling with R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Tuning hyperparameters and overfitting</span>"
    ]
  },
  {
    "objectID": "tidy03_hyperparam.html#set-up-tuning",
    "href": "tidy03_hyperparam.html#set-up-tuning",
    "title": "3  Tuning hyperparameters and overfitting",
    "section": "3.2 Set up tuning",
    "text": "3.2 Set up tuning\nIn the tidymodels universe, hyperparameters are marked for tuning in the specifications for a model. To exemplify, both the number of nearest neighbors and a range of weight functions are tuned.\n\nknn_spec &lt;- nearest_neighbor(neighbors = tune(),\n                             weight_func = tune()) |&gt; \n  set_engine(engine = \"kknn\",\n             trace = 0) |&gt; \n  set_mode(\"classification\")\n\nSecondly, the recipe is set up. As no preprocessing is applied (e.g. log-transformation) it is quite simple.\n\nknn_rec &lt;- recipe(Species ~ ., # Use all other columns as predictors\n                  data = iris)\n\nThe specs and recipe is then combined into a workflow:\n\nknn_wflow &lt;- workflow() |&gt; \n  add_model(knn_spec) |&gt; \n  add_recipe(knn_rec)\n\nIt is possible to inspect which hyperparameters are being tuned, check which values that are tested and change those values. This is done through the use of the dials package.\n\n# Check hyperparameters\nknn_spec |&gt; extract_parameter_set_dials()\n\nCollection of 2 parameters for tuning\n\n  identifier        type    object\n   neighbors   neighbors nparam[+]\n weight_func weight_func dparam[+]\n\n# Check values tested\nknn_spec |&gt; extract_parameter_set_dials() |&gt; \n  extract_parameter_dials(\"weight_func\")\n\nDistance Weighting Function  (qualitative)\n\n\n10 possible values include:\n\n\n'rectangular', 'triangular', 'epanechnikov', 'biweight', 'triweight', 'cos', ... \n\n# Change values, save in new object\nknn_params &lt;- knn_spec |&gt;\n  extract_parameter_set_dials() |&gt;\n  update(weight_func = weight_func(c(\"cos\", \"inv\", \"gaussian\")),\n         neighbors = neighbors(c(1, 15)))\n\n# Check that it is updated\nknn_params |&gt;\n  extract_parameter_dials(\"weight_func\")\n\nDistance Weighting Function  (qualitative)\n\n\n3 possible values include:\n\n\n'cos', 'inv' and 'gaussian' \n\nknn_params |&gt;\n  extract_parameter_dials(\"neighbors\")\n\n# Nearest Neighbors (quantitative)\nRange: [1, 15]\n\n\nDifferent grid_* functions exist to combine the hyperparameters, e.g. grid_random() and grid_regular(). As exemplified below, grid_regular() combines the parameters in all possible ways dependent on the number of levels chosen.\n\ngrid_regular(knn_params,\n             levels = 4)\n\n# A tibble: 12 × 2\n   neighbors weight_func\n       &lt;int&gt; &lt;chr&gt;      \n 1         1 cos        \n 2         5 cos        \n 3        10 cos        \n 4        15 cos        \n 5         1 inv        \n 6         5 inv        \n 7        10 inv        \n 8        15 inv        \n 9         1 gaussian   \n10         5 gaussian   \n11        10 gaussian   \n12        15 gaussian",
    "crumbs": [
      "Tidy Modelling with R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Tuning hyperparameters and overfitting</span>"
    ]
  },
  {
    "objectID": "tidy03_hyperparam.html#measure-performance-of-tuning",
    "href": "tidy03_hyperparam.html#measure-performance-of-tuning",
    "title": "3  Tuning hyperparameters and overfitting",
    "section": "3.3 Measure performance of tuning",
    "text": "3.3 Measure performance of tuning\nA metric is needed to measure the performance of the hyperparameters. The ROC curve is used. The regular grid is tuned:\n\n# Performance metric\nroc &lt;- metric_set(roc_auc)\n\n# Tuning\nknn_tune &lt;- knn_wflow |&gt; \n  tune_grid(iris_folds,\n            grid = knn_params |&gt; grid_regular(levels = 4),\n            metrics = roc)\nknn_tune\n\n# Tuning results\n# 10-fold cross-validation \n# A tibble: 10 × 4\n   splits          id     .metrics          .notes          \n   &lt;list&gt;          &lt;chr&gt;  &lt;list&gt;            &lt;list&gt;          \n 1 &lt;split [90/10]&gt; Fold01 &lt;tibble [12 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 2 &lt;split [90/10]&gt; Fold02 &lt;tibble [12 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 3 &lt;split [90/10]&gt; Fold03 &lt;tibble [12 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 4 &lt;split [90/10]&gt; Fold04 &lt;tibble [12 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 5 &lt;split [90/10]&gt; Fold05 &lt;tibble [12 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 6 &lt;split [90/10]&gt; Fold06 &lt;tibble [12 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 7 &lt;split [90/10]&gt; Fold07 &lt;tibble [12 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 8 &lt;split [90/10]&gt; Fold08 &lt;tibble [12 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 9 &lt;split [90/10]&gt; Fold09 &lt;tibble [12 × 6]&gt; &lt;tibble [0 × 3]&gt;\n10 &lt;split [90/10]&gt; Fold10 &lt;tibble [12 × 6]&gt; &lt;tibble [0 × 3]&gt;\n\n\nTo visualize the performance:\n\nknn_tune |&gt; \n  unnest(cols = .metrics) |&gt; \n  select(id, .metric, neighbors, weight_func, .estimate) |&gt;\n  group_by(neighbors, weight_func) |&gt; \n  mutate(estimate_avg = mean(.estimate)) |&gt;\n  ggplot(aes(x = neighbors,\n             y = estimate_avg)) +\n  geom_point() +\n  geom_line() +\n  scale_x_continuous(breaks = c(1, 5, 10, 15)) +\n  facet_wrap(~ weight_func)",
    "crumbs": [
      "Tidy Modelling with R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Tuning hyperparameters and overfitting</span>"
    ]
  },
  {
    "objectID": "tidy03_hyperparam.html#finalize-hyperparameter-selection",
    "href": "tidy03_hyperparam.html#finalize-hyperparameter-selection",
    "title": "3  Tuning hyperparameters and overfitting",
    "section": "3.4 Finalize hyperparameter selection",
    "text": "3.4 Finalize hyperparameter selection\nIt would seem there is no visual difference between the weight functions. For the number of neighbors, the performance is highest for 10 and 15 neighbors. Preferably, the simplest of the two models is chosen.\n\nfinal_hyperparams &lt;- tibble(weight_func = \"gaussian\",\n                            neighbors = 10)\n\nfinal_knn_wflow &lt;- knn_wflow |&gt; \n  finalize_workflow(final_hyperparams)\nfinal_knn_wflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: nearest_neighbor()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nK-Nearest Neighbor Model Specification (classification)\n\nMain Arguments:\n  neighbors = 10\n  weight_func = gaussian\n\nEngine-Specific Arguments:\n  trace = 0\n\nComputational engine: kknn \n\n\nThe model can now be fit to the data and used for prediction.\n\nfinal_knn_fit &lt;- final_knn_wflow |&gt; \n  fit(iris)\nfinal_knn_fit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: nearest_neighbor()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\nkknn::train.kknn(formula = ..y ~ ., data = data, ks = min_rows(10,     data, 5), kernel = ~\"gaussian\", trace = ~0)\n\nType of response variable: nominal\nMinimal misclassification: 0.07\nBest kernel: gaussian\nBest k: 10",
    "crumbs": [
      "Tidy Modelling with R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Tuning hyperparameters and overfitting</span>"
    ]
  },
  {
    "objectID": "stat00_intro.html",
    "href": "stat00_intro.html",
    "title": "Applied bio-statistical methods",
    "section": "",
    "text": "Load packages.\n\n\nShow the code\nlibrary(\"tidymodels\")\ntidymodels::tidymodels_prefer()\n\n\nIn this chapter, each of the following parts delve into an area of statistics. This ranges from biostatistics such as diversity measures and different statistical models such as Ordinary Linear Regression (OLS). The purpose is to introduce the reader to the statistical concepts that are often used in Bioinformatics and Biostatistics. The chapters are structured in a similar way. First, the method is introduced, followed by the math behind it, and lastly, a visual representation using the tidymodels package.\nIn the first part of the book, the iris data set was used to exemplify the usage of the tidymodels package. For the chapters pertaining to diversit measures, a count matrix from a metagenomics study is used instead. The raw data can be obtained from (Cox et al. 2021). Each row is a sample from a person, each column is a genus and the values represent the abundance of said genus in said sample. Explaining how the count matrix is acquired from raw sequences is out of scope for this book. Scripts used for the pre-processing exists on the GitHub page from which this book is created, located in the src/ sub-folder.\nA snippet of the data set can be seen below:\n\ncount_matrix &lt;- readr::read_rds(\"https://github.com/WilliamH-R/BioStatistics/raw/main/data/count_matrix/count_matrix.rds\")\ncount_matrix\n\n# A tibble: 456 × 124\n   Sample      Actinomyces Adlercreutzia Agathobacter Akkermansia Alistipes\n   &lt;chr&gt;             &lt;int&gt;         &lt;int&gt;        &lt;int&gt;       &lt;int&gt;     &lt;int&gt;\n 1 SRR14214860           0            11          190         486       272\n 2 SRR14214861           0            12            0           5      1158\n 3 SRR14214862           0             0            0           0         0\n 4 SRR14214863           4             0          505          94       361\n 5 SRR14214864           0            45          744           3       794\n 6 SRR14214865           0            29          924         933        45\n 7 SRR14214866           0            41         1187         308       145\n 8 SRR14214867           0            45          648         323       193\n 9 SRR14214868           0             0         6973           0       287\n10 SRR14214869           0             0          362         270        30\n# ℹ 446 more rows\n# ℹ 118 more variables: Anaerofustis &lt;int&gt;, Anaerostipes &lt;int&gt;,\n#   Anaerotruncus &lt;int&gt;, Angelakisella &lt;int&gt;, Bacteroides &lt;int&gt;,\n#   Barnesiella &lt;int&gt;, Bifidobacterium &lt;int&gt;, Bilophila &lt;int&gt;, Blautia &lt;int&gt;,\n#   Butyricicoccus &lt;int&gt;, Butyricimonas &lt;int&gt;, `CAG-352` &lt;int&gt;, `CAG-56` &lt;int&gt;,\n#   `Candidatus Soleaferrea` &lt;int&gt;, `Candidatus Stoquefichus` &lt;int&gt;,\n#   Catenibacillus &lt;int&gt;, Catenibacterium &lt;int&gt;, …\n\n\nTo speed up data processing, and to make results less overwhelming, a subset of the count matrix is used at times as test data when exemplifying.\n\ncount_matrix_test &lt;- count_matrix |&gt; \n  slice_head(n = 10) |&gt; \n  select(Sample, Actinomyces,\n         Adlercreutzia, Agathobacter, Akkermansia)\ncount_matrix_test\n\n# A tibble: 10 × 5\n   Sample      Actinomyces Adlercreutzia Agathobacter Akkermansia\n   &lt;chr&gt;             &lt;int&gt;         &lt;int&gt;        &lt;int&gt;       &lt;int&gt;\n 1 SRR14214860           0            11          190         486\n 2 SRR14214861           0            12            0           5\n 3 SRR14214862           0             0            0           0\n 4 SRR14214863           4             0          505          94\n 5 SRR14214864           0            45          744           3\n 6 SRR14214865           0            29          924         933\n 7 SRR14214866           0            41         1187         308\n 8 SRR14214867           0            45          648         323\n 9 SRR14214868           0             0         6973           0\n10 SRR14214869           0             0          362         270\n\n\nMicrobiome data is inherently compositional, meaning that the sum of all genera in a sample is constant, so the count numbers indicate the proportion of a specific genera. As a consequence of compositional data, commonly used statistical tools are not applicable. To avoid this compositionality, the count matrix is transformed using the centered log-ratio (clr) transformation.\n\ncount_matrix_clr &lt;- readr::read_rds(\"https://github.com/WilliamH-R/BioStatistics/raw/main/data/count_matrix/count_matrix_clr.rds\")\ncount_matrix_clr\n\n# A tibble: 456 × 124\n   Sample      Actinomyces Adlercreutzia Agathobacter Akkermansia Alistipes\n   &lt;chr&gt;             &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n 1 SRR14214860        0           -1.83          1.02       1.96      1.38 \n 2 SRR14214861        0           -2.11          0         -2.98      2.46 \n 3 SRR14214862        0            0             0          0         0    \n 4 SRR14214863       -3.41         0             1.43      -0.255     1.09 \n 5 SRR14214864        0           -0.495         2.31      -3.20      2.38 \n 6 SRR14214865        0           -1.52          1.94       1.95     -1.08 \n 7 SRR14214866        0           -0.835         2.53       1.18      0.428\n 8 SRR14214867        0           -0.581         2.09       1.39      0.876\n 9 SRR14214868        0            0             3.85       0         0.658\n10 SRR14214869        0            0             1.39       1.10     -1.10 \n# ℹ 446 more rows\n# ℹ 118 more variables: Anaerofustis &lt;dbl&gt;, Anaerostipes &lt;dbl&gt;,\n#   Anaerotruncus &lt;dbl&gt;, Angelakisella &lt;dbl&gt;, Bacteroides &lt;dbl&gt;,\n#   Barnesiella &lt;dbl&gt;, Bifidobacterium &lt;dbl&gt;, Bilophila &lt;dbl&gt;, Blautia &lt;dbl&gt;,\n#   Butyricicoccus &lt;dbl&gt;, Butyricimonas &lt;dbl&gt;, `CAG-352` &lt;dbl&gt;, `CAG-56` &lt;dbl&gt;,\n#   `Candidatus Soleaferrea` &lt;dbl&gt;, `Candidatus Stoquefichus` &lt;dbl&gt;,\n#   Catenibacillus &lt;dbl&gt;, Catenibacterium &lt;dbl&gt;, …\n\n\n\n\n\n\nCox, Laura M., Amir Hadi Maghzi, Shirong Liu, Stephanie K. Tankou, Fyonn H. Dhang, Valerie Willocq, Anya Song, et al. 2021. “Gut Microbiome in Progressive Multiple Sclerosis.” Annals of Neurology 89 (June): 1195–1211. https://doi.org/10.1002/ANA.26084.",
    "crumbs": [
      "Applied bio-statistical methods"
    ]
  },
  {
    "objectID": "stat01_diversity.html",
    "href": "stat01_diversity.html",
    "title": "4  Diversity measures",
    "section": "",
    "text": "4.1 Alpha Diversity\nGenerally, alpha diversity measures the within-sample diversity. Multiple ways of calculating alpha diversity exists. A crude way is simply to count unique organisms per sample. One could also take the distribution into account or the phylogeny.\nThe R package vegan is widely used to help calculate diversity and is used here.",
    "crumbs": [
      "Applied bio-statistical methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Diversity measures</span>"
    ]
  },
  {
    "objectID": "stat01_diversity.html#alpha-diversity",
    "href": "stat01_diversity.html#alpha-diversity",
    "title": "4  Diversity measures",
    "section": "",
    "text": "4.1.1 Species Richness\nAs mentioned above, a crude way to measure diversity is to count the number of unique organisms in a given taxon for each sample. That is exactly what the species richness is. Through the vegan package, it can be calculated as follows:\n\nrichness &lt;- count_matrix_test |&gt;\n  column_to_rownames(var = \"Sample\") |&gt; \n  specnumber() |&gt;\n  as_tibble(rownames = \"Sample\") |&gt;\n  rename(Richness = value)\n\ncount_matrix_test |&gt;\n  left_join(richness,\n            by = \"Sample\")\n\n# A tibble: 10 × 6\n   Sample      Actinomyces Adlercreutzia Agathobacter Akkermansia Richness\n   &lt;chr&gt;             &lt;int&gt;         &lt;int&gt;        &lt;int&gt;       &lt;int&gt;    &lt;int&gt;\n 1 SRR14214860           0            11          190         486        3\n 2 SRR14214861           0            12            0           5        2\n 3 SRR14214862           0             0            0           0        0\n 4 SRR14214863           4             0          505          94        3\n 5 SRR14214864           0            45          744           3        3\n 6 SRR14214865           0            29          924         933        3\n 7 SRR14214866           0            41         1187         308        3\n 8 SRR14214867           0            45          648         323        3\n 9 SRR14214868           0             0         6973           0        1\n10 SRR14214869           0             0          362         270        2\n\n\n\n\n4.1.2 Shannon Index\nThe measure is usually referred to as Shannon Index or Shannon Entropy. It is able to take both the richness (described above) and the evenness into account. The evenness refers to how evenly the abundances are distributed among taxa for a species. The richness is considered when summing over all taxonomic units at a specified taxonomic rank. The index is calculated via the following formula:\n\\[\nH' = -\\sum_{i=1}^{R} p_i \\ln(p_i)\n\\]\n\\(R\\) is the number of observed species within a sample, i.e. the Richness. \\(p_{i}\\) is the proportion of the abundance belonging to the \\(i\\)’th species out of the total abundance for a specific sample(Shannon 1948).\nTo illustrate the meaning of the value of the Shannon Index a few examples are calculated. In the first scenario, three species and three samples exists. For the first sample, the proportion of abundances are spread evenly between the three species. For the second and third sample, the proportions are more skewered towards one species. These examples show how the evenness affect the Shannon Index.\n\ntribble(~sample,    ~species_1, ~species_2, ~species_3,\n        \"sample_A\", 0.3,        0.3,        0.3,\n        \"sample_B\", 0.6,        0.2,        0.2,\n        \"sample_C\", 1,          0,          0)\n\nFor each of the three samples, the calculation would be as follows:\n\\[\nH'_{sample_{A}} = -(0.3 \\cdot ln{(0.3)} + 0.3 \\cdot ln{(0.3)} + 0.3 \\cdot ln{(0.3)}) = 1.08\n\\]\n\\[\nH'_{sample_{B}} = -(0.6 \\cdot ln{(0.6)} + 0.2 \\cdot ln{(0.2)} + 0.2 \\cdot ln{(0.2)}) = 0.95\n\\]\n\\[\nH'_{sample_{C}} = -(1 \\cdot ln{(1)}) = 0\n\\]\nWhen the samples are evenly distributed, we observe a higher Shannon Index.\nIn the following example, the number of species are increased to five. Three different samples with different distributions are still investigated.\n\ntribble(~sample,    ~species_1, ~species_2, ~species_3, ~species_4, ~species_5,\n        \"sample_D\", 0.2,        0.2,        0.2,        0.2,        0.2,\n        \"sample_E\", 0.6,        0.1,        0.1,        0.1,        0.1,\n        \"sample_F\", 1,          0,          0,          0,          0)\n\n\\[\nH'_{sample_{D}} = 1.61\n\\]\n\\[\nH'_{sample_{E}} = 1.23\n\\]\n\\[\nH'_{sample_{F}} = 0\n\\]\nAs before, the Shannon Index decreases with a more skewered distribution. Both \\(sample_{A}\\) and \\(sample_{D}\\) have even distributions, but the Shannon Index for \\(sample_{D}\\) higher since the richness is higher, i.e. more species with an even distribution.\nThe diversity in a sample is said to be high when the number of species are high, and the distribution is even. As shown above, that would yield a high Shannon Index. So, to conclude, a high Shannon Index suggests high diversity.\nFinally, Shannon Index calculated for the test set. The index does seem to be higher when the genera are more evenly distributed as show in the above.\n\nshannon &lt;- count_matrix_test |&gt;\n  column_to_rownames(var = \"Sample\") |&gt;\n  diversity(index = \"shannon\") |&gt; \n  as_tibble(rownames = \"Sample\") |&gt; \n  rename(Shannon = value)\n\ncount_matrix_test |&gt;\n  left_join(shannon,\n            by = \"Sample\")\n\n# A tibble: 10 × 6\n   Sample      Actinomyces Adlercreutzia Agathobacter Akkermansia Shannon\n   &lt;chr&gt;             &lt;int&gt;         &lt;int&gt;        &lt;int&gt;       &lt;int&gt;   &lt;dbl&gt;\n 1 SRR14214860           0            11          190         486   0.667\n 2 SRR14214861           0            12            0           5   0.606\n 3 SRR14214862           0             0            0           0   0    \n 4 SRR14214863           4             0          505          94   0.472\n 5 SRR14214864           0            45          744           3   0.243\n 6 SRR14214865           0            29          924         933   0.762\n 7 SRR14214866           0            41         1187         308   0.618\n 8 SRR14214867           0            45          648         323   0.789\n 9 SRR14214868           0             0         6973           0   0    \n10 SRR14214869           0             0          362         270   0.683\n\n\n\n\n4.1.3 Faith’s phylogenetic diversity (Faith’s PD)\nFaith’s PD consider phylogenetic distances within a sample. The PD for a sample is calculated as the total sum of branch lengths from a phylogenetic tree, where the tree only contains the organisms found in a given sample (Faith 1992). Imagine a scenario with four organisms, organism a and b are closely related, and so are organism c and d (Figure 4.1).\n\n\n\n\n\n\nFigure 4.1: Phylogenetic tree\n\n\n\nIn the case where all four organisms were found in the same sample, the PD would be calculated as:\n\\[\nPD = 15 + 6 + 4 + 3 + 1 + 2 + 4 = 35\n\\]\nIf an organism a was lost, the PD would be decreased with 4 units to a value of 31. A higher PD is thus indicative of a higher diversity.",
    "crumbs": [
      "Applied bio-statistical methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Diversity measures</span>"
    ]
  },
  {
    "objectID": "stat01_diversity.html#beta-diversity",
    "href": "stat01_diversity.html#beta-diversity",
    "title": "4  Diversity measures",
    "section": "4.2 Beta Diversity",
    "text": "4.2 Beta Diversity\nBeta diversity describes the between-sample diversity. The diversity is calculated by comparing the diversity in one sample to the diversity in another sample. The diversity can be calculated in different ways, and the choice of method is dependent on the data and the question at hand. The most common methods include Bray-Curtis dissimilarity, UniFrac, and Jaccard similarity index (sometimes called Jaccard similarity coefficient).\nA crude way to calculate beta diversity is to simply count the number of organisms which exists in one sample, but not the other. The mathematical expression is:\n\\[\n\\beta =  (\\alpha_{sample_{1}} - c) + (\\alpha_{sample_{2}} - c)\n\\]\nWhere \\(\\alpha_{sample_{1}}\\) and \\(\\alpha_{sample_{2}}\\) are the number of unique organisms in sample 1 and 2 (the richness), respectively. \\(c\\) is the number of organisms which exists in both samples.\nAs an example, consider the following two samples (the top two rows of the test set):\n\ncount_matrix_test |&gt;\n  slice_head(n = 2)\n\n# A tibble: 2 × 5\n  Sample      Actinomyces Adlercreutzia Agathobacter Akkermansia\n  &lt;chr&gt;             &lt;int&gt;         &lt;int&gt;        &lt;int&gt;       &lt;int&gt;\n1 SRR14214860           0            11          190         486\n2 SRR14214861           0            12            0           5\n\n\nSample 1 (SRR14214860 ) has a richness of 3 and sample 2 (SRR14214861) has a richness of 2. They share 2 genera. The beta diversity would be calculated as:\n\\[\n\\beta = (3 - 2) + (2 - 2) = 1\n\\]\n\n4.2.1 Jaccard similarity index\nThe Jaccard index is another way to calculate beta diversity where only the presence/absence of an organism is taken into account. It is calculated as the number of shared organisms (intersection) divided by the number of unique organisms (union) in the two samples. The equation is as follows::\n\\[\nJ(A, B) = \\frac{|A \\cap B|}{|A \\cup B|}\n\\]\nFrom the equation we see why it is called a similarity index. The union (denominator) is just the number of observed organisms, and the intersection (numerator) is the number of shared organisms. The Jaccard index then represent the fraction of the observed organisms which were observed in both samples, i.e. the similarity of the two samples. The value ranges from 0 to 1, where 0 indicates that the two samples do not share any organisms, and 1 indicates that the two samples are identical.\nWhen comparing the two first samples in the subset, it is apparent that they share 2 genera, and a total of 3 genera is present between the two samples. The calculation by hand gives:\n\\[\nJ(SRR14214860, SRR14214861) = \\frac{2}{3} \\approx 0.67\n\\]\nThe two samples are only somewhat similar, as the Jaccard index is close to 1.\nTo calculate the Jaccard index by using the vegan package, the argument binary is set to TRUE to indicate that the data is binary, i.e. presence/absence of organisms instead of abundance. It is also important to note, that the vegan package actually calculates the dissimilarities, \\(1 - \\frac{|A \\cap B|}{|A \\cup B|}\\) so a higher value indicates a higher dissimilarity. The Jaccard index is calculated for the test data as follows:\n\ncount_matrix_test |&gt;\n  slice_head(n = 2) |&gt;\n  column_to_rownames(var = \"Sample\")|&gt;\n  vegdist(method = \"jaccard\",\n          binary = TRUE)\n\n            SRR14214860\nSRR14214861   0.3333333\n\n\nThe result is in agreement with the manual calculation since the vegan package calculates the dissimilarity. When inspecting the two samples, they also appear somewhat similar, as the majority of the organisms are present in both sample (even though talking about a majority is a bit misleading when only four organisms are compared). The abundances of the organisms are quite different, and to take that into account, another method is needed.\n\n\n4.2.2 Bray-Curtis dissimilarity\nThis measure is a widely used method to calculate beta diversity. Bray-Curtis dissimilarity takes the composition and abundance of the two compared samples into account. It is also important to note that it is a dissimilarity measure, so a higher value indicates a higher dissimilarity. The value ranges from 0 to 1. A value of 0 indicates that the two samples are identical, while a value of 1 indicates that the two samples do not share any organisms. The metric is calculated as:\n\\[\nBC_{jk} = \\frac{\\sum_{i=1}^{p} |S_{ij} - S_{ik}|}{\\sum_{i=1}^{p} (S_{ij} + S_{ik})}\n\\]\nWhere \\(S_{ij}\\) and \\(S_{ik}\\) are the abundances of the \\(i\\)’th taxon in sample \\(j\\) and \\(k\\), respectively. \\(p\\) is the number of taxa. With the test data, the Bray-Curtis dissimilarity is calculated as follows:\n\\[\nBC_{SRR14214860, SRR14214861} = \\frac{|0 - 0| + |11 - 12| + |190 - 0| + |486 - 5|}\n                                     {(0 + 0) + (11 + 12) + (190 + 0) + (486 + 5)}\n                                     = 0.955\n\\]\nBy using the vegan package the Bray-Curtis dissimilarity can be calculated as follows:\n\ncount_matrix_test |&gt;\n  slice_head(n = 2) |&gt;\n  column_to_rownames(var = \"Sample\")|&gt;\n  vegdist(method = \"bray\")\n\n            SRR14214860\nSRR14214861   0.9545455\n\n\nThe results are in agreement. The value is close to 1, indicating a high dissimilarity between the two samples. Interestingly, this is a different find compared to the Jaccard index, which indicated a higher similarity between the two samples. The abundances of the organisms in the two samples are quite different, and the Bray-Curtis dissimilarity takes that into account.\n\n\n4.2.3 UniFrac\nThe method is based on the phylogenetic tree of the organisms and thereby take the phylogenetic distances into account. UniFrac is shorthand notation for Unique Fraction and is calculated as the sum of unique branch lengths in the phylogenetic tree over the sum of shared branch lengths:\n\\[\nU = \\frac{sum \\; of \\; unique \\; branches' \\; length}{sum \\; of \\; all \\; branches'  \\; length}\n\\]\nThe possible values range from 0 to 1. In the case where the two compared samples share all organisms, the value of the numerator would sum to 0, and the UniFrac would be 0. If the two samples do not share any organisms, the value of the numerator would sum to the total length of the phylogenetic tree (the denominator), and the UniFrac would be 1.\nImagine a phylogenetic tree made from two samples, red and blue. Four different organisms were found in total (Figure 4.2). A red colour indicate an organism found in the red sample, a blue colour indicate an organism found in the blue sample and a purple colour indicate an organism was found in both samples.\n\n\n\n\n\n\nFigure 4.2: Phylogenetic tree with cohorts\n\n\n\nFollowing the equation above, the UniFrac distance would be calculated as:\n\\[\nU = \\frac{4 + 2 + 4}{15 + 6 + 4 + 3 + 1 + 2 + 4} = 0.286\n\\]\nThe value indicates that the blue and red samples are quite similar.\nThe method is divided into two different types, unweighted and weighted. It is the unweighted method which is described above. The weighted method takes the abundance of the organisms into account. The weighted method biases towards the most abundant organisms, while the unweighted method biases more towards the rare organisms. A third version exists, called Generalized UniFrac. This metric has another parameter, usually called \\(\\alpha\\), which can be used to control the balance between the unweighted and weighted methods.\n\n\n\n\nFaith, Daniel P. 1992. “Conservation Evaluation and Phylogenetic Diversity.” Biological Conservation 61 (January): 1–10. https://doi.org/10.1016/0006-3207(92)91201-3.\n\n\nShannon, C. E. 1948. “A Mathematical Theory of Communication.” Bell System Technical Journal 27: 623–56. https://doi.org/10.1002/J.1538-7305.1948.TB00917.X.",
    "crumbs": [
      "Applied bio-statistical methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Diversity measures</span>"
    ]
  },
  {
    "objectID": "stat02_wilcoxon.html",
    "href": "stat02_wilcoxon.html",
    "title": "5  Wilcoxon test",
    "section": "",
    "text": "Set seed and load packages.\n\n\nShow the code\nset.seed(1337)\n\nlibrary(\"tidymodels\")\ntidymodels::tidymodels_prefer()\nlibrary(\"vegan\")\n\n\nLoad data.\n\n\nShow the code\ncount_matrix &lt;- readr::read_rds(\"https://github.com/WilliamH-R/BioStatistics/raw/main/data/count_matrix/count_matrix.rds\") |&gt; \n  select(-\"NA\")\n\nmeta &lt;- read.csv(file = \"data/metadata.txt\") |&gt; \n  as_tibble() |&gt;\n  select(Run, chem_administration, ETHNICITY, geo_loc_name,\n         Host_age, host_body_mass_index, Host_disease, host_phenotype, host_sex) |&gt; \n  rename(Sample = Run,\n         Treatment = chem_administration,\n         Ethnicity = ETHNICITY,\n         Location = geo_loc_name,\n         Age = Host_age,\n         BMI = host_body_mass_index,\n         Disease_severity = Host_disease,\n         EDSS = host_phenotype,\n         Sex = host_sex) |&gt;\n  mutate(Patient_status = case_when(Disease_severity == \"1HealthyControl\" ~ \"Healthy\",\n                                    TRUE ~ \"MS\"),\n         EDSS = as.factor(EDSS),\n         EDSS = case_when(is.na(EDSS) & Disease_severity == \"1HealthyControl\" ~ \"-1\",\n                          is.na(EDSS) & Disease_severity != \"1HealthyControl\" ~ \"Unknown\",\n                          TRUE ~ EDSS),\n         EDSS = as.factor(EDSS))\n\n\nA plethora of statistical tests exists. The choice of test depends on the type of data. For example, if the data is normally distributed, the t-test can be used to compare the means of two groups. If the data is not normally distributed, the Wilcoxon test can be used instead. Below, the Shannon Index for the full data set have been calculated and plotted stratified by the patient status, Healthy vs Multiple Sclerosis (MS). As can be seen from the plots, the Shannon Index does not seem to follow a normal distribution.\n\nshannon &lt;- count_matrix |&gt;\n  column_to_rownames(var = \"Sample\") |&gt;\n  diversity(index = \"shannon\") |&gt; \n  as_tibble(rownames = \"Sample\") |&gt; \n  rename(Shannon = value) |&gt; \n  left_join(meta,\n            by = \"Sample\")\n\nshannon |&gt;\n  ggplot(aes(x = Shannon,\n             fill = Patient_status)) +\n  geom_density(alpha = 0.7) +\n  facet_wrap(~ Patient_status)\n\n\n\n\n\n\n\n\nThe Wilcoxon test is a non-parametric test, which means it do not assume that the samples have been taken from a specific distribution, e.g. the normal distribution. Two different Wilcoxon tests exists, one for comparing dependent samples and one for comparing independent samples. Our samples would be dependent if they were taken from the same patient at different times. Since we are comparing different patients, our samples are independent. Hence, the specific Wilcoxon test we will use is the Wilcoxon Rank Sum test, also called the Mann-Whitney U test.\nThe test is rank-based, which means that all the data is ranked and are then used to calculate the test statistic, in this case the U statistic. After ranking the data, the sum of the ranks for the two groups are calculated. U is then calculated as:\n\\[\nU = min(U_1, \\; U_2) = min(R_1 - \\frac{n_1(n_1+1)} {2}, \\; R_2 - \\frac{n_2(n_2+1)} {2})\n\\]\nWhere \\(R_1\\) and \\(R_2\\) is the sum of the ranks for the first and second group, respectively. Likewise, \\(n_1\\) and \\(n_2\\) is the number of samples in the first and second group, respectively. The U statistic is then used to find the p-value, which is used to determine if the difference between the two groups is statistically significant. The p-value is the probability of observing a U statistic as extreme as the one observed, given that the null hypothesis of no difference between the distributions of \\(U_1\\) and \\(U_2\\) is true. Due to the Central Limit Theorem, the distributions of \\(U_1\\) and \\(U_2\\), and thereby \\(U\\), can be assumed to follow a normal distribution when the sample size is larger than approximately 20. The Wilcoxon Rank Sum test for the Shannon Index is shown below. As the p-value is much lower than the usual significance level of 0.05, we can conclude that the Shannon Index is significantly different between the two groups. Here, the two groups are the healthy and MS patients.\n\nwilcox.test(Shannon ~ Patient_status,\n            data = shannon |&gt; filter(Sex == \"male\")) |&gt; \n  broom::tidy()\n\n# A tibble: 1 × 4\n  statistic p.value method                                           alternative\n      &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                                            &lt;chr&gt;      \n1       450 0.00928 Wilcoxon rank sum test with continuity correcti… two.sided",
    "crumbs": [
      "Applied bio-statistical methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Wilcoxon test</span>"
    ]
  },
  {
    "objectID": "stat03_mult_testing.html",
    "href": "stat03_mult_testing.html",
    "title": "6  Multiple testing correction",
    "section": "",
    "text": "6.1 FWER correction (Bonferroni)\nThe Bonferroni correction is a conservative method, which means that it is less likely to find significant results. To apply Bonferroni correction, the significance level is divided by the number of tests, which would then also affect the FWER (the probability of finding at least one false positive among the significant results). The Bonferroni adjusted significance level is then:\nalpha_bonferroni &lt;- alpha / nrow(p_values_per_group)\nalpha_bonferroni\n\n[1] 0.003125\nUsing this, we actually find no significant results:\np_values_per_group |&gt; \n  filter(p_value &lt; alpha_bonferroni)\n\n# A tibble: 0 × 3\n# ℹ 3 variables: Group Name &lt;chr&gt;, Group Value &lt;chr&gt;, p_value &lt;dbl&gt;\nThe FWER becomes:\n\\[\nFWER = 1 - (1 - \\frac{\\alpha}{m})^{m} = 1 - (1 - \\frac{0.05}{16})^{16} = 0.049\n\\]\nThe FWER is reduced from 56% to 4.9%, close to the original significance level of 5%. It is then expected to find fewer false positives, also called Type I errors. However, the chance of finding false negatives also increases (referred to as Type II errors). In this case, finding false negatives means accepting the null hypothesis when it was actually false, i.e. not finding a significant result when one exists. In some cases, using a very conservative method is preferred, especially when the cost of a false positive is high.",
    "crumbs": [
      "Applied bio-statistical methods",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Multiple testing correction</span>"
    ]
  },
  {
    "objectID": "stat03_mult_testing.html#false-discovery-rate-correction-benjamini-hochberg",
    "href": "stat03_mult_testing.html#false-discovery-rate-correction-benjamini-hochberg",
    "title": "6  Multiple testing correction",
    "section": "6.2 False Discovery Rate correction (Benjamini-Hochberg)",
    "text": "6.2 False Discovery Rate correction (Benjamini-Hochberg)\nAnother way to control for multiple testing is through the False Discovery Rate (FDR). The FDR is the proportion of false positives among the significant results:\n\\[\nFDR = \\frac{FP}{FP + TP}\n\\]\nWhere \\(FP = False Positive\\) and \\(TP = True Positive\\). FDR is then an expression for the proportion of false positives among the significant results.\nIn some cases, a less conservative method than the Bonferroni correction is preferred allowing for more significant hits. This comes at a cost of a increase in false positives. The Benjamini-Hochberg Procedure is such a method - a type of False Discovery Rate (FDR) correction.\nTwo ways to implement the method exists. Either the significance level is adjusted, or the p-values are adjusted, both giving the same significant hits. The latter is the most common way, but both are introduced.\nIn general, each p-value is compared to a moving significance level instead of compared to a static value of \\(\\alpha\\), e.g. \\(0.05\\). The moving significance level is dependent on the rank of the p-value and the number of tests. The lowest p-value gets rank 1, the second lowest rank 2, and so on. With a higher rank, the significance level increases, allowing for more significant hits.\n\n6.2.1 Adjusting the significance level\nAll p-values are sorted in ascending order and ranked as mentioned above. A critical value for each rank is calculated by:\n\\[\n\\text{Critical Value} = \\frac{k}{m} \\cdot \\alpha\n\\]\nWhere \\(m\\) is the number of tests, \\(k\\) is the rank of the considered p-value and \\(\\alpha\\) is the significance level. The largest value of \\(k\\) where the p-value is less than the critical value is found:\n\\[\n\\max_{k} \\left( p_{(k)} \\leq Critical \\; Value \\right) = \\max_{k} \\left( p_{(k)} \\leq \\frac{k}{m} \\cdot \\alpha \\right)\n\\]\nAll p-values with a rank less than or equal to the largest \\(k\\) are then considered significant. Note that is is irrelevant if a p-value of lower rank than the rank found from the above, is larger than the critical value. It is still considered significant.\nAs for the Bonferroni correction, no significant results are found:\n\np_values_per_group |&gt; \n  arrange(p_value) |&gt; \n  mutate(rank = row_number(),\n         critical_value = (rank / nrow(p_values_per_group)) * alpha,\n         significant = p_value &lt;= critical_value)\n\n# A tibble: 16 × 6\n   `Group Name` `Group Value` p_value  rank critical_value significant\n   &lt;chr&gt;        &lt;chr&gt;           &lt;dbl&gt; &lt;int&gt;          &lt;dbl&gt; &lt;lgl&gt;      \n 1 Age_group    30-36.6        0.0192     1        0.00312 FALSE      \n 2 BMI_group    25.77-28.51    0.0249     2        0.00625 FALSE      \n 3 Age_group    63.2+          0.0444     3        0.00938 FALSE      \n 4 BMI_group    23.77-25.77    0.0449     4        0.0125  FALSE      \n 5 Age_group    51.2-53        0.128      5        0.0156  FALSE      \n 6 Age_group    55.7-59.3      0.133      6        0.0188  FALSE      \n 7 BMI_group    33.31+         0.239      7        0.0219  FALSE      \n 8 Age_group    48.6-51.2      0.246      8        0.025   FALSE      \n 9 Age_group    36.6-41        0.277      9        0.0281  FALSE      \n10 BMI_group    17-21.7        0.317     10        0.0312  FALSE      \n11 Age_group    53-55.7        0.562     11        0.0344  FALSE      \n12 Age_group    41-45          0.681     12        0.0375  FALSE      \n13 BMI_group    21.7-23.77     0.843     13        0.0406  FALSE      \n14 Age_group    45-48.6        0.901     14        0.0438  FALSE      \n15 BMI_group    28.51-33.31    0.955     15        0.0469  FALSE      \n16 Age_group    59.3-63.2      0.978     16        0.05    FALSE      \n\n\n\n\n6.2.2 Adjusting the p-values\nThe p-values are again sorted and ranked exactly as before. Apply the following formula to each p-value as an intermediary step:\n\\[\np\\textrm{-}value = p_{k} \\cdot \\frac{m}{k}\n\\]\nWhere \\(m\\) is the number of tests, \\(k\\) is the rank of the considered p-value and \\(p_{k}\\) is the p-value of said rank. The adjusted p-values can then be found.\nStarting with the highest rank, the adjusted p-value is simply the intermediary value found from the above. For the second highest rank, the adjusted p-value is the minimum of the intermediary value and the adjusted p-value of the rank above. This is repeated for all ranks. The adjusted p-values are then compared to the significance level \\(\\alpha\\), and all p-values less than the significance level are considered significant. Once again, no significant results are found:\n\np_values_per_group |&gt; \n  mutate(p_value_adjusted = p.adjust(p = p_value,\n                                     method = \"BH\"),\n         significant = p_value_adjusted &lt; 0.05)\n\n# A tibble: 16 × 5\n   `Group Name` `Group Value` p_value p_value_adjusted significant\n   &lt;chr&gt;        &lt;chr&gt;           &lt;dbl&gt;            &lt;dbl&gt; &lt;lgl&gt;      \n 1 Age_group    30-36.6        0.0192            0.180 FALSE      \n 2 Age_group    36.6-41        0.277             0.492 FALSE      \n 3 Age_group    41-45          0.681             0.908 FALSE      \n 4 Age_group    45-48.6        0.901             0.978 FALSE      \n 5 Age_group    48.6-51.2      0.246             0.492 FALSE      \n 6 Age_group    51.2-53        0.128             0.356 FALSE      \n 7 Age_group    53-55.7        0.562             0.818 FALSE      \n 8 Age_group    55.7-59.3      0.133             0.356 FALSE      \n 9 Age_group    59.3-63.2      0.978             0.978 FALSE      \n10 Age_group    63.2+          0.0444            0.180 FALSE      \n11 BMI_group    17-21.7        0.317             0.507 FALSE      \n12 BMI_group    21.7-23.77     0.843             0.978 FALSE      \n13 BMI_group    23.77-25.77    0.0449            0.180 FALSE      \n14 BMI_group    25.77-28.51    0.0249            0.180 FALSE      \n15 BMI_group    28.51-33.31    0.955             0.978 FALSE      \n16 BMI_group    33.31+         0.239             0.492 FALSE",
    "crumbs": [
      "Applied bio-statistical methods",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Multiple testing correction</span>"
    ]
  },
  {
    "objectID": "stat04_OLS.html",
    "href": "stat04_OLS.html",
    "title": "7  Ordinary Least Squares Regression",
    "section": "",
    "text": "Set seed and load packages.\n\n\nShow the code\nset.seed(1337)\n\nlibrary(\"tidymodels\")\ntidymodels::tidymodels_prefer()\n\n\nLoad data.\n\n\nShow the code\nmtcars &lt;- mtcars |&gt; \n  as_tibble()\n\n\nA wide range of models for predicting data exists. One of the most common models is the linear regression model. In this case, the model is called the Ordinary Least Squares (OLS) which is a type of linear regression model.\nThe general idea is to fit a line to the data, such that the sum of the squared residuals is minimized, hence the name least squares. Residuals are the difference between the actual value and the predicted value of the model.\nAnother famously used data set is the mtcars. Below is an example of a scatter plot of the miles pr gallon (mpg) vs the weight of the car (wt). By fitting a linear regression model, the goal is to predict the miles pr gallon based on the weight of the car. For an initial guess, the mean of the miles pr gallon is used and shown as the red dashed line. The residuals are the distance between the actual value (dots) and the predicted value (red dashed line). The sum of the residuals is then used as a measure of how well the model fits the data. As some predicted values are below the actual value, and some values are above, these can counter balance each other, such that the sum of the residuals is 0. To avoid this, the residuals are squared, such that all residuals are positive. Further, by squaring the residuals the bigger residuals have a larger effect on the sum of the residuals squared. Since the goal is to minimize the sum of the residuals squared, the bigger residuals are more important to reduce.\n\nmpg_mean &lt;- mtcars |&gt; \n  pull(mpg) |&gt; \n  mean()\n\nmtcars |&gt; \n  ggplot(aes(x = wt,\n             y = mpg)) +\n  geom_point(size = 3,\n             colour = \"steelblue\") +\n  geom_hline(yintercept = mpg_mean,\n             colour = \"firebrick\",\n             linetype = \"dashed\") +\n  labs(title = \"Miles pr Gallon vs Weight\",\n       x = \"Weight (1000 lbs)\",\n       y = \"Miles pr Gallon\",\n       colour = \"Miles per gallon\")\n\n\n\n\n\n\n\n\nIn this simple case, the model is fitting the line to the data by changing the slope and intercept of the line. The combination which minimizes the sum of the squared residuals (SSR) is chosen. This results in a three dimensional space, where the slope and intercept are plotted along the x and y axis, and the sum of the squared residuals are plotted along the z-axis. Some combination of slope and intercept results in a minimum in the 3D plane. This combination can be found by taking the derivative of the sum of the squared residuals with respect to the slope and intercept, and set it equal to zero.\nThe model can be generalized to more than one variable, such that the model can predict the outcome based on multiple variables. Usually, the simple linear case is written as \\(y = ax + b\\) where \\(a\\) is the slope and \\(b\\) is the intercept. When using multiple variables, it is generalized to \\(y = \\beta_0 + \\beta_{1}x_1 + \\beta_{2}x_2 + ... + \\beta_{n}x_n\\). In matrix form it can be written as \\(y = X\\beta\\) where \\(X\\) is a matrix with \\(n\\) rows (number of samples) and \\(p\\) columns (number of variables). \\(\\beta\\) is a vector of length \\(p\\) which includes a parameter for each variable.\nAs mentioned earlier, the goal is to minimize the sum of the squared residuals. On matrix form, SSR is written as:\n\\[\nSSR = ||y-X\\beta||_{2}^{2}\n\\]\nWhere the subscript 2 denotes the L2 norm, i.e. Euclidean distance. The term \\(X\\beta\\) is the predicted value of the model as we saw from \\(y = X\\beta\\) in the above. SSR is also written as:\n\\[\nSSR = \\sum_{i=1}^{n} (y_i - X_{i}\\beta)^2 =  \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n\\]\nWhere \\(\\hat{y}_i\\) is the predicted value of the model for the \\(i\\)’th sample. Since the goal is to minimize the SSR, the \\(\\beta\\)s should be chosen such that the SSR is minimized:\n\\[\n\\beta_{OLS} = \\min_{\\beta} SSR = \\min_{\\beta} ||y-X\\beta||_{2}^{2}\n\\]\nAs mentioned in the above, the \\(\\beta\\)s can be found by taking the derivative of the SSR with respect to \\(\\beta\\) and set it equal to zero. Since this is generalized to multiple variables, the gradient is taken with respect to the vector \\(\\beta\\) instead. The equation is then:\n\\[\n\\nabla_\\beta||y-X\\beta||_{2}^{2} = 0\n\\]",
    "crumbs": [
      "Applied bio-statistical methods",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Ordinary Least Squares Regression</span>"
    ]
  },
  {
    "objectID": "stat05_log_reg.html",
    "href": "stat05_log_reg.html",
    "title": "8  Logistic Regression",
    "section": "",
    "text": "Set seed and load packages.\n\n\nShow the code\nset.seed(1337)\n\nlibrary(\"tidymodels\")\ntidymodels::tidymodels_prefer()\nlibrary(\"MASS\")\n\n\nLoad data.\n\n\nShow the code\ndata(\"iris\")\niris &lt;- iris |&gt;\n  tibble::as_tibble() |&gt; \n  filter(Species != \"setosa\") |&gt; \n  droplevels()\n\n\nOpposite Ordinary Least Squares (OLS), which predicts a continuous value, logistic regression predicts a probability between two classes (usually referred to as a TRUE/FALSE). This is done by fitting a logistic function, also called a sigmoid function, which has the form \\(p = \\frac{1}{1 + e^{-y}}\\) and have a characteristic S-shape. When plotting these probabilites, the y-axis ranges from 0 to 1 reflecting the probability of the class measured along the y-axis.\nWhen applying logistic regression, the probabilites are transformed into a continuous scale with the logit function which has the form \\(log(odds) = log(\\frac{p}{1-p})\\). The logit function converts the probabilities into log-odds. For example, a probability of 0.5 is 0 log-odds, the center on both y-axes. A probability of 0.731 is 1 log-odds and so forth. The points that were classified as TRUE, i.e. 1 on the y-axis, is transformed to positive infinity on the new y-axis as:\n\\[\nlog(odds) = log(\\frac{p}{1-p}) = log(\\frac{1}{1-1}) = log(\\frac{1}{0}) = log(1) - log(0) = \\infty\n\\]\nThe points that were classified as FALSE, i.e. 0 on the previous y-axis, is transformed to negative infinity on the new y-axis as:\n\\[\nlog(odds) = log(\\frac{p}{1-p}) = log(\\frac{0}{1-0}) = log(\\frac{0}{1}) = log(0) - log(1) = -\\infty\n\\]\nSo, the interval of 0.5 to 1 on the old axis as been transformed into an interval from 0 to positive infinity and vice versa. The data now exists on a continuous scale from negative to positive infinity, and a straight line can be fitted to the data.\nFor finding the coefficients for the fitted line, logistic regression cannot apply sum of squared residuals as OLS did, as the distance between a point and a candidate line tends to be infinity given the transformation mentioned above. Instead, maximum likelihood is used to find the best fit line. The term refers to maximizing the likelihood of observing the data given the model. The likelihood is the product of the probabilities of the data points, so a good fit also maximizes the product of the probabilities.\nAfter finding a candidate line on the log-odds scale (the continuous scale), the data points are projected onto the line. The log-odds values are read off the y-axis and converted back into probabilities with the logistic function. The product of probabilities, the likelihood, is calculated where the product is found differently for the TRUE and FALSE classes:\n\\[\nL(\\beta,x_i,y_i) = \\prod_{i \\; in \\; y = 1}^{n} x_i \\cdot \\prod_{i \\; in \\; y = 0}^{n} (1-x_i)\n\\]\nWhere ‘n’ is the number of observations, ‘y’ is the class, and ‘x’ is individual probabilities. It is the above product that is maximized by trying different lines on the log-odds scale. The line that maximizes the product is the best fit line which can be written as:\n\\[\n\\beta_{LR} = \\max_{\\beta} \\; L(\\beta,x_i,y_i)\n\\]\nThe process results in the best set of coefficients for the linear line that maximizes the product of probabilities, i.e. maximum likelihood. It is possible to convert the linearly fit line back to the original scale with the logistic function by replacing ‘y’ with the linear equation:\n\\[\np = \\frac{1}{1 + e^{-y}} = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 \\cdot x_1+ \\beta_2 \\cdot x_2 + ... + \\beta_n \\cdot x_n)}}\n\\]\nFor illustrating logistic regression, we will use the iris dataset and predict the probability of a flower being a virginica based on the petal length. As previously, the setosa class have been removed from the data set to only contain two classes. The species is transformed into a binary variable, where versicolor is 0 and virginica is 1. Below the data is visualized where the petal length is seen on the x-axis and the probability of being a virginica is seen on the y-axis.\n\niris_prob &lt;- iris |&gt;\n  mutate(Species_prob = case_when(Species == \"versicolor\" ~ 0,\n                                     Species == \"virginica\" ~ 1))\n\niris_prob_plot &lt;- iris_prob |&gt;\n  ggplot(aes(x = Petal.Length,\n             y = Species_prob)) +\n  geom_point() +\n  labs(x = \"Petal Length\",\n       y = \"Probability of virginica\")\niris_prob_plot\n\n\n\n\n\n\n\n\nThe probabilities are then transformed to log(odds) with the logit function. In this data set, all values were either 1 or 0, which is why the log(odds) values are either positive or negative infinity as seen below:\n\niris_logit_plot &lt;- iris_prob |&gt; \n  mutate(Species_logit = log(Species_prob / (1 - Species_prob))) |&gt; \n  mutate(Species_logit = case_when(Species_logit == Inf ~ \"Inf\",\n                                   Species_logit == -Inf ~ \"-Inf\",\n                                   TRUE ~ as.character(Species_logit)\n                                   )) |&gt;\n  ggplot(aes(x = Petal.Length,\n             y = Species_logit)) +\n  geom_point() +\n  scale_y_discrete(limits = c(\"-Inf\", \"-2\", \"-1\", \"0\", \"1\", \"2\", \"Inf\")) +\n  labs(x = \"Petal Length\",\n       y = \"log(odds) of virginica\")\niris_logit_plot\n\n\n\n\n\n\n\n\nAs a candidate fit, see the red dashed lined below. The data points are projected onto the line and the log(odds) values are read off the y-axis. The log(odds) values are then converted back into probabilities with the logistic function and the product for the TRUE and FALSE classes are found. The product of the two classes is then calculated. The candidate line that maximizes the product is the best fit line.\n\niris_logit_plot +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"red\")\n\n\n\n\n\n\n\n\nUltimately, the optimal sigmoid shaped line is found:\n\niris_prob_plot +\n  geom_smooth(method = \"glm\",\n              method.args = list(family = \"binomial\"),\n              se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "Applied bio-statistical methods",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "stat06_enet.html",
    "href": "stat06_enet.html",
    "title": "9  Elastic Net",
    "section": "",
    "text": "9.1 Ridge Regression\nRecall from the OLS section that the goal is minimize the sum of squared residuals (SSR).\n\\[\n\\beta_{OLS} = \\min_{\\beta} ||y-X\\beta||_{2}^{2}\n\\]\nRidge is essentially OLS, except for an added penalty term to the minimization of the SSR:\n\\[\n\\beta_{Ridge} = \\min_{\\beta} ||y-X\\beta||_{2}^{2} + \\lambda||\\beta||_{2}^{2}\n\\]\nFinding the coefficients for the linear model now becomes a compromise between minimizing the SSR and minimizing the size of the coefficients. \\(\\lambda\\) is a hyperparameter that controls the trade-off between the two. When \\(\\lambda = 0\\), Ridge regression is the same as OLS. When \\(\\lambda\\) is very large, the coefficients are driven towards zero as the penalty term \\(\\lambda||\\beta||_{2}^{2}\\) becomes large compared to the residuals. The coefficients can never be set to zero, but they can be driven asymptotically close to zero. The reason is, when the coefficients are close to zero, the model prefer to fitting to the data instead of further reducing the size of the coefficients.\nIntroducing the penalty term makes the fit to the data worse, but it can improve the generalization to new data. The coefficients obtained through Ridge tends to be small compared to OLS, so the model tends to be less sensitive to changes in the data. In machine learning terms, this is called introducing bias (inability to fit data perfectly) to reduce variance (the model fits better to new data). The bias-variance trade-off is a fundamental concept in machine learning.\nFor visualization, remember the mtcars data set used previously. The following code chunk shows a 3D scatter plot of the weight of the car (wt) vs the horse power (hp) vs the miles per gallon (mpg). The color of the dots represent the horse power. When orienting the plot correctly, it is possible to see a linear correlation between the variables.\nmtcars |&gt; \n  plot_ly(x = ~wt,\n          y = ~hp,\n          z = ~mpg,\n          color = ~hp,\n          type = \"scatter3d\",\n          mode = \"markers\") |&gt; \n  plotly::layout(\n    scene = list(\n      xaxis = list(title = \"Weight\"),\n      yaxis = list(title = \"Horse Power\"),\n      zaxis = list(title = \"Miles per Gallon\"),\n      camera = list(\n        eye = list(x = 1.5, y = -2, z = 0.5)\n      )),\n    showlegend = FALSE)\nTwo coefficients are used to try and model the data. The coefficients are the slope of the line for the weight and horse power. High coefficients means steep slopes, which in turn suggests that the outcome (mpg) is sensitive to the variables (wt and hp). When presented to new data which is different from the training data, the model predicts wildly different outcomes due to the sensitivity to changes. The model then generalizes poorly to new data and is said to have high variance. By increasing lambda, the slope is reduced, and the model becomes less sensitive to the variables. This is the bias-variance trade-off in action as seen in the following code chunks.\n# CV folds object\nmtcars_split &lt;- initial_split(mtcars)\nmtcars_train &lt;- training(mtcars_split)\nmtcars_test &lt;- testing(mtcars_split)\nmtcars_folds &lt;- vfold_cv(mtcars_train,\n                         v = 5)\n\n# Setup model specs\nridge_spec &lt;- linear_reg(penalty = tune(),\n                         mixture = 0) |&gt; \n  set_engine(engine = \"glmnet\") |&gt; \n  set_mode(\"regression\")\n\nOLS_spec &lt;- linear_reg() |&gt;  # added OLS to have lambda = 0\n  set_engine(engine = \"lm\") |&gt; \n  set_mode(\"regression\")\n\n# Model recipe, no preprocessing\nridge_recipe &lt;- recipe(mpg ~ wt + hp,\n                       data = mtcars_train)\n\n\n# Combine to worflow\nridge_wflow &lt;- workflow_set(\n  preproc = list(recipe = ridge_recipe),\n  models = list(OLS = OLS_spec,\n                Ridge = ridge_spec)\n  )\n\n# Change values of hyperparameters to search across\nridge_params &lt;- ridge_spec |&gt;\n  extract_parameter_set_dials() |&gt;\n  update(penalty = penalty(c(-1, 1)))\n\n# Add params to workflow\nridge_wflow &lt;-\n  ridge_wflow |&gt;\n  option_add(id = \"recipe_Ridge\",\n             param_info = ridge_params) |&gt;\n  option_add(id = \"recipe_Ridge\",\n             control = control_grid(extract = function(x) x))\n\n# Run through grid\nridge_grid_results &lt;- ridge_wflow |&gt; \n  workflow_map(\n    seed = 1337,\n    resamples = mtcars_folds,\n    grid = 5\n  )\nThe following code chunk shows the results of the grid search. The plot shows the root mean square error (RMSE) for the Ridge regression model against the different lambdas. The RMSE increases as lambda increases suggesting a low lambda is preferred.\nautoplot(ridge_grid_results,\n         id = \"recipe_Ridge\",\n         metric = \"rmse\")\nIt is also possible to visualize the actual coefficients for the Ridge regression model. In the above, the lambda values were treated as hyperparameters and was optimized. If the goal is to visualize the coefficients for a specific lambda value, each value of lambda needs to be treated as its own model. The following code chunk shows how to do this.\n# Define a grid of lambda values\nlambda_grid &lt;- 10^seq(-2, 2, length.out = 10)\n\n# Create each function\nridge_models &lt;- lambda_grid |&gt;\n  map(function(lambda) {\n    \n    linear_reg(penalty = lambda,\n               mixture = 0) |&gt; \n      set_engine(\"glmnet\") |&gt; \n      fit(mpg ~ wt + hp,\n          data = mtcars)\n})\nThe coefficients are then plotted against the lambda values. The coefficients are reduced as lambda increases. The coefficients are not set to zero, but they are driven towards zero (and might visually appear to be zero).\n# Unnest coefficients\nridge_coefficients &lt;- bind_rows(map_dfr(ridge_models, ~tidy(.x))) %&gt;%\n  mutate(lambda = rep(lambda_grid, each = 3))\n\n# Visualize coefficients\nridge_coefficients |&gt;\n  ggplot(aes(x = lambda,\n             y = estimate,\n             color = term)) +\n  geom_line() +\n  scale_x_log10() +\n  labs(title = \"Ridge Regression Coefficients vs. Lambda\",\n       x = \"Lambda\",\n       y = \"Coefficient Estimate\")\nWe can plot each of the fitted models on top of the scatterplot to see how the coefficients change with lambda. Due to the limitations of 3D-plotting with plotly, legends for each lambda value are not shown. The lambda values can instead be found be hovering each of the planes in the plot. It is seen, that the slopes are reduced as lambda increases.\n# Make predictions for each function\nridge_predictions &lt;- ridge_models |&gt;\n  map2(lambda_grid, function(model, lambda) {\n    model |&gt; \n      predict(mtcars) |&gt;\n      bind_cols(lambda = lambda)\n  }) |&gt; \n  bind_rows()\n\n# Bind col predictions to data set\nmtcars_train_w_pred &lt;- mtcars |&gt; \n  bind_cols(ridge_predictions |&gt;\n              pivot_wider(names_from = \"lambda\",\n                          values_from = .pred,\n                          names_prefix = \"lambda_\",\n                          values_fn = list) |&gt;\n              unnest(everything())) |&gt; \n  pivot_longer(cols = starts_with(\"lambda_\"),\n               names_to = \"lambda\",\n               values_to = \"prediction\") |&gt; \n  mutate(lambda = as.numeric(stringr::str_remove(lambda, \"lambda_\")),\n         lambda = round(lambda, 3))\n\n# Create visualization\nmtcars_3d_scatter &lt;- mtcars |&gt; \n  plot_ly(x = ~wt,\n          y = ~hp,\n          z = ~mpg,\n          color = ~hp,\n          type = \"scatter3d\",\n          mode = \"markers\")\n\nplot_w_predictions &lt;- mtcars |&gt;\n  plot_ly(\n    x = ~ wt,\n    y = ~ hp,\n    z = ~ mpg,\n    type = \"scatter3d\",\n    mode = \"markers\"\n  ) |&gt;\n  plotly::layout(\n    scene = list(\n      xaxis = list(title = \"Weight\"),\n      yaxis = list(title = \"Horse Power\"),\n      zaxis = list(title = \"Miles per Gallon\"),\n      camera = list(\n        eye = list(x = 1.5, y = -2, z = 0.5)\n      )),\n    showlegend = FALSE)\n\nfor (lambda in lambda_grid) {\n  mtcars_train_w_pred_filt &lt;- mtcars_train_w_pred |&gt;\n    filter(lambda == !!round(lambda, 3))\n\n  plot_w_predictions &lt;- plot_w_predictions |&gt;\n    add_trace(data = mtcars_train_w_pred_filt,\n              x = ~wt,\n              y = ~hp,\n              z = ~prediction,\n              color = ~lambda,\n              type = \"mesh3d\",\n              name = stringr::str_c(\"Lambda: \", lambda))\n} \nplot_w_predictions",
    "crumbs": [
      "Applied bio-statistical methods",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Elastic Net</span>"
    ]
  },
  {
    "objectID": "stat06_enet.html#ridge-regression",
    "href": "stat06_enet.html#ridge-regression",
    "title": "9  Elastic Net",
    "section": "",
    "text": "9.1.1 Applying Ridge Regression to Logistic Regression\nRecall from Chapter 8 on Logistic Regression, that the algorithm seek to maximize the likelihoods:\n\\[\n\\beta_{LR} = \\max_{\\beta} \\; L(\\beta,x_i,y_i)\n\\]\nWhere \\(L(\\beta,x_i,y_i)\\) is the likelihood function mentioned in the chapter, \\(\\beta\\) are the coefficients, ‘y’ are the classes, and ‘x’ are individual probabilities. Similarly to OLS, a penalization term is included when applying Ridge Regression. Since Logistic Regression seek to maximize likelihoods, whereas OLS seek to minimize residuals, the included penalization term is subtracted from the expression:\n\\[\n\\beta_{LR, Ridge} = \\max_{\\beta} \\; L(\\beta,x_i,y_i) - \\lambda||\\beta||_{2}^{2}\n\\]",
    "crumbs": [
      "Applied bio-statistical methods",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Elastic Net</span>"
    ]
  },
  {
    "objectID": "stat06_enet.html#lasso-regression",
    "href": "stat06_enet.html#lasso-regression",
    "title": "9  Elastic Net",
    "section": "9.2 Lasso Regression",
    "text": "9.2 Lasso Regression\nLasso is similar to Ridge, except the penalty uses the absolute values of the coefficients instead of squaring them. This enables the Lasso Regression to set some coefficients to 0, resulting in a less complex model as it contains fewer variables. Again, the \\(\\lamba\\) hyperparameter is used to control the penalization. \\(\\lambda\\) ranges from 0 to \\(+\\infty\\), where higher values of \\(\\lambda\\) gives a more penalized model, i.e. contain fewer variables. The Lasso Regression is given by:\n\\[\n\\beta_{Lasso} = \\min_{\\beta} ||y-X\\beta||_{2}^{2} + \\lambda||\\beta||_{1}\n\\]\nBy removing variables, the algorithm can be used for dimensionality reduction. To exemplify, the below code chunk tries to predict mpg given all variables using Lasso Regression. The expectation is, that some variables contain too little information about the outcome, and is excepted to be removed at higher values of \\(\\lambda\\). Since we are not comparing different models, a much simpler approach can be applied:\n\nset.seed(1337)\n# Setup model specs\nlasso_spec &lt;- linear_reg(penalty = tune(),\n                         mixture = 1) |&gt; \n  set_engine(engine = \"glmnet\") |&gt; \n  set_mode(\"regression\")\n\n\n# Change values of hyperparameters to search across\nlasso_params &lt;- lasso_spec |&gt;\n  extract_parameter_set_dials() |&gt;\n  update(penalty = penalty(c(-1, 0.1)))\n\n# Tune the grid and find optimal lambda\nlasso_grid_results &lt;- lasso_spec |&gt;\n  tune_grid(preprocessor = mpg ~ .,\n            resamples = mtcars_folds,\n            param_info = lasso_params,\n            metrics = metric_set(rmse),\n            grid = 100) |&gt;\n  select_best()\n\nWarning in select_best(tune_grid(lasso_spec, preprocessor = mpg ~ ., resamples\n= mtcars_folds, : No value of `metric` was given; \"rmse\" will be used.\n\n# Update our model with the tuned parameter\nlasso_tuned &lt;- lasso_spec |&gt;\n  finalize_model(lasso_grid_results)\n\n# Fit model to data and get terms\nlasso_tuned |&gt;\n  fit(mpg ~ .,\n      data = mtcars_train) |&gt; \n  tidy() |&gt; \n  filter(estimate != 0)\n\n# A tibble: 7 × 3\n  term        estimate penalty\n  &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)  11.9      0.150\n2 hp           -0.0209   0.150\n3 drat          2.24     0.150\n4 wt           -1.83     0.150\n5 qsec          0.501    0.150\n6 am            3.23     0.150\n7 carb         -0.295    0.150\n\n\nAs can be seen from the final output, the Lasso Regression has removed some variables from the model. Four variables remains in the model, whereas the last seven are set to zero.\n\n9.2.1 Applying Logistic Regression to Logistic Regression\nAs for Ridge, Lasso can be used with logistic regression as well by subtracting the L1 norm of the coefficients.\n\\[\n\\beta_{LR, Lasso} = \\max_{\\beta} \\; L(\\beta,x_i,y_i) - \\lambda||\\beta||_{2}\n\\]",
    "crumbs": [
      "Applied bio-statistical methods",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Elastic Net</span>"
    ]
  },
  {
    "objectID": "stat07_bias_var.html",
    "href": "stat07_bias_var.html",
    "title": "10  Generate random data",
    "section": "",
    "text": "Set seed and load packages.\n\n\nShow the code\nset.seed(1337)\n\nlibrary(\"tidymodels\")\ntidymodels::tidymodels_prefer()\nlibrary(\"patchwork\")\n\n\nLoad data.\n\n\nShow the code\nX &lt;- seq(0, 10, length.out = 100)\n\ny_poly &lt;- sin(X) + rnorm(100, 0, 0.4)\ndf_poly &lt;- tibble(X = X,\n                  y = y_poly)\n\n\nHelper functions.\n\n\nShow the code\n# Function for creating polynomial fits of different degrees\ncreate_poly_pred &lt;- function(data,\n                             degree) {\n  # Resample the data to simulate different training sets\n  df_sample &lt;- data |&gt;\n    sample_frac(1,\n                replace = TRUE)\n    \n  poly_spec &lt;- linear_reg() |&gt;\n    set_engine(\"lm\") |&gt;\n    set_mode(\"regression\")\n  \n  poly_recipe &lt;- recipe(y ~ X,\n                        data = data) |&gt;\n    step_poly(X,\n              degree = degree)\n  \n  poly_workflow &lt;- workflow() |&gt;\n    add_model(poly_spec) |&gt;\n    add_recipe(poly_recipe)\n  \n  poly_fit &lt;- poly_workflow |&gt;\n    fit(data = df_sample)\n  \n  poly_pred &lt;- data |&gt;\n    mutate(y_pred = predict(poly_fit, new_data = data)$.pred)\n  \n  return(poly_pred)\n}\n\n# Function to plot the model with multiple poly fits\nplot_multiple_poly_models &lt;- function(data,\n                                      degree,\n                                      n_models) {\n  p &lt;- ggplot(data,\n              aes(x = X,\n                  y = y)) +\n    geom_point(aes(color = \"Data points\")) +\n    stat_function(fun = sin,\n                  aes(color = \"True sine function\")) +\n    labs(title = stringr::str_c(\"Polynomial Degree \", degree, \" with Multiple Fits\"),\n         x = \"X\",\n         y = \"y\",\n         color = \"Legend\") +\n    lims(y = c(-2, 2))\n  \n  for (i in 1:n_models) {\n    \n    df_pred_poly &lt;- data |&gt;\n      create_poly_pred(degree = degree)\n    \n    p &lt;- p + geom_line(data = df_pred_poly,\n                       aes(y = y_pred,\n                           color = \"Polynomial\"),\n                       alpha = 0.3)\n  }\n  \n  p &lt;- p + scale_color_manual(values = c(\"Data points\" = \"steelblue\",\n                                         \"True sine function\" = \"black\",\n                                         \"Polynomial\" = \"red\"))\n  \n  return(p)\n}\n\n\n\n11 Bias and Variance\nIn supervised machine learning, the ultimate goal is usually to create models which can predict outcome of unseen data. This is the ability of a model to generalize well to new data and is measured by the Generalization Error. The ability to generalize can be decomposed into the balance between bias and variance of a model. Therefore, understanding bias and variance is crucial in machine learning because it helps in developing models that generalize well to new data.\nThe bias of a model is measured by how close the predictions for different training sets are to the true values. A model with low bias make predictions close to the true values, whereas a model with high bias make predictions that are far from the true values. High bias usually occurs when a model is too simple, and cannot capture the underlying complex structure of the data. For example, a linear model will have high bias when the true relationship between the features and the target is non-linear. Opposite, low bias occurs when a model is complex enough to capture the underlying structure of the data, but can also occur when the model is too complex and overfits the data. To measure whether the model has overfit, the variance is introduced.\nThe variance of a model is measured by how much the predictions vary for different training sets, i.e. how much the model simply just memorizes the training data and fits too closely to the data points. A model with low variance makes similar predictions for different training sets, whereas a model with high variance makes different predictions for different training sets. High variance usually occurs when a model is too complex and fits the noise of the data. A polynomial model of high degree is an example of a model with high variance, as it models tightly to the data points. A low variance model generalizes well to new data, as it makes similar predictions for different training sets, but it tends to make incorrect predictions, i.e. have high bias.\nModels with low bias (precise on average) tends to have high variance (inconsistent across training sets) and vice versa. An optimal model has a both low bias and variance, but since it is difficult to obtain, a good balance between bias and variance is usually sought. This is called the Bias-Variance Tradeoff.\nRidge Regression is a good example of the Bias-Variance Tradeoff. When the penalty is set to 0, the Ridge model is the OLS solution. Assuming that there is a linear relationship between the variables and the outcome, the OLS model has low bias as it on average makes predictions close to the true values. However, the OLS model has high variance as it is sensitive to the noise in the data. By applying Ridge penalty, the coefficients of the model is reduced, and is thereby less sensitive to the noise in the data - a lower chance of overfitting and lower variance. Meanwhile, it also forces the model to fit the data less closely, which increases the bias of the model.\nA detail which proves difficult to understand, is that an obtained data set, e.g. the iris data set is in itself a sample from some total population. It is not feasible to measure all iris flowers, and as such the true relationship between the features and the target is unknown. Therefore, the bias and variance of a model is not directly measurable, as it is not possible to compare the predictions of the model to the true values. Instead, it is assumed that the obtained data set is a good representation of the total population, i.e. they have similar distributions. Bias and variance is estimated by fitting a model multiple times to different training sets, but only one data set is available. Cross-validation is a method to estimate the bias and variance of a model, as it simulates different training sets by splitting the data set into multiple training and validation sets, again assuming each fold is a good representation of the total population. Another method is the Bootstrap method which is applied in the below examples. A Bootstrap sample is a sample drawn with replacement from the original data set. Usually, the sample drawn is of the same size as the total data set. With replacement means that the same observation can be drawn multiple times, which is how the Bootstrap samples differs from the original data set.\nTo exemplify what is meant by the Bootstrap method, a sample of 10 numbers is drawn from the numbers 1 to 10. The sample is drawn with replacement, meaning that the same number can be drawn multiple times:\n\nsample_10 &lt;- tibble(1:10)\nsample_frac(sample_10,\n            1,\n            replace = TRUE)\n\n# A tibble: 10 × 1\n   `1:10`\n    &lt;int&gt;\n 1     10\n 2      6\n 3      1\n 4     10\n 5      3\n 6     10\n 7      4\n 8      4\n 9     10\n10      5\n\n\nTo visualize the Bootstrap method, 10000 samples are drawn from a normal distribution with mean 0 and standard deviation 1, and plotted. The histogram is expected to follow the known bell shape of the normal distribution:\n\nsamples_10k &lt;- rnorm(10000) |&gt; \n  as_tibble()\n\nsamples_10k |&gt;\n  ggplot(aes(x = value)) +\n  geom_histogram(bins = 50,\n                 fill = \"steelblue\") +\n  labs(title = \"Normal Distribution\",\n       x = \"Value\",\n       y = \"Count\")\n\n\n\n\n\n\n\n\nBootstrapping the samples should result in a similar distribution, but contains different values, i,e. different data sets but drawn from the same distribution:\n\n# Bootstrap sample\nbootstrap_10k &lt;- samples_10k |&gt; \n  sample_frac(1,\n              replace = TRUE)\n\n\nbootstrap_10k |&gt;\n  ggplot(aes(x = value)) +\n  geom_histogram(bins = 50,\n                 fill = \"steelblue\") +\n  labs(title = \"Bootstrap Sample\",\n       x = \"Value\",\n       y = \"Count\")\n\n\n\n\n\n\n\n\nTo visualize the meaning of bias and variance, a data set has been randomly generated with a sine function and some noise (random jitter). Polynomials of different degrees are then fitted to the data. For properly visualizing the terms, multiple models have to bit fit on data from the same distribution. Therefore, the data set is resampled with the Bootstrap method to simulate different training sets.\nThe expectation is, that a polynomial of low degree makes a bad prediction, but is consistent across different training sets, i.e. high bias and low variance. A polynomial of high degree makes a good prediction, but is inconsistent across different training sets, i.e. low bias and high variance. The optimal model is a polynomial of moderate degree, which makes good predictions and is consistent across different training sets.\nThe data generated show clear signs of following a sine function, but with some noise. The black line is the true sine function, and the blue points are the generated data.\n\nggplot(df_poly, aes(x = X, y = y)) +\n  geom_point(aes(color = \"Data points\")) +\n  stat_function(aes(color = \"True sine function\"),\n                fun = sin) +\n  labs(title = \"Generated Data\",\n       x = \"X\",\n       y = \"y\",\n       color = \"Legend\") +\n  scale_color_manual(values = c(\"Data points\" = \"steelblue\",\n                                \"True sine function\" = \"black\"))\n\n\n\n\n\n\n\n\nA polynomial of degree 1 (a linear prediction), obviously do not capture the complexity of the data, but is consistent across different training sets, i.e. high bias and low variance.\n\ndf_poly |&gt; \n  plot_multiple_poly_models(degree = 1,\n                            n_models = 30)\n\n\n\n\n\n\n\n\nA polynomial of degree 20 captures the complexity of the data, but also models the noise which was added, but the average of the models are close to the true sine function. The model is overfitting the data, thereby creating variance between different Bootstrap samples, i.e. low bias and high variance.\n\ndf_poly |&gt; \n  plot_multiple_poly_models(degree = 20,\n                            n_models = 30)\n\n\n\n\n\n\n\n\nA polynomial of degree 4 captures the complexity of the data without fitting too tightly to the noise. The average of the models are close to the true sine function, and the models are consistent across different training sets, i.e. low bias and low variance. Hence, it is the optimal model out of degree 1, 20 and 4.\n\ndf_poly |&gt; \n  plot_multiple_poly_models(degree = 4,\n                            n_models = 10)",
    "crumbs": [
      "Applied bio-statistical methods",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Generate random data</span>"
    ]
  },
  {
    "objectID": "pre00_intro.html",
    "href": "pre00_intro.html",
    "title": "Preprocessing of Data",
    "section": "",
    "text": "It is rarely seen that data is ready for analysis as soon as it is collected. Data preprocessing is a crucial step in the modeling process. It involves cleaning, transforming, and encoding data to optimize it for further analysis. There is no one-step process to data preprocessing. It is an iterative process that involves multiple steps, and is highly dependent on the model you are building, the data you are working with and the objective you are trying to achieve.\nSome issues that often needs to be addressed during data preprocessing are missing values, outliers, scaling and encoding categorical data, whereas some other issues are more specific to the model you are building, such as dimensionality reduction and data augmentation.\nMissing data: The absence of a value in a variable is a common issue, and could arise from various reasons such as malfunction in measurement equipment or respondents not answering a question. Many models cannot handle missing data, and it is important to address this issue before proceeding with the analysis. One way would be to remove rows or columns with missing values, but this could lead to loss of valuable information. Otherwise, the missing values could be imputed with e.g. the mean.\nOutliers: Outliers are values that are somehow different from the rest of the data. It can be discussed if some value is an outlier or just an extreme value. Outliers are usually values that should not be able to occur, and would therefore have a negative impact on the model. It is important to identify and remove outliers before proceeding with the analysis.\nData Transformation: For some data sets, the data could be skewed, which means that the data is not normally distributed, but some models assume that the data is normally distributed. A solution could be to take the logarithm or square root of the data. In the field of metagenomics, data is usually compositional, which means that the data is in the form of proportions, and the sum of the data is constant. To move the data from the simplex to the Euclidean space, the data could be transformed with the centered log-ratio transformation.\nScaling: Many machine learning models are sensitive to the scale of the data. For example, Principal Component Analysis finds variance along axis, and if the values along the x-axis are much larger than the values along the y-axis, the variance will be dominated by the x-axis which is showcased in the chapter on the topic. This could lead to misleading results. It is therefore important to scale the data before applying such models. A common way to scale is by standardizing, which means that the data is transformed to have a mean of 0 and a standard deviation of 1. This is achieved by subtracting the mean and dividing by the standard deviation.\nEncode categorical data: Categorical data could easily be string values, and many models cannot handle this. The issue is solved by encoding the data, e.g. with one-hot encoding. A categorical value with k categories is transformed into k binary variables, where one of them is 1 and the others are 0.\nDimensionality reduction: Data of high dimensionality, usually with more variables than observations can impose challenges for the analysis. Some models simply do not work with high-dimensional data such as OLS, whereas others could be computationally expensive or lead to overfitting. The more features, the more observations are needed to fill out the feature space. Dimensionality reduction is a way to reduce the number of features in the data, while still preserving the most important information. Principal Component Analysis and Uniform Manifold Approximation and Projection are common methods for dimensionality reduction. A simple feature selection also results in dimensionality reduction. All solution would also improve the computational costs.\nData Augmentation: When working with small data sets, the model could be overfitting. Data augmentation is used to increase the size of the data set by creating new data points from the existing data. When working with image data, the data set could be increased by rotating, flipping, or zooming the images. This is done to increase the size of the data set, which could lead to a more robust model.\nIn the final chapter of this book, the disease status of individuals is predicted based on their gut microbiota. Such data is usually high-dimensional, and some of the data preprocessing steps mentioned above are crucial to obtain a good model. There is usually no missing values, as all data is generated from a computational pipeline, but the data is compositional and needs to be transformed. The data is also high-dimensional, and dimensionality reduction techniques are used to extract the important information from the data. Hence, the next chapters is on different dimensionality reduction techniques which are also available in the tidymodels package.",
    "crumbs": [
      "Preprocessing of Data"
    ]
  },
  {
    "objectID": "pre01_pca.html",
    "href": "pre01_pca.html",
    "title": "11  Principal Component Analysis",
    "section": "",
    "text": "11.1 Explanation and Example\nMultiple reasons for applying Principal Component Analysis (PCA) exists. One of the most common reasons is to visualize high-dimensional data in lower dimensions. Another reason could be to decorrelate variables or to reduce the noise in the data by only maintaining the variables that contain most of the variance.\nWhen having more than 3 variables, it is not possible to visualize them in a single plot. To overcome this, PCA can reduce the dimensionality of the data to lower dimensions (e.g. 2 or 3) while preserving the variance of the data. Each Principal Component (PC) is a linear combination of the original variables, and can now be considered as a variable on its own replacing the originals. A data set with many variables, e.g. the 123 variables in the count matrix, can be visualised with a 2D plot. Loosing that many dimensions is almost bound to loose some information, but a PCA optimizes towards retaining as much as possible.\nTo exemplify and showcasing the method and the underlying math, the mtcars data set is used. For this example, two dimensions wt (weight) and mpg (miles pr. gallon) are used. The points are coloured by cyl (cylinders) to show how the data is grouped in the 2D space. The goal is to retain the information, but reduce the dimensions to a single line that best describes the data. This line is called the first Principal Component (PC1).\nmtcars |&gt; \n  ggplot(aes(x = wt,\n             y = mpg,\n             col = cyl)) +\n  geom_point(size = 3) +\n  labs(title = \"Miles pr Gallon vs Weight\",\n       x = \"Weight (1000 lbs)\",\n       y = \"Miles pr Gallon\",\n       colour = \"Cylinders\") +\n  scale_colour_viridis_c()\nThe data is centered, i.e. the mean of each variable is subtracted from each observation within that variable.\n\\[\nX_{i,j \\;centered} = X_{i,j} - \\mu_{j}\n\\]\nWhere X is the data matrix, i is the observation and j is the variable and \\(\\mu\\) the mean. By subtracting the mean, all data observations are centered around the origin. This makes some mathematical operations easier, such as rotating the coordinate system. Centering the data do not affect the correlation between observations. The resulting plot is then:\nmtcars |&gt; \n  ggplot(aes(x = wt - mean(wt),\n             y = mpg - mean(mpg),\n             col = cyl - mean(cyl))) +\n  geom_point(size = 3) +\n  labs(title = \"Miles pr Gallon vs Weight\",\n       x = \"Weight (1000 lbs)\",\n       y = \"Miles pr Gallon\",\n       colour = \"Cylinders\") +\n  scale_colour_viridis_c()\nA line is to be fit to the data. This line should maximize the variance of the data, i.e. maximize the sum of squared distances from the origin to the points projected on to the line. A rough sketch of the scenario can be seen below. A proposed best fit line, that maximizes the variance of the data, is drawn in red. The dashed green line is a point projected onto the proposed best fit line. The dashed blue line is the distance from the point on the line to the origin. It is the length of the blue line that is maximized across all data points.\nmtcars |&gt; \n  ggplot(aes(x = wt - mean(wt),\n             y = mpg - mean(mpg),\n             col = cyl - mean(cyl))) +\n  geom_point(size = 3) +\n  geom_abline(intercept = 0, slope = -6, colour = \"red\") +\n  geom_segment(aes(x = -1, y = 12.5, xend = -1.35, yend = 8.4),\n               colour = \"green\",\n               linetype = \"dashed\",\n               size = 1) +\n  geom_segment(aes(x = -1.35, y = 8.4, xend = 0, yend = 0), \n               colour = \"steelblue\",\n               linetype = \"dashed\",\n               size = 1) +\n  labs(title = \"Miles pr Gallon vs Weight\",\n       x = \"Weight (1000 lbs)\",\n       y = \"Miles pr Gallon\",\n       colour = \"Cylinders\") +\n  scale_colour_viridis_c()\nThe line that maximizes the sum of squared distances is called the first Principal Component (PC1). All future Principal Components are orthogonal to the previous, and run through the origin, which means no optimization. To visualize PC1, all data points are projected onto the line, and the line is rotated to be horizontal and act as a new x-axis. The resulting plot is:\npca &lt;- mtcars |&gt; \n  select(wt, mpg) |&gt;\n  prcomp(center = TRUE) |&gt; \n  tidy(matrix = \"x\") |&gt; \n  pivot_wider(names_from = \"PC\",\n              names_prefix = \"PC\",\n              values_from = \"value\") |&gt;\n  rename(car = row)\n\npca_mtcars &lt;- pca |&gt; \n  left_join(mtcars |&gt;\n              rownames_to_column(var = \"car\"),\n            by = \"car\")\n\npca_mtcars |&gt;\n  ggplot(aes(x = PC1,\n             y = 0,\n             col = cyl)) +\n  geom_point(size = 3) +\n  labs(title = \"Miles pr Gallon vs Weight\",\n       x = \"PC1\",\n       y = \"\",\n       colour = \"Cylinders\") +\n  scale_colour_viridis_c()\nAs can be observed from plotting PC1, the groupings of the data is conserved, but the dimensions are reduced to a single line. The same method can be applied to high dimensional data, such as the count matrix.",
    "crumbs": [
      "Preprocessing of Data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Principal Component Analysis</span>"
    ]
  },
  {
    "objectID": "pre01_pca.html#mathematical-explanation",
    "href": "pre01_pca.html#mathematical-explanation",
    "title": "11  Principal Component Analysis",
    "section": "11.2 Mathematical Explanation",
    "text": "11.2 Mathematical Explanation\n\n11.2.1 Covariance Matrix\nTo mathematically describe the method the covariance matrix, often denoted \\(\\Sigma\\), of the data is calculated. The covariance matrix is a square matrix where the diagonal contains the variance of each variable, and the off-diagonal contains the covariance between each variable:\n\\[\n\\Sigma =  \\begin{bmatrix}\nVar(x_1) & Cov(x_2, x_1) \\\\\nCov(x_1, x_2) & Var(x_2)\n\\end{bmatrix}\n\\]\nWhere \\(x_1\\) and \\(x_2\\) are columns in a data set \\(X\\). The variance in a variable is the spread of the data along the axis, and the covariance is the spread of the data between the axes. The covariance matrix is symmetric, meaning the covariance between \\(x_1\\) and \\(x_2\\) is the same as the covariance between \\(x_2\\) and \\(x_1\\). The variance of each variable is calculated as:\n\\[\nVar(x) = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\mu_x)^2\n\\]\nRemember that the data was centered by subtracting the mean, \\(\\mu_x\\). As a consequence, the mean of all columns of \\(X\\) is 0, and the expression can be reduced to:\n\\[\nVar(x) = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i)^2\n\\] The covariance between two variables is calculated as:\n\\[\nCov(x_1, x_2) = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_{1i} - \\mu_{x_1})(x_{2i} - \\mu_{x_2})\n\\]\nWhere \\(x_{1i}\\) and \\(x_{2i}\\) are the \\(i\\)’th observations in columns \\(x_1\\) and \\(x_2\\), respectively. Again, the mean is zero, and the expression can be reduced to:\n\\[\nCov(x_1, x_2) = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_{1i} \\cdot x_{2i})\n\\]\nThe covariance of the mtcars data set used in the example is then:\n\nmtcars |&gt;\n  select(wt, mpg) |&gt; \n  cov()\n\n           wt       mpg\nwt   0.957379 -5.116685\nmpg -5.116685 36.324103\n\n\n\n\n11.2.2 Eigenvectors and Eigenvalues\nAs the covariance matrix show how the variables varies within themselves and between each other, the eigenvectors of such a matrix spans the directions in the feature space where the data have the most variance. By convention, eigenvectors have unit length (length 1), and as such it is their associated eigenvalues that quantify the length of the eigenvector, and thereby the variance explained by each eigenvector.\nTo find the eigenvectors, the eigenvalues are found first by solving the equation:\n\\[\ndet(\\Sigma - \\lambda I) = 0\n\\]\nWhere \\(\\Sigma\\) is the covariance matrix, \\(\\lambda\\) is the eigenvalue, and \\(I\\) is the identity matrix. As the covariance matrix in the example is a 2x2 matrix, the identity matrix is likewise a 2x2 matrix and two eigenvalues are found. The eigenvalues are then found by solving the equation:\n$$ det(\n\\[\\begin{bmatrix}\n0.96 & -5.12 \\\\\n-5.12 & 36.32\n\\end{bmatrix}\\]\n\n\\[\\begin{bmatrix}\n\\lambda & 0 \\\\\n0 & \\lambda\n\\end{bmatrix}\\]\n)\n\n=\ndet(\n\\[\\begin{bmatrix}\n(0.96 - \\lambda) & -5.12 \\\\\n-5.12 & (36.32 - \\lambda)\n\\end{bmatrix}\\]\n)\n=\n0 $$\nIt can be further simplified to:\n\\[\n(0.96 - \\lambda)(36.32 - \\lambda) - (-5.12 \\cdot -5.12) =\\\\\n34.87 - 0.96\\lambda - 36.32\\lambda + \\lambda^2 - 26.21 = \\\\\n8.66 - 37.28\\lambda + \\lambda^2 = 0\n\\]\nSolving the quadratic equation, the \\(\\lambda\\) values are:\n\\[\n\\lambda_1 = 37.05, \\; \\; \\lambda_2 = 0.234\n\\]\nThe eigenvectors are then found by solving the equation:\n\\[\n\\Sigma \\cdot v = \\lambda \\cdot v\n\\]\nWhere \\(\\Sigma\\) is the covariance matrix, \\(v\\) is the eigenvector, and \\(\\lambda\\) is the eigenvalue. For the two eigenvectors, the equations are:\n$$\n\\[\\begin{bmatrix}\n0.96 & -5.12 \\\\\n-5.12 & 36.32\n\\end{bmatrix} \\cdot\\]\nv_1 = 37.05 v_1 \\\n\\[\\begin{bmatrix}\n0.96 & -5.12 \\\\\n-5.12 & 36.32\n\\end{bmatrix} \\cdot\\]\nv_2 = 0.234 v_2\n$$\nThe two vectors are then:\n\\[\nv_1 = \\begin{bmatrix}\n-0.14 \\\\\n1\n\\end{bmatrix}, \\; \\; v_2 = \\begin{bmatrix}\n7.05 \\\\\n1\n\\end{bmatrix}\n\\]\nIn unit length, the eigenvectors are:\n\\[\nv_1 = \\begin{bmatrix}\n-0.14 \\\\\n1\n\\end{bmatrix}, \\; \\; v_2 = \\begin{bmatrix}\n1 \\\\\n0.14\n\\end{bmatrix}\n\\] ### Plotting of Eigenvectors Two eigenvectors can be plotted on top of the data to show the direction of the variance in the data. The green line is the first eigenvector, i.e. has the largest eigenvalue, and the blue line is the second eigenvector. An immediate observation is, that these two lines are not orthogonal, which is a requirement for eigenvectors. This is due to the scaling of the plot.\n\nmtcars |&gt; \n  ggplot(aes(x = wt - mean(wt),\n             y = mpg - mean(mpg),\n             col = cyl - mean(cyl))) +\n  geom_point(size = 3) +\n  geom_segment(aes(x = 0, y = 0, xend = -0.14, yend = 1),\n               colour = \"green\",\n               size = 1) +\n  geom_segment(aes(x = 0, y = 0, xend = 1, yend = 0.14), \n               colour = \"steelblue\",\n               size = 1) +\n  labs(title = \"Miles pr Gallon vs Weight\",\n       x = \"Weight (1000 lbs)\",\n       y = \"Miles pr Gallon\",\n       colour = \"Cylinders\") +\n  scale_colour_viridis_c()\n\n\n\n\n\n\n\n\nTo visually show that the two eigenvectors are indeed orthogonal, the plot can either be scaled to have the same aspect ratio on both axis, or to limit the axis to the same range:\n\nmtcars |&gt; \n  ggplot(aes(x = wt - mean(wt),\n             y = mpg - mean(mpg),\n             col = cyl - mean(cyl))) +\n  geom_point(size = 3) +\n  geom_segment(aes(x = 0, y = 0, xend = -0.14, yend = 1),\n               colour = \"green\",\n               size = 1) +\n  geom_segment(aes(x = 0, y = 0, xend = 1, yend = 0.14), \n               colour = \"steelblue\",\n               size = 1) +\n  labs(title = \"Miles pr Gallon vs Weight\",\n       x = \"Weight (1000 lbs)\",\n       y = \"Miles pr Gallon\",\n       colour = \"Cylinders\") +\n  scale_colour_viridis_c() +\n  coord_fixed()\n\n\n\n\n\n\n\nmtcars |&gt; \n  ggplot(aes(x = wt - mean(wt),\n             y = mpg - mean(mpg),\n             col = cyl - mean(cyl))) +\n  geom_point(size = 3) +\n  geom_segment(aes(x = 0, y = 0, xend = -0.14, yend = 1),\n               colour = \"green\",\n               size = 1) +\n  geom_segment(aes(x = 0, y = 0, xend = 1, yend = 0.14), \n               colour = \"steelblue\",\n               size = 1) +\n  labs(title = \"Miles pr Gallon vs Weight\",\n       x = \"Weight (1000 lbs)\",\n       y = \"Miles pr Gallon\",\n       colour = \"Cylinders\") +\n  scale_colour_viridis_c() +\n  lims(x = c(-2, 2), y = c(-2, 2))\n\n\n\n\n\n\n\n\n\n\n11.2.3 Importance of Scaling\nThe issue of scaling goes to show how important it is, that values are on the same scale before applying PCA. If the values are not on the same scale, the PCA will be biased towards the variables with the largest values. This can be seen in the example, where the wt variable has a larger range than the mpg variable. The eigenvectors are then biased towards the wt variable. The data can be scaled by both centering and dividing by the standard deviation, which is called standardization. The data is then on the same scale, and the PCA is not biased towards any variable:\n\\[\nX_{i,j \\;standardized} = \\frac{X_{i,j} - \\mu_{j}}{sd_j}\n\\]\nWhere X is the data matrix, i is the observation and j is the variable, \\(\\mu\\) is the mean and \\(sd\\) is the standard deviation. Using standardized data, the eigenvectors and their eigenvalues are:\n$$ v_1 =\n\\[\\begin{bmatrix}\n0.707 \\\\\n-0.707\n\\end{bmatrix}\\]\n, ; ; _1 = 1.87\\\nv_2 =\n\\[\\begin{bmatrix}\n0.707 \\\\\n0.707\n\\end{bmatrix}\\]\n, ; ; _2 = 0.13 $$\nThe eigenvectors are now orthogonal, and the first eigenvector (green) is the one that maximizes the variance of the data. The eigenvectors are scaled by their associated eigenvalue:\n\nmtcars |&gt; \n  ggplot(aes(x = (wt - mean(wt))/sd(wt),\n             y = (mpg - mean(mpg))/sd(mpg),\n             col = (cyl - mean(cyl)/sd(cyl)))) +\n  geom_point(size = 3) +\n  geom_segment(aes(x = 0, y = 0, xend = 0.707*1.87, yend = -0.707*1.87),\n               colour = \"green\",\n               size = 1) +\n  geom_segment(aes(x = 0, y = 0, xend = 0.707*0.13, yend = 0.707*0.13), \n               colour = \"steelblue\",\n               size = 1) +\n  labs(title = \"Miles pr Gallon vs Weight\",\n       x = \"Weight (1000 lbs)\",\n       y = \"Miles pr Gallon\",\n       colour = \"Cylinders\") +\n  scale_colour_viridis_c()\n\n\n\n\n\n\n\n\n\n\n11.2.4 Explained Variance\nUsing all PCs, it is possible to fully reconstruct the data since the data is projected onto the PCs, it can be converted back to the original space. The total variance of the data is the sum of the eigenvalues, and the variance explained by each PC is the eigenvalue of that PC. Therefore, it is also possible to calculate the proportion of variance explained by each PC. For the case of the scaled data, the two eigenvalues were \\(\\lambda_1 = 1.87\\) and \\(\\lambda_2 = 0.13\\). The proportion of variance explained by each PC is then:\n\\[\n\\frac{\\lambda_1}{\\lambda_1 + \\lambda_2} = 0.94, \\; \\; \\frac{\\lambda_2}{\\lambda_1 + \\lambda_2} = 0.06\n\\]\nThe proportion of variance explained by each PC is usually plotted to get an idea of how many PCs are needed to explain the data. The plot is called a scree plot, and can be seen below:\n\nvar_explained &lt;- tibble(prop_explained = c(0.94, 0.06),\n                        PC = c(\"PC1\", \"PC2\"))\n\nvar_explained |&gt;\n  ggplot(aes(x = PC,\n             y = prop_explained)) +\n  geom_col(fill = \"steelblue\") +\n  geom_text(aes(label = prop_explained),\n            vjust = -0.5) +\n  scale_y_continuous(expand = c(0, 0.06)) +\n  labs(title = \"Proportion of Variance Explained\",\n       x = \"Principal Component\",\n       y = \"Proportion Explained\")\n\n\n\n\n\n\n\n\n\n\n11.2.5 Projection of Data onto PCs\nThe original data can be projected onto the PCA space by multiplying the data matrix with the eigenvectors. The projection ensures a change of basis, such that PC1 is the new x-axis, PC2 is the new y-axis, and so forth. The projection happens by:\n\\[\nS = XU\n\\]\nWhere \\(S\\) is the projected data, \\(X\\) is the original data matrix, and \\(U\\) is the matrix of eigenvectors. The matrix \\(U\\) then essentially works as a rotation matrix, rotating the data into the PCA space. When projecting unto fewer dimensions, the data is reduced to the number of eigenvectors (PCs) used in \\(U\\).\nAs an example, the first observation in the scaled mtcars data set is projected onto the PCA space. The projection happens by:\n\\[\nS =\n\\begin{bmatrix}\n-0.6 & 0.91\n\\end{bmatrix} \\cdot\n\\begin{bmatrix}\n-0.14 & -0.99 \\\\\n0.99 & -0.14\n\\end{bmatrix} =\n\\begin{bmatrix}\n0.98 & 0.46\n\\end{bmatrix}\n\\]\nThe data point \\((-0.6, 0.91)\\) is then rotated such that the coordinates are \\((0.98, 0.46)\\) in the PCA space. What is important to note is how the values of the rotation matrix are used to rotate the data point.\nEach data point in the original space are on the form \\((x, y) = (wt, mpg)\\), so the first coordinate denotes wt and the second denotes mpg. Similarly, each PC consists of two coordinates. The first is multiplied by the first coordinate of the data point (here wt), and the second is multiplied by the second coordinate of the data point (here mpg) as per the definition of a matrix product. This implies, that the values of the eigenvectors are the weights of the data points in the PCA space. The first eigenvector has the values \\((-0.14, 0.99)\\), which means that the first coordinate of the data point is multiplied by \\(-0.14\\) and the second coordinate is multiplied by \\(0.99\\), i.e. the second coordinate is weighted more than the first. This can be related back to the fact, that mpg has a larger range than wt, and as such the first PC is primarily explains mpg. As mentioned, the data is not scaled, and so this is a biased conclusion.\nThe important implication of this is, that a row in the rotation matrix contains values describing how each PC weights the original variables. A common method is to plot the loadings, i.e. the rows of the rotation matrix, in the PCA space. The loadings are equal to the eigenvector matrix \\(U\\), but is at times scaled by the square root of the eigenvalues. It is done to encapsulate the difference in variance each PC explains. The loadings are then used to interpret the data in the PCA space, as they show which variables that drive the variance within in PC. By using the R function prcomp the loadings is found as rotation. As can be observed, the loading for mpg points in the direction of PC1 where the biggest variance (spread) is observed. Once again, this is a biased observation, as the data is not scaled.\n\nloadings &lt;- mtcars |&gt; \n  select(wt, mpg) |&gt;\n  prcomp(center = TRUE) |&gt; \n  tidy(matrix = \"rotation\") |&gt; \n  pivot_wider(names_from = \"PC\",\n              names_prefix = \"PC\",\n              values_from = \"value\")\n\narrow_style &lt;- arrow(angle = 20,\n                     ends = \"first\",\n                     type = \"closed\",\n                     length = grid::unit(8, \"pt\"))\n\npca_mtcars |&gt; \n  ggplot() +\n  geom_point(aes(x = PC1,\n             y = PC2),\n             size = 3,\n             color = \"steelblue\") +\n  geom_segment(aes(x = PC1, y = PC2),\n               data = loadings,\n               xend = 0,\n               yend = 0,\n               arrow = arrow_style) +\n  geom_text(aes(x = PC1,\n                y = PC2,\n                label = column),\n            data = loadings,\n            nudge_y = -0.2,\n            color = \"orange\") +\n  labs(title = \"PC2 vs PC1\",\n       x = \"PC1\",\n       y = \"PC2\") +\n  coord_fixed()",
    "crumbs": [
      "Preprocessing of Data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Principal Component Analysis</span>"
    ]
  },
  {
    "objectID": "pre01_pca.html#count-matrix",
    "href": "pre01_pca.html#count-matrix",
    "title": "11  Principal Component Analysis",
    "section": "11.3 Count Matrix",
    "text": "11.3 Count Matrix\nTo apply PCA to an example of higher dimensions, the count matrix is used. The built in function prcomp is used to calculate the PCs along with the loadings and eigenvalues (explained variance).\n\npca_count_matrix &lt;- count_matrix_clr |&gt; \n  column_to_rownames(var = \"Sample\") |&gt;\n  prcomp(center = TRUE,\n         scale = TRUE)\n\nThe proportion of variance explained by each PC is calculated and plotted. Here, the first PC explains less than 1% of the variance.\n\npca_count_matrix |&gt;\n  tidy(matrix = \"eigenvalues\") |&gt;\n  ggplot(aes(x = reorder(PC, -percent),\n             y = percent)) +\n  geom_col(fill = \"steelblue\") +\n  scale_y_continuous(expand = c(0, 0)) +\n  labs(title = \"Proportion of Variance Explained\",\n       x = \"Principal Component\",\n       y = \"Proportion Explained\") +\n  theme(axis.text.x = element_blank(),\n        axis.ticks.x = element_blank())\n\n\n\n\n\n\n\n\nA cummulative plot over the PCs can be used to determine how many PCs are needed to explain the data. The green dashed line is set at 70% of the variance explained, and the red dashed line is set at the 49th PC. The plot then shows that 49 PCs are needed to explain 70% of the variance.\n\npca_count_matrix |&gt;\n  tidy(matrix = \"eigenvalues\") |&gt;\n  ggplot(aes(x = reorder(PC, -percent),\n             y = cumulative)) +\n  geom_col(fill = \"steelblue\") +\n  geom_hline(yintercept = 0.7,\n             linetype = \"dashed\",\n             colour = \"green\",\n             size = 1) +\n  geom_vline(xintercept = \"49\",\n             linetype = \"dashed\",\n             colour = \"red\",\n             size = 1) +\n  scale_y_continuous(expand = c(0, 0)) +\n  labs(title = \"Cummulative Proportion of Variance Explained\",\n       x = \"Principal Component\",\n       y = \"Cummulative Proportion Explained\") +\n  theme(axis.text.x = element_blank(),\n        axis.ticks.x = element_blank())\n\n\n\n\n\n\n\n\nIt is for quite obvious reasons not possible to plot the first 49 PCs in a single plot. Instead, the first two PCs are plotted to show the data in a 2D space. The data is colored by the Patient_status variable, which shows that the data is not easily separable in the 2D space. Since the first two PCs only explain approximately 1% of the variance, it is expected that the data is not separable. It is then also an assumption, that the data is even separable in the high dimensional space which is unknown. If the data is separable, it is expected that the separation is more pronounced in the higher dimensions as more variance is explained.\n\npca_count_matrix |&gt;\n  tidy(matrix = \"x\") |&gt; \n  pivot_wider(names_from = \"PC\",\n              names_prefix = \"PC\",\n              values_from = \"value\") |&gt; \n  rename(Sample = row) |&gt; \n  left_join(meta,\n            by = \"Sample\") |&gt;\n  ggplot(aes(x = PC1,\n             y = PC2,\n             colour = factor(Patient_status))) +\n  geom_point(size = 3) +\n  labs(title = \"Principal Component Analysis\",\n       x = \"PC1\",\n       y = \"PC2\",\n       colour = \"Patient Status\")",
    "crumbs": [
      "Preprocessing of Data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Principal Component Analysis</span>"
    ]
  },
  {
    "objectID": "apply00_intro.html",
    "href": "apply00_intro.html",
    "title": "Predictive Modeling of Microbiome Data using Tidymodeling",
    "section": "",
    "text": "In the first chapter, the concepts of tidymodeling were introduced. In the second chapter, the statistical concepts that are often used in Bioinformatics and Biostatistics were introduced. The third chapter touched upon different preprocessing techniques to apply. This chapter combines the first three chapters to build and evaluate models for a real-world dataset. As mentioned in the preface, the dataset used is 16S rRNA amplicon reads used to describe multiple sclerosis (MS) (Cox et al. 2021). Here, the data is used to predict the disease status of the person.\n\n\n\n\nCox, Laura M., Amir Hadi Maghzi, Shirong Liu, Stephanie K. Tankou, Fyonn H. Dhang, Valerie Willocq, Anya Song, et al. 2021. “Gut Microbiome in Progressive Multiple Sclerosis.” Annals of Neurology 89 (June): 1195–1211. https://doi.org/10.1002/ANA.26084.",
    "crumbs": [
      "Predictive Modeling of Microbiome Data using Tidymodeling"
    ]
  },
  {
    "objectID": "apply01_model_data.html",
    "href": "apply01_model_data.html",
    "title": "13  Modelling",
    "section": "",
    "text": "13.1 Data wrangling\nThe data is joined with the metadata as the Patient_status is used as the outcome variable. Only the columns that are needed for the model are kept. Notice inner_join() is used such that only rows for which both the count matrix and metadata are available are kept.\ncount_matrix_clr &lt;- count_matrix_clr |&gt;\n  inner_join(meta,\n             by = \"Sample\") |&gt;\n  select(-c(Sample, Treatment, Ethnicity, Location,\n            Age, BMI, Disease_severity, EDSS, Sex)) |&gt;\n  relocate(Patient_status)",
    "crumbs": [
      "Predictive Modeling of Microbiome Data using Tidymodeling",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Modelling</span>"
    ]
  },
  {
    "objectID": "apply01_model_data.html#cross-validation",
    "href": "apply01_model_data.html#cross-validation",
    "title": "13  Modelling",
    "section": "13.2 Cross-Validation",
    "text": "13.2 Cross-Validation\nThe data is split into a training and testing set using cross-validation to avoid overfitting. This is especially needed, as some hyperparameters needs to be tuned.\nThe data is split:\n\ncount_matrix_clr_split &lt;- initial_split(count_matrix_clr,\n                                        prop = params$percentage_train,\n                                        strata = Patient_status)\ncount_matrix_clr_train &lt;- training(count_matrix_clr_split)\ncount_matrix_clr_test &lt;- testing(count_matrix_clr_split)\n\nThe CV object is created:\n\ncount_matrix_clr_folds &lt;- vfold_cv(count_matrix_clr_train,\n                                   v = params$n_folds)\ncount_matrix_clr_folds\n\n#  10-fold cross-validation \n# A tibble: 10 × 2\n   splits           id    \n   &lt;list&gt;           &lt;chr&gt; \n 1 &lt;split [327/37]&gt; Fold01\n 2 &lt;split [327/37]&gt; Fold02\n 3 &lt;split [327/37]&gt; Fold03\n 4 &lt;split [327/37]&gt; Fold04\n 5 &lt;split [328/36]&gt; Fold05\n 6 &lt;split [328/36]&gt; Fold06\n 7 &lt;split [328/36]&gt; Fold07\n 8 &lt;split [328/36]&gt; Fold08\n 9 &lt;split [328/36]&gt; Fold09\n10 &lt;split [328/36]&gt; Fold10",
    "crumbs": [
      "Predictive Modeling of Microbiome Data using Tidymodeling",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Modelling</span>"
    ]
  },
  {
    "objectID": "apply01_model_data.html#recipes",
    "href": "apply01_model_data.html#recipes",
    "title": "13  Modelling",
    "section": "13.3 Recipes",
    "text": "13.3 Recipes\nThe recipes contain two pieces of important information. The first is the formula that describes the relationship between the variables and the outcome. In this case, which genera to use for predicting disease. The second is the pre-processing steps that are applied to the data before the model is built. This could include steps such as scaling, log-transform or imputing missing values. Here, the data is already clr-transformed.\nTwo recipes are created. One without any pre-processing steps, and one where Principal Component Analysis (PCA) is applied to the data. In both cases, all variables is used to try and predict the outcome Patient_status.\n\nstand_recipe &lt;- recipe(Patient_status ~ .,\n                       data = count_matrix_clr_train)\n\npca_recipe &lt;- stand_recipe |&gt; \n  step_pca(all_predictors(),\n           num_comp = 7)",
    "crumbs": [
      "Predictive Modeling of Microbiome Data using Tidymodeling",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Modelling</span>"
    ]
  },
  {
    "objectID": "apply01_model_data.html#model-specifications",
    "href": "apply01_model_data.html#model-specifications",
    "title": "13  Modelling",
    "section": "13.4 Model Specifications",
    "text": "13.4 Model Specifications\nTo model the data, a specification is needed. This includes the model to use, the engine to use, the mode of prediction and choosing hyperparameters. Since the outcome is binary, a logistic regression model is used. The model is later tuned using a grid search over the hyperparameters penalty and mixture. Models with exclusively Ridge and Lasso regularization are built, as well as a model with a mixture of both.\n\nlog_spec &lt;- logistic_reg(penalty = tune(),\n                         mixture = tune()) |&gt;\n  set_engine(\"glmnet\") |&gt;\n  set_mode(\"classification\")\n\nlog_ridge_spec &lt;- logistic_reg(penalty = tune(),\n                               mixture = 0) |&gt;\n  set_engine(\"glmnet\") |&gt;\n  set_mode(\"classification\")\n\nlog_lasso_spec &lt;- logistic_reg(penalty = tune(),\n                               mixture = 1) |&gt;\n  set_engine(\"glmnet\") |&gt;\n  set_mode(\"classification\")",
    "crumbs": [
      "Predictive Modeling of Microbiome Data using Tidymodeling",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Modelling</span>"
    ]
  },
  {
    "objectID": "apply01_model_data.html#change-hyperparameter-values",
    "href": "apply01_model_data.html#change-hyperparameter-values",
    "title": "13  Modelling",
    "section": "13.5 Change Hyperparameter Values",
    "text": "13.5 Change Hyperparameter Values\nThe hyperparameters penalty and mixture are extracted from the model specification. This is done to see the possible values that can be used in the grid search.\nFor the mixture, values between \\(0.05\\) and \\(1\\) is used which seem to be decent values.\n\nlog_spec |&gt; \n  extract_parameter_set_dials() |&gt; \n  extract_parameter_dials(\"mixture\")\n\nProportion of Lasso Penalty (quantitative)\nRange: [0.05, 1]\n\n\nFor the penalty, values between \\(10^-10\\) and \\(1\\) are used pr default. The range is increased to \\(5*10^1\\). The updated range is saved in a new object, but the information is not added to the models until the workflow is created further down. As seen from the output, the information do not pertain to a specific model, but simply contain ranges for different hyperparameters.\n\nlog_param_ranges &lt;- log_spec |&gt; \n  extract_parameter_set_dials() |&gt;\n  update(penalty = penalty(c(params$penalty_lower,\n                             params$penalty_higher)))\n\nlog_penalty_param_ranges &lt;- log_lasso_spec |&gt; \n  extract_parameter_set_dials() |&gt;\n  update(penalty = penalty(c(params$penalty_lower,\n                             params$penalty_higher)))\n\nlog_param_ranges\n\nCollection of 2 parameters for tuning\n\n identifier    type    object\n    penalty penalty nparam[+]\n    mixture mixture nparam[+]\n\nlog_penalty_param_ranges\n\nCollection of 1 parameters for tuning\n\n identifier    type    object\n    penalty penalty nparam[+]",
    "crumbs": [
      "Predictive Modeling of Microbiome Data using Tidymodeling",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Modelling</span>"
    ]
  },
  {
    "objectID": "apply01_model_data.html#create-the-workflow-set",
    "href": "apply01_model_data.html#create-the-workflow-set",
    "title": "13  Modelling",
    "section": "13.6 Create the Workflow Set",
    "text": "13.6 Create the Workflow Set\nThe recipes are combined with the model specifications to create a workflow set. The workflow set is used to fit the models to the data and evaluate the models. Notice here it is called a workflow set, as it contains multiple workflows. Each row of the workflow set is a workflow of its own which further contain the recipe and model.\nA workflow set for all combinations of recipes and models is created:\n\nworkflow_set &lt;- workflow_set(\n  preproc = list(stand = stand_recipe,\n                 pca = pca_recipe),\n  models = list(log = log_spec,\n                log_ridge = log_ridge_spec,\n                log_lasso = log_lasso_spec)\n)\n\nworkflow_set\n\n# A workflow set/tibble: 6 × 4\n  wflow_id        info             option    result    \n  &lt;chr&gt;           &lt;list&gt;           &lt;list&gt;    &lt;list&gt;    \n1 stand_log       &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n2 stand_log_ridge &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n3 stand_log_lasso &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n4 pca_log         &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n5 pca_log_ridge   &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n6 pca_log_lasso   &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n\n\nThe parameter objects which contain the hyperparameter ranges are added to the workflow:\n\nworkflow_set &lt;- workflow_set |&gt;\n  option_add(id = \"stand_log\",\n             param_info = log_param_ranges) |&gt;\n  option_add(id = \"pca_log\",\n             param_info = log_param_ranges) |&gt;\n  option_add(id = \"stand_log_ridge\",\n             param_info = log_penalty_param_ranges) |&gt;\n  option_add(id = \"stand_log_lasso\",\n             param_info = log_penalty_param_ranges) |&gt;\n  option_add(id = \"pca_log_ridge\",\n             param_info = log_penalty_param_ranges) |&gt;\n  option_add(id = \"pca_log_lasso\",\n             param_info = log_penalty_param_ranges)\nworkflow_set\n\n# A workflow set/tibble: 6 × 4\n  wflow_id        info             option    result    \n  &lt;chr&gt;           &lt;list&gt;           &lt;list&gt;    &lt;list&gt;    \n1 stand_log       &lt;tibble [1 × 4]&gt; &lt;opts[1]&gt; &lt;list [0]&gt;\n2 stand_log_ridge &lt;tibble [1 × 4]&gt; &lt;opts[1]&gt; &lt;list [0]&gt;\n3 stand_log_lasso &lt;tibble [1 × 4]&gt; &lt;opts[1]&gt; &lt;list [0]&gt;\n4 pca_log         &lt;tibble [1 × 4]&gt; &lt;opts[1]&gt; &lt;list [0]&gt;\n5 pca_log_ridge   &lt;tibble [1 × 4]&gt; &lt;opts[1]&gt; &lt;list [0]&gt;\n6 pca_log_lasso   &lt;tibble [1 × 4]&gt; &lt;opts[1]&gt; &lt;list [0]&gt;",
    "crumbs": [
      "Predictive Modeling of Microbiome Data using Tidymodeling",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Modelling</span>"
    ]
  },
  {
    "objectID": "apply01_model_data.html#tune-hyperparameters",
    "href": "apply01_model_data.html#tune-hyperparameters",
    "title": "13  Modelling",
    "section": "13.7 Tune hyperparameters",
    "text": "13.7 Tune hyperparameters\nAll workflows in the workflow set contain hyperparameters. These are tuned with a search grid using the function tune_grid(). As workflow_set contain multiple workflows, the tune_grid() function can be mapped over the workflows to tune all hyperparameters using workflow_map(), a purrr-like map function.\nSeveral settings for tuning the grid exists. This includes e.g. whether or not to parallelize, what output to save and how verbose the output should be, i.e. how much should be printed to std.out. The settings are set to:\n\ngrid_settings &lt;-\n   control_grid(\n      save_pred = TRUE,\n      parallel_over = \"everything\",\n      save_workflow = TRUE,\n      extract = function(x) x\n   )\n\nThe grid search is performed for a grid of size 10. This means that 10 different combinations of the hyperparameters are tried for each model.\n\ngrid_results &lt;- workflow_set |&gt;\n  workflow_map(\n    fn = \"tune_grid\",\n    seed = 1337,\n    resamples = count_matrix_clr_folds,\n    grid = params$grid_size,\n    control = grid_settings\n  )\n\ntidymodels comes with a lot of convenience functions. One of these is autoplot() that can, among other things, plot the results of the grid search. The best result of each workflow is selected based on the AUC.\n\nautoplot(\n  grid_results,\n  rank_metric = \"roc_auc\",\n  metric = \"roc_auc\",\n  type = \"wflow_id\"\n) +\n  geom_point(aes(y = mean)) +\n  lims(y = c(0.4, 1)) +\n  theme(legend.position = \"none\") +\n  facet_wrap(~ wflow_id,\n             ncol = 2)\n\n\n\n\n\n\n\n\nThe issue with wrappers, e.g. the convenience function autoplot(), is that they do not always provide the flexibility needed.\n\ngrid_results |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"roc_auc\") |&gt; \n  group_by(wflow_id) |&gt;\n  arrange(desc(mean)) |&gt;\n  mutate(rank = row_number()) |&gt;\n  ungroup() |&gt;\n  ggplot(aes(x = rank,\n             y = mean,\n             col = wflow_id)) +\n  geom_point() +\n  geom_errorbar(aes(ymin = mean - std_err,\n                    ymax = mean + std_err)) +\n  theme(legend.position = \"none\") +\n  labs(x = \"Rank\",\n       y = \"AUC\") +\n  facet_wrap(~ wflow_id,\n             ncol = 2, scales = \"free_y\")\n\n\n\n\n\n\n\n\nThe hyperparameters resulting in the best AUC is extracted for each workflow. Note that the value of mixture is an order of magnitude different for stand_log and pca_log. In the PCA space, the model emphasizes Lasso over Ridge. This is likely due to the PCA reducing the number of variables, which in turn seems to reduce the need for setting coefficients to 0 (Lasso regularization).\nNote that the value of mixture is close to zero for both Elastic Net models, which means that the model emphasizes Ridge over Lasso. Note also that the penalty is much lower for the workflow with PCA pre-processing. This is likely due to the PCA reducing the number of variables, which in turn seems to reduce the need for regularization.\n\nworkflow_ids &lt;- grid_results |&gt;\n  pull(wflow_id)\n\n\ntuning_params_result &lt;- workflow_ids |&gt;\n  purrr::map(~{grid_results |&gt;\n      extract_workflow_set_result(.x) |&gt;\n      select_best(metric = \"roc_auc\") |&gt; \n      mutate(wflow_id = .x)\n    }) |&gt;\n  bind_rows() |&gt;\n  mutate(mixture = case_when(stringr::str_detect(string = wflow_id,\n                                                 pattern = \"lasso\") ~ 1,\n                             stringr::str_detect(string = wflow_id,\n                                                 pattern = \"ridge\") ~ 0,\n                             TRUE ~ mixture)) |&gt;\n  select(wflow_id, penalty, mixture)\n  \ntuning_params_result\n\n# A tibble: 6 × 3\n  wflow_id         penalty mixture\n  &lt;chr&gt;              &lt;dbl&gt;   &lt;dbl&gt;\n1 stand_log       6.32e- 2  0.0560\n2 stand_log_ridge 3.24e+ 0  0     \n3 stand_log_lasso 3.85e- 2  1     \n4 pca_log         2.02e- 3  0.236 \n5 pca_log_ridge   1.13e-10  0     \n6 pca_log_lasso   1.13e-10  1",
    "crumbs": [
      "Predictive Modeling of Microbiome Data using Tidymodeling",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Modelling</span>"
    ]
  },
  {
    "objectID": "apply01_model_data.html#finalize-workflow",
    "href": "apply01_model_data.html#finalize-workflow",
    "title": "13  Modelling",
    "section": "13.8 Finalize workflow",
    "text": "13.8 Finalize workflow\nAs the hyperparameters have been tuned, the final workflow can be created. This is done by adding the hyperparameters to the workflow. When the hyperparameters have been added, the workflows can be fitted.\n\nworkflow_set_final &lt;- workflow_ids |&gt;\n  purrr::map(~{\n    grid_results |&gt;\n      extract_workflow(.x) |&gt; \n      finalize_workflow(tuning_params_result |&gt; \n                          filter(wflow_id == .x))\n    })\nnames(workflow_set_final) &lt;- workflow_ids\n\nTo exemplify, the pca_log workflow is shown from before finalizing and after finalizing. As can be seen, the hyperparameters are added to the workflow such that they have a value instead of tune().\n\ngrid_results |&gt;\n  extract_workflow(\"pca_log\")\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_pca()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = tune()\n  mixture = tune()\n\nComputational engine: glmnet \n\nworkflow_set_final$pca_log\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_pca()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = 0.00202417340370754\n  mixture = 0.235985305487236\n\nComputational engine: glmnet",
    "crumbs": [
      "Predictive Modeling of Microbiome Data using Tidymodeling",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Modelling</span>"
    ]
  },
  {
    "objectID": "apply01_model_data.html#fit-and-predict-for-the-final-workflow",
    "href": "apply01_model_data.html#fit-and-predict-for-the-final-workflow",
    "title": "13  Modelling",
    "section": "13.9 Fit and Predict for the Final Workflow",
    "text": "13.9 Fit and Predict for the Final Workflow\nWith the hyperparameters set, the workflows can be fit to the data, and the models can be evaluated. The last_fit() function is used to fit the model to the data. The split argument is set to the cross-validation object created earlier. This is done to avoid overfitting.\n\nworkflow_set_fit &lt;- workflow_ids |&gt;\n  purrr::map(~{\n    workflow_set_final[[.x]] |&gt;\n      last_fit(split = count_matrix_clr_split)\n  })\nnames(workflow_set_fit) &lt;- workflow_ids",
    "crumbs": [
      "Predictive Modeling of Microbiome Data using Tidymodeling",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Modelling</span>"
    ]
  },
  {
    "objectID": "apply01_model_data.html#predict-and-visualize-performance",
    "href": "apply01_model_data.html#predict-and-visualize-performance",
    "title": "13  Modelling",
    "section": "13.10 Predict and visualize performance",
    "text": "13.10 Predict and visualize performance\nFor each of the fitted models, the predictions are collected and the performance is evaluated. One option is to look at the confusion matrices, e.g.:\n\nworkflow_set_fit$stand_log |&gt;\n  collect_predictions()|&gt; \n  conf_mat(truth = Patient_status,\n           estimate = .pred_class)\n\n          Truth\nPrediction Healthy MS\n   Healthy       0  1\n   MS           10 81\n\n\nAnother more visual approach is to plot the ROC-curve. This is done for all models in the workflow set. First, the specificity and sensitivity is calculated for each model with a convenience function roc_curve()\n\nroc_auc_results &lt;- workflow_ids |&gt;\n  map( ~ {\n    workflow_set_fit[[.x]] |&gt;\n      collect_predictions() |&gt;\n      roc_curve(truth = Patient_status,\n                .pred_MS,\n                event_level = \"second\") |&gt;\n      mutate(wflow_id = .x)\n  }) |&gt; bind_rows()\n\nroc_auc_results\n\n# A tibble: 564 × 4\n   .threshold specificity sensitivity wflow_id \n        &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;    \n 1   -Inf             0         1     stand_log\n 2      0.498         0         1     stand_log\n 3      0.505         0         0.988 stand_log\n 4      0.570         0.1       0.988 stand_log\n 5      0.653         0.1       0.976 stand_log\n 6      0.714         0.1       0.963 stand_log\n 7      0.762         0.2       0.963 stand_log\n 8      0.781         0.2       0.951 stand_log\n 9      0.782         0.3       0.951 stand_log\n10      0.789         0.3       0.939 stand_log\n# ℹ 554 more rows\n\n\nFinally, the ROC-curve is plotted stratified by the workflow ID. The ROC-curve is a plot of the sensitivity against (1 - specificity). Sensitivity is the proportion of true positives out of all positives, i.e. true positive rate. In this case, the proportion of correctly classified MS patients out of all MS patients. Specificity is the proportion of true negatives out of all negatives. In this case, the proportion of correctly classified healthy patients out of all healthy patients. Since the x-axis plots (1 - specificity), it is actually the proportion of false positives out of all negatives, i.e. the false positive rate.\n\nroc_auc_results |&gt;\n  ggplot(aes(x = 1 - specificity,\n             y = sensitivity,\n             col = wflow_id)) + \n  geom_path(lwd = 1,\n            alpha = 0.6) +\n  geom_abline(lty = 3) + \n  coord_equal() + \n  scale_color_viridis_d()",
    "crumbs": [
      "Predictive Modeling of Microbiome Data using Tidymodeling",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Modelling</span>"
    ]
  },
  {
    "objectID": "apply01_model_data.html#fine-tuning-hyperparameters",
    "href": "apply01_model_data.html#fine-tuning-hyperparameters",
    "title": "13  Modelling",
    "section": "13.11 Fine Tuning Hyperparameters",
    "text": "13.11 Fine Tuning Hyperparameters\nThe hyperparameters are tuned using a grid search. The grid search is performed for a grid of size 200. The best result of each workflow is selected based on the AUC and is chosen strictly on the best numerical value. It is very possible, that a model with a slightly lower AUC is preferable due to simplicity. To investigate this, plots of the hyperparameters plotted against the AUC is created.\nIt can be easily illustrated with the autoplot() function. The plot is not very informative with multiple hyperparameters, as it is not shown how they affect the AUC in combination.\n\nautoplot(grid_results,\n         id = \"stand_log\",\n         metric = \"roc_auc\")\n\n\n\n\n\n\n\n\nFor the two ENet models, the hyperparameters are extracted manually and plotted against the AUC. For the Ridge and Lasso models, the hyperparameters performance is plotted using the autoplot function.\n\n13.11.1 Standard Preprocessing, Lasso\nBy inspecting the plot, it would seem the optimal penalty is indeed at the optimum roc_auc.\n\nautoplot(grid_results,\n         id = \"stand_log_lasso\",\n         metric = \"roc_auc\")\n\n\n\n\n\n\n\n\nThe manually chosen hyperparameter is collected.\n\nstand_log_lasso_penalty &lt;- grid_results |&gt;\n  extract_workflow_set_result(\"stand_log_lasso\") |&gt;\n  show_best(metric = \"roc_auc\",\n            n = 1) |&gt;\n  pull(penalty)\n\nstand_log_lasso_mixture &lt;- 1\n\n\n\n13.11.2 Standard Preprocessing, Ridge\nBy inspecting the plot, it would seem the optimal penalty is indeed at the optimum roc_auc.\n\nautoplot(grid_results,\n         id = \"stand_log_ridge\",\n         metric = \"roc_auc\")\n\n\n\n\n\n\n\n\nThe manually chosen hyperparameter is collected.\n\nstand_log_ridge_penalty &lt;- grid_results |&gt;\n  extract_workflow_set_result(\"stand_log_ridge\") |&gt;\n  show_best(metric = \"roc_auc\",\n            n = 1) |&gt;\n  pull(penalty)\n\nstand_log_ridge_mixture &lt;- 0\n\n\n\n13.11.3 Standard Preprocessing, Elastic Net\nFor the ENet models, two parameters are tuned: penalty and mixture. Instead of plotting against AUC, the hyperparameters are plotted against each other, and the color represents the AUC. The best AUC are the more purple colors. The plot shows that the best AUC is achieved with a penalty close to zero.\n\nstand_log_plot &lt;- grid_results |&gt;\n  extract_workflow_set_result(\"stand_log\") |&gt;\n  show_best(metric = \"roc_auc\",\n            n = params$grid_size) |&gt;\n  ggplot(aes(x = penalty,\n             y = mixture,\n             col = mean)) +\n  geom_point() +\n  scale_color_viridis_c(direction = -1) +\n  labs(title = \"Standard Preprocessing, Elastic Net\",\n       x = \"Penalty\",\n       y = \"Mixture\",\n       col = \"AUC\")\nstand_log_plot\n\n\n\n\n\n\n\n\nTo better see the effect of the hyperparameters on the AUC, the x-axis is limited as the higher values are observed close to \\(penalty = 0\\).\n\np &lt;- stand_log_plot +\n  lims(x = c(0, 1))\n\nplotly::ggplotly(p)\n\n\n\n\n\n\nstand_log_penalty &lt;- 0.06319747\n\nstand_log_mixture &lt;- 0.05603853\n\n\n\n13.11.4 PCA Preprocessing, Lasso\nBy inspecting the plot, it would seem the optimal penalty is indeed at the optimum roc_auc.\n\nautoplot(grid_results,\n         id = \"pca_log_lasso\",\n         metric = \"roc_auc\")\n\n\n\n\n\n\n\n\nThe manually chosen hyperparameter is collected.\n\npca_log_lasso_penalty &lt;- grid_results |&gt;\n  extract_workflow_set_result(\"pca_log_lasso\") |&gt;\n  show_best(metric = \"roc_auc\",\n            n = 1) |&gt;\n  pull(penalty)\n\npca_log_lasso_mixture &lt;- 1\n\n\n\n13.11.5 PCA Preprocessing, Ridge\nBy inspecting the plot, the ROC values do not vary much with the penalty. The optimal penalty is chosen to be the maximum penalty.\n\nautoplot(grid_results,\n         id = \"pca_log_ridge\",\n         metric = \"roc_auc\")\n\n\n\n\n\n\n\n\nThe manually chosen hyperparameter is collected.\n\npca_log_ridge_penalty &lt;- grid_results |&gt;\n  extract_workflow_set_result(\"pca_log_ridge\") |&gt;\n  show_best(metric = \"roc_auc\",\n            n = params$grid_size) |&gt;\n  pull(penalty) |&gt; \n  max()\n\npca_log_ridge_mixture &lt;- 0\n\n\n\n13.11.6 PCA Preprocessing, Elastic Net\nAgain, the hyperparameter values are plotted against each other, and the colour signifies the AUC. The best AUC are the more purple colors.\n\npca_log_plot &lt;- grid_results |&gt;\n  extract_workflow_set_result(\"pca_log\") |&gt;\n  show_best(metric = \"roc_auc\",\n            n = params$grid_size) |&gt;\n  ggplot(aes(x = penalty,\n             y = mixture,\n             col = mean)) +\n  geom_point() +\n  scale_color_viridis_c(direction = -1) +\n  labs(title = \"PCA Preprocessing, Elastic Net\",\n       x = \"Penalty\",\n       y = \"Mixture\",\n       col = \"AUC\")\npca_log_plot\n\n\n\n\n\n\n\n\nTo better see the effect of the hyperparameters on the AUC, the x-axis is limited as the higher values are observed close to \\(penalty = 0\\).\n\np &lt;- pca_log_plot +\n  lims(x = c(0, 1))\n\nplotly::ggplotly(p)\n\n\n\n\n\nBy inspecting the plotly object, the parameters are chosen.\n\npca_log_penalty &lt;- 0.3379935\n\npca_log_mixture &lt;- 0.12525226\n\n\n\n13.11.7 Gather Manually Chosen Hyperparameters\n\ntuning_params_result_manual &lt;- tribble(~wflow_id, ~penalty, ~mixture,\n                                       \"stand_log_lasso\", stand_log_lasso_penalty, 1,\n                                       \"stand_log_ridge\", stand_log_ridge_penalty, 0,\n                                       \"stand_log\", stand_log_penalty, stand_log_mixture,\n                                       \"pca_log_lasso\", pca_log_lasso_penalty, 1,\n                                       \"pca_log_ridge\", pca_log_ridge_penalty, 0,\n                                       \"pca_log\", pca_log_penalty, pca_log_mixture)",
    "crumbs": [
      "Predictive Modeling of Microbiome Data using Tidymodeling",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Modelling</span>"
    ]
  },
  {
    "objectID": "apply01_model_data.html#finalize-fit-and-predict-with-manually-chosen-hyperparameters",
    "href": "apply01_model_data.html#finalize-fit-and-predict-with-manually-chosen-hyperparameters",
    "title": "13  Modelling",
    "section": "13.12 Finalize, Fit and Predict with Manually Chosen Hyperparameters",
    "text": "13.12 Finalize, Fit and Predict with Manually Chosen Hyperparameters\nThe finalization, fitting and predicting is done as before, but with the manually chosen hyperparameters.\n\n# Finalize\nworkflow_set_final_manual &lt;- workflow_ids |&gt;\n  purrr::map(~{\n    grid_results |&gt;\n      extract_workflow(.x) |&gt; \n      finalize_workflow(tuning_params_result_manual |&gt; \n                          filter(wflow_id == .x))\n    })\nnames(workflow_set_final_manual) &lt;- workflow_ids\n\n# Fit\nworkflow_set_fit_manual &lt;- workflow_ids |&gt;\n  purrr::map(~{\n    workflow_set_final_manual[[.x]] |&gt;\n      last_fit(split = count_matrix_clr_split)\n  })\nnames(workflow_set_fit_manual) &lt;- workflow_ids\n\n# Predict\nroc_auc_results_manual &lt;- workflow_ids |&gt;\n  map( ~ {\n    workflow_set_fit_manual[[.x]] |&gt;\n      collect_predictions() |&gt;\n      roc_curve(truth = Patient_status,\n                .pred_MS,\n                event_level = \"second\") |&gt;\n      mutate(wflow_id = .x)\n  }) |&gt; bind_rows()\n\n# Visualize performance\nroc_auc_results_manual |&gt;\n  ggplot(aes(x = 1 - specificity,\n             y = sensitivity,\n             col = wflow_id)) + \n  geom_path(lwd = 1,\n            alpha = 0.6) +\n  geom_abline(lty = 3) + \n  coord_equal() + \n  scale_color_viridis_d()",
    "crumbs": [
      "Predictive Modeling of Microbiome Data using Tidymodeling",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Modelling</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Cox, Laura M., Amir Hadi Maghzi, Shirong Liu, Stephanie K. Tankou, Fyonn\nH. Dhang, Valerie Willocq, Anya Song, et al. 2021. “Gut Microbiome\nin Progressive Multiple Sclerosis.” Annals of Neurology\n89 (June): 1195–1211. https://doi.org/10.1002/ANA.26084.\n\n\nFaith, Daniel P. 1992. “Conservation Evaluation and Phylogenetic\nDiversity.” Biological Conservation 61 (January): 1–10.\nhttps://doi.org/10.1016/0006-3207(92)91201-3.\n\n\nShannon, C. E. 1948. “A Mathematical Theory of\nCommunication.” Bell System Technical Journal 27:\n623–56. https://doi.org/10.1002/J.1538-7305.1948.TB00917.X.",
    "crumbs": [
      "References"
    ]
  }
]
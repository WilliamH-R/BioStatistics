[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BioStatistics",
    "section": "",
    "text": "Preface\nThis book introduces the concept of Tidy Modelling through the package tidymodels, and is designed to be a practical guide to the tools and techniques that are used in the Tidy Modelling framework. It is aimed at people who are familiar with the basics of R and tidyverse and are interested in learning how to use the tidymodels package to build and evaluate models. Further, a series of statistical concepts often used in Bioinformatics and Biostatistics are introduced in the context of Tidy Modelling.\nTo cover all the topics, the book is divided into four parts. The first part introduces the tidymodels package and its core principles such as data splitting, pre-processing, model building, and evaluation. The second part introduces some statistical concepts such as hypothesis testing, linear regression, logistic regression, and diversity measures. The concepts are introduced in a structured manner. A problem is introduced followed by a description of the method and how it solves the introduced problem. Then, the math behind the method is introduced and lastly, visual representation. Deviations from this structure are made when necessary. The third part contain some commonly applied preprocessing techniques useful when analysing data with many variables as they focus on dimensionality reduction.\nThe final part combines the concepts introduced in the first three parts to build and evaluate models for a real-world dataset. A study pertaining to microbiome and disease prediction is used. The raw data are 16S rRNA amplicon reads used to describe multiple sclerosis (MS) (Cox et al. 2021). The samples were pre-processed using the pipeline described in the GitHub repository from which this book is created. After preprocessing, a count matrix was obtained with 456 observations (samples), and the abundance of 123 genera. For each observation, the sex and age of the person is known among other information. All of these features are used to build several models trying to predict the disease status of the person.\n\n\n\n\nCox, Laura M., Amir Hadi Maghzi, Shirong Liu, Stephanie K. Tankou, Fyonn H. Dhang, Valerie Willocq, Anya Song, et al. 2021. “Gut Microbiome in Progressive Multiple Sclerosis.” Annals of Neurology 89 (June): 1195–1211. https://doi.org/10.1002/ANA.26084.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "tidy00_intro.html",
    "href": "tidy00_intro.html",
    "title": "Tidy Modelling with R",
    "section": "",
    "text": "R is already a great tool for data wrangling, analysis and visualisation. However, when it comes to building and evaluating models, it can be a bit cumbersome. Different packages have different syntax and conventions, which makes it difficult to switch between them. The tidymodels package aims to solve this problem by providing a consistent and easy-to-use interface for building and evaluating models.\nThe tidymodels package is a collection of packages that work together to make the process of building and evaluating models easier and more consistent. It works excellently as an extension to tidyverse by using the same methodology and syntax. In this chapter, the principles of the tidymodels package is introduced as well as how to create, evaluate, and tune many models at a time.",
    "crumbs": [
      "Tidy Modelling with R"
    ]
  },
  {
    "objectID": "tidy01_intro_package.html",
    "href": "tidy01_intro_package.html",
    "title": "1  Introduction to Tidy Modelling",
    "section": "",
    "text": "1.1 Introdution to the Dataset\nFor simplicity, the iris dataset is used since the structure is simple, and readers should be familiar with it. As a quick reminder, the dataset consists of five variables (columns) and 150 observations (rows). The first four variables are doubles describing the size of the flower, used as features. The last variable, the response, is a factor indicating the species of flower, used as the response. To make this a binary logistic classification problem, and not multiclass, the setosa species have been excluded.\ndim(iris)\n\n[1] 100   5\n\niris |&gt; \n  slice_head(n = 5) |&gt;\n  str()\n\ntibble [5 × 5] (S3: tbl_df/tbl/data.frame)\n $ Sepal.Length: num [1:5] 7 6.4 6.9 5.5 6.5\n $ Sepal.Width : num [1:5] 3.2 3.2 3.1 2.3 2.8\n $ Petal.Length: num [1:5] 4.7 4.5 4.9 4 4.6\n $ Petal.Width : num [1:5] 1.4 1.5 1.5 1.3 1.5\n $ Species     : Factor w/ 2 levels \"versicolor\",\"virginica\": 1 1 1 1 1",
    "crumbs": [
      "Tidy Modelling with R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Tidy Modelling</span>"
    ]
  },
  {
    "objectID": "tidy01_intro_package.html#training--and-test-set",
    "href": "tidy01_intro_package.html#training--and-test-set",
    "title": "1  Introduction to Tidy Modelling",
    "section": "1.2 Training- and Test Set",
    "text": "1.2 Training- and Test Set\nAs is customary when modeling, the dataset is split into a training- and testing set. It is needed to check for class imbalance in case some responses (here species) are more likely in the dataset. As seen from the plot below, it is not the case.\n\niris |&gt; \n  ggplot(aes(x = Species,\n             fill = Species)) +\n  geom_bar() +\n  theme(text=element_text(size=13)) +\n  scale_y_continuous(expand = c(0, 0,\n                                0.01, 0.1))\n\n\n\n\n\n\n\n\nIf a class imbalance was observed, using the strata argument should be used as in the below to ensure that the training- and testing set have the same distribution of the response variable. The prop argument indicates that 90% of the data is used for training and 10% for testing.\n\niris_split &lt;- initial_split(iris,\n                            prop = 0.90,\n                            strata = Species)\niris_train &lt;- training(iris_split)\niris_test &lt;- testing(iris_split)\n\nAs it is not the case, just using the prop argument is sufficient.\n\niris_split &lt;- initial_split(iris,\n                            prop = 0.90)\niris_train &lt;- training(iris_split)\niris_test &lt;- testing(iris_split)",
    "crumbs": [
      "Tidy Modelling with R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Tidy Modelling</span>"
    ]
  },
  {
    "objectID": "tidy01_intro_package.html#fitting-a-model",
    "href": "tidy01_intro_package.html#fitting-a-model",
    "title": "1  Introduction to Tidy Modelling",
    "section": "1.3 Fitting A Model",
    "text": "1.3 Fitting A Model\nThe parsnip package standardizes creating models. When learning a new technique, here parsnip, it is a good idea to apply well known theory. Therefore, a linear logistic regression model is used to predict the species of a flower given the dimensions of the sepal and the petal. If the reader is unfamiliar with the theory, it is recommended to read the chapter on logistic regression (Chapter 8).\n\nlg_model &lt;- logistic_reg()\n\nDifferent packages contain different ways of applying a linear logistic regression model. The issues are more apparent for more advances models, where e.g. the arguments have different names or should be supplied in a different way. The parsnip package standardizes this. It is important to choose which engine to use - i.e. which package. The show_engines() function can be used to see which packages are available for the model. Here, the glm package is used.\n\n# Show available packages for the model\nshow_engines(\"logistic_reg\")\n\n# A tibble: 7 × 2\n  engine    mode          \n  &lt;chr&gt;     &lt;chr&gt;         \n1 glm       classification\n2 glmnet    classification\n3 LiblineaR classification\n4 spark     classification\n5 keras     classification\n6 stan      classification\n7 brulee    classification\n\nlg_model &lt;- lg_model |&gt; \n  set_engine(\"glm\")\n\nUsing the formula syntax, it is specified which variables are the features and the response. The tidy() function can be used to get a summary of the model in a tibble format.\n\nlg_fit &lt;- lg_model |&gt;\n  fit(Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width,\n      data = iris_train)\n\nlg_fit |&gt; tidy()\n\n# A tibble: 5 × 5\n  term         estimate std.error statistic p.value\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)    -41.6      24.6      -1.69  0.0912\n2 Sepal.Length    -2.35      2.33     -1.01  0.314 \n3 Sepal.Width     -6.94      4.49     -1.54  0.123 \n4 Petal.Length     9.55      4.86      1.97  0.0493\n5 Petal.Width     17.2       9.31      1.84  0.0653",
    "crumbs": [
      "Tidy Modelling with R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Tidy Modelling</span>"
    ]
  },
  {
    "objectID": "tidy01_intro_package.html#prediction",
    "href": "tidy01_intro_package.html#prediction",
    "title": "1  Introduction to Tidy Modelling",
    "section": "1.4 Prediction",
    "text": "1.4 Prediction\nMaking predictions is also standardized and requires the function predict(). The model seem to predict correctly in all cases.\n\niris_test |&gt; \n  select(Species) |&gt; \n  bind_cols(predict(lg_fit,\n                    new_data = iris_test))\n\n# A tibble: 10 × 2\n   Species    .pred_class\n   &lt;fct&gt;      &lt;fct&gt;      \n 1 versicolor versicolor \n 2 versicolor versicolor \n 3 versicolor versicolor \n 4 versicolor versicolor \n 5 versicolor versicolor \n 6 versicolor versicolor \n 7 virginica  virginica  \n 8 virginica  virginica  \n 9 virginica  virginica  \n10 virginica  virginica",
    "crumbs": [
      "Tidy Modelling with R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Tidy Modelling</span>"
    ]
  },
  {
    "objectID": "tidy02_mult_models.html",
    "href": "tidy02_mult_models.html",
    "title": "2  Creating Multiple Models",
    "section": "",
    "text": "2.1 Cross-validation\nThe reasonings for, and theoretical aspects of, cross-validation (CV) are expected to be already known. In the tidymodels universe, CV is setup as:\niris_folds &lt;- vfold_cv(iris_train,\n                       v = 10)\niris_folds\n\n#  10-fold cross-validation \n# A tibble: 10 × 2\n   splits         id    \n   &lt;list&gt;         &lt;chr&gt; \n 1 &lt;split [81/9]&gt; Fold01\n 2 &lt;split [81/9]&gt; Fold02\n 3 &lt;split [81/9]&gt; Fold03\n 4 &lt;split [81/9]&gt; Fold04\n 5 &lt;split [81/9]&gt; Fold05\n 6 &lt;split [81/9]&gt; Fold06\n 7 &lt;split [81/9]&gt; Fold07\n 8 &lt;split [81/9]&gt; Fold08\n 9 &lt;split [81/9]&gt; Fold09\n10 &lt;split [81/9]&gt; Fold10",
    "crumbs": [
      "Tidy Modelling with R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Creating Multiple Models</span>"
    ]
  },
  {
    "objectID": "tidy02_mult_models.html#recipes",
    "href": "tidy02_mult_models.html#recipes",
    "title": "2  Creating Multiple Models",
    "section": "2.2 Recipes",
    "text": "2.2 Recipes\nWhen it comes to modelling, creating and comparing multiple models is essential. This is easily achieved through the use of recipe() from the recipes package. A recipe is a collection of both a fomula and preprocessing steps. Here preprocessing steps is not applied, but could be: log-transforming data, creating dummy variables (e.g. one hot encoding), sum up low occurring categories to handle class imbalance and dimensionality reduction (see ?sec-preprocessing). A series of recipes are creating and combined:\n\nSL_rec &lt;- recipe(Species ~ Sepal.Length,\n                 data = iris_train)\n\nSLW_rec &lt;- recipe(Species ~ Sepal.Length + Sepal.Width,\n                  data = iris_train)\n\nSLW_int_rec &lt;- recipe(Species ~ Sepal.Length + Sepal.Width,\n                      data = iris_train) |&gt;\n  step_interact(~ Sepal.Length:Sepal.Width)\n\nPL_rec &lt;- recipe(Species ~ Petal.Length,\n                 data = iris_train)\n\nPLW_rec &lt;- recipe(Species ~ Petal.Length + Petal.Width,\n                  data = iris_train)\n\nPLW_int_rec &lt;- recipe(Species ~ Petal.Length + Petal.Width,\n                      data = iris_train) |&gt;\n  step_interact(~ Petal.Length:Petal.Width)\n\nrecipe_list &lt;- list(SL = SL_rec,\n                    SLW = SLW_rec,\n                    SLW_int = SLW_int_rec,\n                    PL = PL_rec,\n                    PLW = PLW_rec,\n                    PLW_int = PLW_int_rec)\n\nlg_models &lt;- workflow_set(preproc = recipe_list,\n                          models = list(logistic = logistic_reg()),\n                          cross = FALSE)\n\nEach of the models are fitted with a purr-like workflow function:\n\n# To save the predicted values and used workflows\nkeep_pred &lt;- control_resamples(save_pred = TRUE,\n                               save_workflow = TRUE)\n\n\nlg_models &lt;- lg_models |&gt; \n  workflow_map(\"fit_resamples\",\n               resamples = iris_folds,\n               control = keep_pred,\n               seed = 1337)\nlg_models\n\n# A workflow set/tibble: 6 × 4\n  wflow_id         info             option    result   \n  &lt;chr&gt;            &lt;list&gt;           &lt;list&gt;    &lt;list&gt;   \n1 SL_logistic      &lt;tibble [1 × 4]&gt; &lt;opts[2]&gt; &lt;rsmp[+]&gt;\n2 SLW_logistic     &lt;tibble [1 × 4]&gt; &lt;opts[2]&gt; &lt;rsmp[+]&gt;\n3 SLW_int_logistic &lt;tibble [1 × 4]&gt; &lt;opts[2]&gt; &lt;rsmp[+]&gt;\n4 PL_logistic      &lt;tibble [1 × 4]&gt; &lt;opts[2]&gt; &lt;rsmp[+]&gt;\n5 PLW_logistic     &lt;tibble [1 × 4]&gt; &lt;opts[2]&gt; &lt;rsmp[+]&gt;\n6 PLW_int_logistic &lt;tibble [1 × 4]&gt; &lt;opts[2]&gt; &lt;rsmp[+]&gt;",
    "crumbs": [
      "Tidy Modelling with R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Creating Multiple Models</span>"
    ]
  },
  {
    "objectID": "tidy02_mult_models.html#evaluation-metrics-across-models",
    "href": "tidy02_mult_models.html#evaluation-metrics-across-models",
    "title": "2  Creating Multiple Models",
    "section": "2.3 Evaluation Metrics Across Models",
    "text": "2.3 Evaluation Metrics Across Models\nTwo simple evaluation metrics for logistic regressions are the accuracy and Area Under the Curve (AUC). It is implicit that it is area under the Receiver Operating Characteristic (ROC) curve. To view these two evaluation metrics (the column mean):\n\ncollect_metrics(lg_models) |&gt; \n  select(-c(.config, preproc, .estimator)) |&gt; \n  filter(.metric == \"accuracy\") |&gt; \n  arrange(desc(mean))\n\n# A tibble: 6 × 6\n  wflow_id         model        .metric   mean     n std_err\n  &lt;chr&gt;            &lt;chr&gt;        &lt;chr&gt;    &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1 PL_logistic      logistic_reg accuracy 0.933    10  0.0296\n2 PLW_logistic     logistic_reg accuracy 0.933    10  0.0246\n3 PLW_int_logistic logistic_reg accuracy 0.933    10  0.0246\n4 SL_logistic      logistic_reg accuracy 0.756    10  0.0363\n5 SLW_logistic     logistic_reg accuracy 0.711    10  0.0444\n6 SLW_int_logistic logistic_reg accuracy 0.711    10  0.0474\n\ncollect_metrics(lg_models) |&gt; \n  select(-c(.config, preproc, .estimator)) |&gt; \n  filter(.metric == \"roc_auc\") |&gt; \n  arrange(desc(mean))\n\n# A tibble: 6 × 6\n  wflow_id         model        .metric  mean     n std_err\n  &lt;chr&gt;            &lt;chr&gt;        &lt;chr&gt;   &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1 PLW_logistic     logistic_reg roc_auc 0.989    10 0.00705\n2 PLW_int_logistic logistic_reg roc_auc 0.989    10 0.00705\n3 PL_logistic      logistic_reg roc_auc 0.987    10 0.00923\n4 SL_logistic      logistic_reg roc_auc 0.804    10 0.0633 \n5 SLW_logistic     logistic_reg roc_auc 0.798    10 0.0664 \n6 SLW_int_logistic logistic_reg roc_auc 0.795    10 0.0737 \n\n\nTo illustrate the two metrics:\n\nlg_models |&gt; \n  autoplot(metric = \"accuracy\") +\n  geom_label(aes(label = wflow_id)) +\n  theme(legend.position = \"none\") +\n  xlim(c(0.5, 6.5)) +\n  ggtitle(\"Accuracy Stratified on Model\") +\n  theme(text=element_text(size=13))\n\n\n\n\n\n\n\nlg_models |&gt; \n  collect_predictions() |&gt; \n  group_by(wflow_id) |&gt; \n  roc_curve(truth = Species,\n            .pred_versicolor) |&gt; \n  autoplot() +\n  ggtitle(\"ROC Rurve Stratified on Model\") +\n  theme(text=element_text(size=12))",
    "crumbs": [
      "Tidy Modelling with R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Creating Multiple Models</span>"
    ]
  },
  {
    "objectID": "tidy03_hyperparam.html",
    "href": "tidy03_hyperparam.html",
    "title": "3  Tuning Hyperparameters and Overfitting",
    "section": "",
    "text": "3.1 Introduction to Hyperparameters\nA classical and simple example of a hyperparameter is the number of neighbors, usually denoted k, in a k-nearest neighbors (KNN) algorithm. This is a hyperparameter as it is not estimated during model fitting, but is specified a priori making it impossible to optimize during parameter estimation.",
    "crumbs": [
      "Tidy Modelling with R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Tuning Hyperparameters and Overfitting</span>"
    ]
  },
  {
    "objectID": "tidy03_hyperparam.html#setting-up-tuning",
    "href": "tidy03_hyperparam.html#setting-up-tuning",
    "title": "3  Tuning Hyperparameters and Overfitting",
    "section": "3.2 Setting Up Tuning",
    "text": "3.2 Setting Up Tuning\nIn the tidymodels universe, hyperparameters are marked for tuning in the specifications for a model. To exemplify, both the number of nearest neighbors and a range of weight functions are tuned.\n\nknn_spec &lt;- nearest_neighbor(neighbors = tune(),\n                             weight_func = tune()) |&gt; \n  set_engine(engine = \"kknn\",\n             trace = 0) |&gt; \n  set_mode(\"classification\")\n\nSecondly, the recipe is set up. As no preprocessing is applied (e.g. log-transformation) it is quite simple.\n\nknn_rec &lt;- recipe(Species ~ ., # Use all other columns as features (called predictors in tidymodels)\n                  data = iris)\n\nThe specs and recipe is then combined into a workflow:\n\nknn_wflow &lt;- workflow() |&gt; \n  add_model(knn_spec) |&gt; \n  add_recipe(knn_rec)\n\nIt is possible to inspect which hyperparameters are being tuned, check which values that are tested and change those values. This is done through the use of the dials package.\n\n# Check hyperparameters\nknn_spec |&gt; extract_parameter_set_dials()\n\nCollection of 2 parameters for tuning\n\n  identifier        type    object\n   neighbors   neighbors nparam[+]\n weight_func weight_func dparam[+]\n\n# Check values tested\nknn_spec |&gt; extract_parameter_set_dials() |&gt; \n  extract_parameter_dials(\"weight_func\")\n\nDistance Weighting Function  (qualitative)\n\n\n10 possible values include:\n\n\n'rectangular', 'triangular', 'epanechnikov', 'biweight', 'triweight', 'cos', ... \n\n# Change values, save in new object\nknn_params &lt;- knn_spec |&gt;\n  extract_parameter_set_dials() |&gt;\n  update(weight_func = weight_func(c(\"cos\", \"inv\", \"gaussian\")),\n         neighbors = neighbors(c(1, 15)))\n\n# Check that it is updated\nknn_params |&gt;\n  extract_parameter_dials(\"weight_func\")\n\nDistance Weighting Function  (qualitative)\n\n\n3 possible values include:\n\n\n'cos', 'inv' and 'gaussian' \n\nknn_params |&gt;\n  extract_parameter_dials(\"neighbors\")\n\n# Nearest Neighbors (quantitative)\nRange: [1, 15]\n\n\nDifferent grid_* functions exist to combine the hyperparameters, e.g. grid_random() and grid_regular(). As exemplified below, grid_regular() combines the parameters in all possible ways dependent on the number of levels chosen.\n\ngrid_regular(knn_params,\n             levels = 4)\n\n# A tibble: 12 × 2\n   neighbors weight_func\n       &lt;int&gt; &lt;chr&gt;      \n 1         1 cos        \n 2         5 cos        \n 3        10 cos        \n 4        15 cos        \n 5         1 inv        \n 6         5 inv        \n 7        10 inv        \n 8        15 inv        \n 9         1 gaussian   \n10         5 gaussian   \n11        10 gaussian   \n12        15 gaussian",
    "crumbs": [
      "Tidy Modelling with R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Tuning Hyperparameters and Overfitting</span>"
    ]
  },
  {
    "objectID": "tidy03_hyperparam.html#measure-performance-of-tuning",
    "href": "tidy03_hyperparam.html#measure-performance-of-tuning",
    "title": "3  Tuning Hyperparameters and Overfitting",
    "section": "3.3 Measure Performance of Tuning",
    "text": "3.3 Measure Performance of Tuning\nA metric is needed to measure the performance of the hyperparameters. The ROC curve is used. The regular grid is tuned:\n\n# Performance metric\nroc &lt;- metric_set(roc_auc)\n\n# Tuning\nknn_tune &lt;- knn_wflow |&gt; \n  tune_grid(iris_folds,\n            grid = knn_params |&gt; grid_regular(levels = 4),\n            metrics = roc)\nknn_tune\n\n# Tuning results\n# 10-fold cross-validation \n# A tibble: 10 × 4\n   splits          id     .metrics          .notes          \n   &lt;list&gt;          &lt;chr&gt;  &lt;list&gt;            &lt;list&gt;          \n 1 &lt;split [90/10]&gt; Fold01 &lt;tibble [12 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 2 &lt;split [90/10]&gt; Fold02 &lt;tibble [12 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 3 &lt;split [90/10]&gt; Fold03 &lt;tibble [12 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 4 &lt;split [90/10]&gt; Fold04 &lt;tibble [12 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 5 &lt;split [90/10]&gt; Fold05 &lt;tibble [12 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 6 &lt;split [90/10]&gt; Fold06 &lt;tibble [12 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 7 &lt;split [90/10]&gt; Fold07 &lt;tibble [12 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 8 &lt;split [90/10]&gt; Fold08 &lt;tibble [12 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 9 &lt;split [90/10]&gt; Fold09 &lt;tibble [12 × 6]&gt; &lt;tibble [0 × 3]&gt;\n10 &lt;split [90/10]&gt; Fold10 &lt;tibble [12 × 6]&gt; &lt;tibble [0 × 3]&gt;\n\n\nTo visualize the performance:\n\nknn_tune |&gt; \n  unnest(cols = .metrics) |&gt; \n  select(id, .metric, neighbors, weight_func, .estimate) |&gt;\n  group_by(neighbors, weight_func) |&gt; \n  mutate(estimate_avg = mean(.estimate)) |&gt;\n  ggplot(aes(x = neighbors,\n             y = estimate_avg)) +\n  geom_point(size = 3) +\n  geom_line(linewidth = 0.7) +\n  scale_x_continuous(breaks = c(1, 5, 10, 15)) +\n  facet_wrap(~ weight_func) +\n  theme(text=element_text(size=13))",
    "crumbs": [
      "Tidy Modelling with R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Tuning Hyperparameters and Overfitting</span>"
    ]
  },
  {
    "objectID": "tidy03_hyperparam.html#finalize-hyperparameter-selection",
    "href": "tidy03_hyperparam.html#finalize-hyperparameter-selection",
    "title": "3  Tuning Hyperparameters and Overfitting",
    "section": "3.4 Finalize Hyperparameter Selection",
    "text": "3.4 Finalize Hyperparameter Selection\nIt would seem there is no visual difference between the weight functions. For the number of neighbors, the performance is highest for 10 and 15 neighbors. Preferably, the simplest of the two models is chosen.\n\nfinal_hyperparams &lt;- tibble(weight_func = \"gaussian\",\n                            neighbors = 10)\n\nfinal_knn_wflow &lt;- knn_wflow |&gt; \n  finalize_workflow(final_hyperparams)\nfinal_knn_wflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: nearest_neighbor()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nK-Nearest Neighbor Model Specification (classification)\n\nMain Arguments:\n  neighbors = 10\n  weight_func = gaussian\n\nEngine-Specific Arguments:\n  trace = 0\n\nComputational engine: kknn \n\n\nThe model can now be fit to the data and used for prediction.\n\nfinal_knn_fit &lt;- final_knn_wflow |&gt; \n  fit(iris)\nfinal_knn_fit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: nearest_neighbor()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\nkknn::train.kknn(formula = ..y ~ ., data = data, ks = min_rows(10,     data, 5), kernel = ~\"gaussian\", trace = ~0)\n\nType of response variable: nominal\nMinimal misclassification: 0.07\nBest kernel: gaussian\nBest k: 10",
    "crumbs": [
      "Tidy Modelling with R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Tuning Hyperparameters and Overfitting</span>"
    ]
  },
  {
    "objectID": "stat00_intro.html",
    "href": "stat00_intro.html",
    "title": "Applied Biostatistical Methods",
    "section": "",
    "text": "Load packages.\n\n\nShow the code\nlibrary(\"tidymodels\")\ntidymodels::tidymodels_prefer()\n\n\nIn this part, each of the following parts delve into an area of biostatistics. This includes a chapter on diversity measures, hypothesis testing and several chapters on models such as Ordinary Linear Regression. Each chapter is structured in a similar way. First, a commonly occurring problem is introduced, followed by an explanation of the method and how it solves the problem. Then, the math behind the method is then explained, and lastly, a visual representation. Deviations from this structure may occur when e.g. a text based introduction is insufficient.\nIn the first part of the book, the iris data set was used to exemplify the usage of the tidymodels package. Some of the following chapters reuses the iris data ses wheras some uses the also common mtcars data set. For the chapters pertaining to diversity measures, a count matrix from a metagenomics study is used instead. The raw data can be obtained from (Cox et al. 2021). Each row is a sample from a person, each column is a genus and the values represent the abundance of said genus in said sample. Explaining how the count matrix is acquired from raw sequences is out of scope for this book. Scripts used for the preprocessing exists in the GitHub repository from which this book is created, located in the src/ sub-folder.\nA snippet of the data set can be seen below:\n\ncount_matrix &lt;- readr::read_rds(\"https://github.com/WilliamH-R/BioStatistics/raw/main/data/count_matrix/count_matrix.rds\")\ncount_matrix\n\n# A tibble: 456 × 124\n   Sample      Actinomyces Adlercreutzia Agathobacter Akkermansia Alistipes\n   &lt;chr&gt;             &lt;int&gt;         &lt;int&gt;        &lt;int&gt;       &lt;int&gt;     &lt;int&gt;\n 1 SRR14214860           0            11          190         486       272\n 2 SRR14214861           0            12            0           5      1158\n 3 SRR14214862           0             0            0           0         0\n 4 SRR14214863           4             0          505          94       361\n 5 SRR14214864           0            45          744           3       794\n 6 SRR14214865           0            29          924         933        45\n 7 SRR14214866           0            41         1187         308       145\n 8 SRR14214867           0            45          648         323       193\n 9 SRR14214868           0             0         6973           0       287\n10 SRR14214869           0             0          362         270        30\n# ℹ 446 more rows\n# ℹ 118 more variables: Anaerofustis &lt;int&gt;, Anaerostipes &lt;int&gt;,\n#   Anaerotruncus &lt;int&gt;, Angelakisella &lt;int&gt;, Bacteroides &lt;int&gt;,\n#   Barnesiella &lt;int&gt;, Bifidobacterium &lt;int&gt;, Bilophila &lt;int&gt;, Blautia &lt;int&gt;,\n#   Butyricicoccus &lt;int&gt;, Butyricimonas &lt;int&gt;, `CAG-352` &lt;int&gt;, `CAG-56` &lt;int&gt;,\n#   `Candidatus Soleaferrea` &lt;int&gt;, `Candidatus Stoquefichus` &lt;int&gt;,\n#   Catenibacillus &lt;int&gt;, Catenibacterium &lt;int&gt;, …\n\n\nTo speed up data processing, and to make results less overwhelming, a subset of the count matrix is used at times as test data when exemplifying.\n\ncount_matrix_test &lt;- count_matrix |&gt; \n  slice_head(n = 10) |&gt; \n  select(Sample, Actinomyces,\n         Adlercreutzia, Agathobacter, Akkermansia)\ncount_matrix_test\n\n# A tibble: 10 × 5\n   Sample      Actinomyces Adlercreutzia Agathobacter Akkermansia\n   &lt;chr&gt;             &lt;int&gt;         &lt;int&gt;        &lt;int&gt;       &lt;int&gt;\n 1 SRR14214860           0            11          190         486\n 2 SRR14214861           0            12            0           5\n 3 SRR14214862           0             0            0           0\n 4 SRR14214863           4             0          505          94\n 5 SRR14214864           0            45          744           3\n 6 SRR14214865           0            29          924         933\n 7 SRR14214866           0            41         1187         308\n 8 SRR14214867           0            45          648         323\n 9 SRR14214868           0             0         6973           0\n10 SRR14214869           0             0          362         270\n\n\nMicrobiome data is inherently compositional, meaning that the sum of all genera in a sample is constant, so the count numbers indicate the proportion of a specific genus. As a consequence of compositional data, commonly used statistical tools are not applicable. To avoid this compositionality, the count matrix is transformed using the centered log-ratio (clr) transformation.\n\ncount_matrix_clr &lt;- readr::read_rds(\"https://github.com/WilliamH-R/BioStatistics/raw/main/data/count_matrix/count_matrix_clr.rds\")\ncount_matrix_clr\n\n# A tibble: 456 × 124\n   Sample      Actinomyces Adlercreutzia Agathobacter Akkermansia Alistipes\n   &lt;chr&gt;             &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n 1 SRR14214860        0           -1.83          1.02       1.96      1.38 \n 2 SRR14214861        0           -2.11          0         -2.98      2.46 \n 3 SRR14214862        0            0             0          0         0    \n 4 SRR14214863       -3.41         0             1.43      -0.255     1.09 \n 5 SRR14214864        0           -0.495         2.31      -3.20      2.38 \n 6 SRR14214865        0           -1.52          1.94       1.95     -1.08 \n 7 SRR14214866        0           -0.835         2.53       1.18      0.428\n 8 SRR14214867        0           -0.581         2.09       1.39      0.876\n 9 SRR14214868        0            0             3.85       0         0.658\n10 SRR14214869        0            0             1.39       1.10     -1.10 \n# ℹ 446 more rows\n# ℹ 118 more variables: Anaerofustis &lt;dbl&gt;, Anaerostipes &lt;dbl&gt;,\n#   Anaerotruncus &lt;dbl&gt;, Angelakisella &lt;dbl&gt;, Bacteroides &lt;dbl&gt;,\n#   Barnesiella &lt;dbl&gt;, Bifidobacterium &lt;dbl&gt;, Bilophila &lt;dbl&gt;, Blautia &lt;dbl&gt;,\n#   Butyricicoccus &lt;dbl&gt;, Butyricimonas &lt;dbl&gt;, `CAG-352` &lt;dbl&gt;, `CAG-56` &lt;dbl&gt;,\n#   `Candidatus Soleaferrea` &lt;dbl&gt;, `Candidatus Stoquefichus` &lt;dbl&gt;,\n#   Catenibacillus &lt;dbl&gt;, Catenibacterium &lt;dbl&gt;, …\n\n\n\nSession Info\n\nsessioninfo::session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.3 (2024-02-29 ucrt)\n os       Windows 11 x64 (build 22631)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_United Kingdom.utf8\n ctype    English_United Kingdom.utf8\n tz       Europe/Copenhagen\n date     2024-05-30\n pandoc   3.1.11 @ C:/Program Files/RStudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package      * version    date (UTC) lib source\n backports      1.4.1      2021-12-13 [1] CRAN (R 4.3.1)\n broom        * 1.0.5      2023-06-09 [1] CRAN (R 4.3.3)\n cachem         1.0.8      2023-05-01 [1] CRAN (R 4.3.3)\n class          7.3-22     2023-05-03 [2] CRAN (R 4.3.3)\n cli            3.6.2      2023-12-11 [1] CRAN (R 4.3.3)\n codetools      0.2-19     2023-02-01 [2] CRAN (R 4.3.3)\n colorspace     2.1-0      2023-01-23 [1] CRAN (R 4.3.3)\n conflicted     1.2.0      2023-02-01 [1] CRAN (R 4.3.3)\n data.table     1.15.4     2024-03-30 [1] CRAN (R 4.3.3)\n dials        * 1.2.1      2024-02-22 [1] CRAN (R 4.3.3)\n DiceDesign     1.10       2023-12-07 [1] CRAN (R 4.3.3)\n digest         0.6.35     2024-03-11 [1] CRAN (R 4.3.3)\n dplyr        * 1.1.4      2023-11-17 [1] CRAN (R 4.3.2)\n evaluate       0.23       2023-11-01 [1] CRAN (R 4.3.3)\n fansi          1.0.6      2023-12-08 [1] CRAN (R 4.3.3)\n fastmap        1.1.1      2023-02-24 [1] CRAN (R 4.3.3)\n foreach        1.5.2      2022-02-02 [1] CRAN (R 4.3.3)\n furrr          0.3.1      2022-08-15 [1] CRAN (R 4.3.3)\n future         1.33.2     2024-03-26 [1] CRAN (R 4.3.3)\n future.apply   1.11.2     2024-03-28 [1] CRAN (R 4.3.3)\n generics       0.1.3      2022-07-05 [1] CRAN (R 4.3.3)\n ggplot2      * 3.5.1      2024-04-23 [1] CRAN (R 4.3.3)\n globals        0.16.3     2024-03-08 [1] CRAN (R 4.3.3)\n glue           1.7.0      2024-01-09 [1] CRAN (R 4.3.3)\n gower          1.0.1      2022-12-22 [1] CRAN (R 4.3.1)\n GPfit          1.0-8      2019-02-08 [1] CRAN (R 4.3.3)\n gtable         0.3.5      2024-04-22 [1] CRAN (R 4.3.3)\n hardhat        1.3.1      2024-02-02 [1] CRAN (R 4.3.3)\n hms            1.1.3      2023-03-21 [1] CRAN (R 4.3.3)\n htmltools      0.5.8.1    2024-04-04 [1] CRAN (R 4.3.3)\n htmlwidgets    1.6.4      2023-12-06 [1] CRAN (R 4.3.3)\n infer        * 1.0.7      2024-03-25 [1] CRAN (R 4.3.3)\n ipred          0.9-14     2023-03-09 [1] CRAN (R 4.3.3)\n iterators      1.0.14     2022-02-05 [1] CRAN (R 4.3.3)\n jsonlite       1.8.8      2023-12-04 [1] CRAN (R 4.3.3)\n knitr          1.46       2024-04-06 [1] CRAN (R 4.3.3)\n lattice        0.22-5     2023-10-24 [2] CRAN (R 4.3.3)\n lava           1.8.0      2024-03-05 [1] CRAN (R 4.3.3)\n lhs            1.1.6      2022-12-17 [1] CRAN (R 4.3.3)\n lifecycle      1.0.4      2023-11-07 [1] CRAN (R 4.3.3)\n listenv        0.9.1      2024-01-29 [1] CRAN (R 4.3.3)\n lubridate      1.9.3      2023-09-27 [1] CRAN (R 4.3.3)\n magrittr       2.0.3      2022-03-30 [1] CRAN (R 4.3.3)\n MASS           7.3-60.0.1 2024-01-13 [2] CRAN (R 4.3.3)\n Matrix         1.6-5      2024-01-11 [2] CRAN (R 4.3.3)\n memoise        2.0.1      2021-11-26 [1] CRAN (R 4.3.3)\n modeldata    * 1.3.0      2024-01-21 [1] CRAN (R 4.3.3)\n munsell        0.5.1      2024-04-01 [1] CRAN (R 4.3.3)\n nnet           7.3-19     2023-05-03 [2] CRAN (R 4.3.3)\n parallelly     1.37.1     2024-02-29 [1] CRAN (R 4.3.3)\n parsnip      * 1.2.1      2024-03-22 [1] CRAN (R 4.3.3)\n pillar         1.9.0      2023-03-22 [1] CRAN (R 4.3.3)\n pkgconfig      2.0.3      2019-09-22 [1] CRAN (R 4.3.3)\n prodlim        2023.08.28 2023-08-28 [1] CRAN (R 4.3.3)\n purrr        * 1.0.2      2023-08-10 [1] CRAN (R 4.3.3)\n R6             2.5.1      2021-08-19 [1] CRAN (R 4.3.3)\n Rcpp           1.0.12     2024-01-09 [1] CRAN (R 4.3.3)\n readr          2.1.5      2024-01-10 [1] CRAN (R 4.3.3)\n recipes      * 1.0.10     2024-02-18 [1] CRAN (R 4.3.3)\n rlang          1.1.3      2024-01-10 [1] CRAN (R 4.3.3)\n rmarkdown      2.26       2024-03-05 [1] CRAN (R 4.3.3)\n rpart          4.1.23     2023-12-05 [2] CRAN (R 4.3.3)\n rsample      * 1.2.1      2024-03-25 [1] CRAN (R 4.3.3)\n rstudioapi     0.16.0     2024-03-24 [1] CRAN (R 4.3.3)\n scales       * 1.3.0      2023-11-28 [1] CRAN (R 4.3.3)\n sessioninfo    1.2.2      2021-12-06 [1] CRAN (R 4.3.3)\n survival       3.5-8      2024-02-14 [2] CRAN (R 4.3.3)\n tibble       * 3.2.1      2023-03-20 [1] CRAN (R 4.3.3)\n tidymodels   * 1.2.0      2024-03-25 [1] CRAN (R 4.3.3)\n tidyr        * 1.3.1      2024-01-24 [1] CRAN (R 4.3.3)\n tidyselect     1.2.1      2024-03-11 [1] CRAN (R 4.3.3)\n timechange     0.3.0      2024-01-18 [1] CRAN (R 4.3.3)\n timeDate       4032.109   2023-12-14 [1] CRAN (R 4.3.2)\n tune         * 1.2.1      2024-04-18 [1] CRAN (R 4.3.3)\n tzdb           0.4.0      2023-05-12 [1] CRAN (R 4.3.3)\n utf8           1.2.4      2023-10-22 [1] CRAN (R 4.3.3)\n vctrs          0.6.5      2023-12-01 [1] CRAN (R 4.3.3)\n withr          3.0.0      2024-01-16 [1] CRAN (R 4.3.3)\n workflows    * 1.1.4      2024-02-19 [1] CRAN (R 4.3.3)\n workflowsets * 1.1.0      2024-03-21 [1] CRAN (R 4.3.3)\n xfun           0.43       2024-03-25 [1] CRAN (R 4.3.3)\n yaml           2.3.8      2023-12-11 [1] CRAN (R 4.3.2)\n yardstick    * 1.3.1      2024-03-21 [1] CRAN (R 4.3.3)\n\n [1] C:/Users/Willi/AppData/Local/R/win-library/4.3\n [2] C:/Program Files/R/R-4.3.3/library\n\n──────────────────────────────────────────────────────────────────────────────\n\n\n\n\n\n\nCox, Laura M., Amir Hadi Maghzi, Shirong Liu, Stephanie K. Tankou, Fyonn H. Dhang, Valerie Willocq, Anya Song, et al. 2021. “Gut Microbiome in Progressive Multiple Sclerosis.” Annals of Neurology 89 (June): 1195–1211. https://doi.org/10.1002/ANA.26084.",
    "crumbs": [
      "Applied Biostatistical Methods"
    ]
  },
  {
    "objectID": "stat01_diversity.html",
    "href": "stat01_diversity.html",
    "title": "4  Diversity Measures",
    "section": "",
    "text": "4.1 Alpha Diversity\nGenerally, alpha diversity measures the within-sample diversity generating a metric for diversity for each sample in the data. It then focuses on the spread and count of different species (here genera) for each sample. Multiple metrics for alpha diversity exists. A crude way is simply to count unique organisms per sample. One could also take the distribution or phylogeny into account.\nThe R package vegan is widely used to help calculate diversity and is used here.",
    "crumbs": [
      "Applied Biostatistical Methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Diversity Measures</span>"
    ]
  },
  {
    "objectID": "stat01_diversity.html#alpha-diversity",
    "href": "stat01_diversity.html#alpha-diversity",
    "title": "4  Diversity Measures",
    "section": "",
    "text": "4.1.1 Species Richness\nAs mentioned above, a crude way to measure diversity is to count the number of unique organisms in a given taxon for each sample. That is exactly what the species richness is. Through the vegan package, it can be calculated as follows:\n\nrichness &lt;- count_matrix_test |&gt;\n  column_to_rownames(var = \"Sample\") |&gt; \n  specnumber() |&gt;\n  as_tibble(rownames = \"Sample\") |&gt;\n  rename(Richness = value)\n\ncount_matrix_test |&gt;\n  left_join(richness,\n            by = \"Sample\")\n\n# A tibble: 10 × 6\n   Sample      Actinomyces Adlercreutzia Agathobacter Akkermansia Richness\n   &lt;chr&gt;             &lt;int&gt;         &lt;int&gt;        &lt;int&gt;       &lt;int&gt;    &lt;int&gt;\n 1 SRR14214860           0            11          190         486        3\n 2 SRR14214861           0            12            0           5        2\n 3 SRR14214862           0             0            0           0        0\n 4 SRR14214863           4             0          505          94        3\n 5 SRR14214864           0            45          744           3        3\n 6 SRR14214865           0            29          924         933        3\n 7 SRR14214866           0            41         1187         308        3\n 8 SRR14214867           0            45          648         323        3\n 9 SRR14214868           0             0         6973           0        1\n10 SRR14214869           0             0          362         270        2\n\n\n\n\n4.1.2 Shannon Index\nThe measure is usually referred to as Shannon Index or Shannon Entropy. It is able to take both the richness (described above) and the evenness into account. The evenness refers to how evenly the abundances are distributed among taxa for a sample. The richness is considered when summing over all taxonomic units at a specified taxonomic rank. The index is calculated via the following formula:\n\\[\nH' = -\\sum_{i=1}^{R} p_i \\ln(p_i)\n\\]\nWhere \\(R\\) is the number of observed species within a sample, i.e. the Richness. \\(p_{i}\\) is the proportion of the abundance belonging to the \\(i\\)’th species out of the total abundance for a specific sample (Shannon 1948).\nTo illustrate the meaning of the value of the Shannon Index, a few examples are calculated. In the first scenario, three species and three samples exists. For the first sample, the proportion of abundances are spread evenly between the three species. For the second and third sample, the proportions are more skewered towards one species. These examples show how the evenness affect the Shannon Index.\n\ntribble(~sample,    ~species_1, ~species_2, ~species_3,\n        \"sample_A\", 0.3,        0.3,        0.3,\n        \"sample_B\", 0.6,        0.2,        0.2,\n        \"sample_C\", 1,          0,          0)\n\nFor each of the three samples, the calculation would be as follows:\n\\[\nH'_{sample_{A}} = -(0.3 \\cdot ln{(0.3)} + 0.3 \\cdot ln{(0.3)} + 0.3 \\cdot ln{(0.3)}) = 1.08\n\\]\n\\[\nH'_{sample_{B}} = -(0.6 \\cdot ln{(0.6)} + 0.2 \\cdot ln{(0.2)} + 0.2 \\cdot ln{(0.2)}) = 0.95\n\\]\n\\[\nH'_{sample_{C}} = -(1 \\cdot ln{(1)}) = 0\n\\]\nWhen the samples are evenly distributed, we observe a higher Shannon Index.\nIn the following example, the number of species are increased to five. Three different samples with different distributions are still investigated.\n\ntribble(~sample,    ~species_1, ~species_2, ~species_3, ~species_4, ~species_5,\n        \"sample_D\", 0.2,        0.2,        0.2,        0.2,        0.2,\n        \"sample_E\", 0.6,        0.1,        0.1,        0.1,        0.1,\n        \"sample_F\", 1,          0,          0,          0,          0)\n\n\\[\nH'_{sample_{D}} = 1.61\n\\]\n\\[\nH'_{sample_{E}} = 1.23\n\\]\n\\[\nH'_{sample_{F}} = 0\n\\]\nAs before, the Shannon Index decreases with a more skewered distribution. Both \\(sample_{A}\\) and \\(sample_{D}\\) have even distributions, but the Shannon Index for \\(sample_{D}\\) higher since the richness is higher, i.e. higher number of species with an even distribution.\nThe diversity in a sample is said to be high when the number of species are high, and the distribution is even. As shown above, that would yield a high Shannon Index. So, to conclude, a high Shannon Index suggests high diversity.\nFinally, Shannon Index calculated for the test set. The index does seem to be higher when the genera are more evenly distributed as show in the above.\n\nshannon &lt;- count_matrix_test |&gt;\n  column_to_rownames(var = \"Sample\") |&gt;\n  diversity(index = \"shannon\") |&gt; \n  as_tibble(rownames = \"Sample\") |&gt; \n  rename(Shannon = value)\n\ncount_matrix_test |&gt;\n  left_join(shannon,\n            by = \"Sample\")\n\n# A tibble: 10 × 6\n   Sample      Actinomyces Adlercreutzia Agathobacter Akkermansia Shannon\n   &lt;chr&gt;             &lt;int&gt;         &lt;int&gt;        &lt;int&gt;       &lt;int&gt;   &lt;dbl&gt;\n 1 SRR14214860           0            11          190         486   0.667\n 2 SRR14214861           0            12            0           5   0.606\n 3 SRR14214862           0             0            0           0   0    \n 4 SRR14214863           4             0          505          94   0.472\n 5 SRR14214864           0            45          744           3   0.243\n 6 SRR14214865           0            29          924         933   0.762\n 7 SRR14214866           0            41         1187         308   0.618\n 8 SRR14214867           0            45          648         323   0.789\n 9 SRR14214868           0             0         6973           0   0    \n10 SRR14214869           0             0          362         270   0.683\n\n\n\n\n4.1.3 Species Evenness\nEvenness is an overall measure of how evenly the abundances are distributed among taxa for a sample. Different metrics exists, here the Pielou Index is used. The value ranges from 0 to 1, where 1 indicates that the abundances are exactly evenly distributed among all species and 0 indicates that the abundance is dominated by one species. The Pielou Index is calculated by dividing the Shannon Index of the sample by the theoretical maximum Shannon Index for the sample given its size. Thereby, the Pielou Index is not dependent on Richness due to the division (Pielou 1966). The theoretical maximum Shannon Index turns out to be more simply calculated as the natural logarithm of the number of species in the sample. The formula is as follows:\n\\[\nJ = \\frac{H'}{\\ln(R)}\n\\]\nWhere \\(R\\) is the number of observed species within a sample, and \\(H'\\) is the Shannon Index within the sample.\nThe evenness is calculated for the test set as follows:\n\nshannon |&gt;\n  mutate(Pielou = Shannon / log(n()))\n\n# A tibble: 10 × 3\n   Sample      Shannon Pielou\n   &lt;chr&gt;         &lt;dbl&gt;  &lt;dbl&gt;\n 1 SRR14214860   0.667  0.289\n 2 SRR14214861   0.606  0.263\n 3 SRR14214862   0      0    \n 4 SRR14214863   0.472  0.205\n 5 SRR14214864   0.243  0.105\n 6 SRR14214865   0.762  0.331\n 7 SRR14214866   0.618  0.268\n 8 SRR14214867   0.789  0.343\n 9 SRR14214868   0      0    \n10 SRR14214869   0.683  0.296\n\n\n\n\n4.1.4 Faith’s Phylogenetic Diversity\nFaith’s Phylogenetic Diversity (Faith’s PD) consider phylogenetic distances within a sample. The PD for a sample is calculated as the total sum of branch lengths from a phylogenetic tree, where the tree only contains the organisms found in a given sample (Faith 1992). Imagine a scenario with four organisms, where organism a and b are closely related, and so are organism c and d:\n\n\n\n\n\n\nFigure 4.1: Phylogenetic tree\n\n\n\nIn the case where all four organisms were found in the same sample, the PD would be calculated as:\n\\[\nPD = 15 + 6 + 4 + 3 + 1 + 2 + 4 = 35\n\\]\nIf organism a was lost, the PD would be decreased with 4 units to a value of 31. A higher PD is thus indicative of a higher diversity.",
    "crumbs": [
      "Applied Biostatistical Methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Diversity Measures</span>"
    ]
  },
  {
    "objectID": "stat01_diversity.html#beta-diversity",
    "href": "stat01_diversity.html#beta-diversity",
    "title": "4  Diversity Measures",
    "section": "4.2 Beta Diversity",
    "text": "4.2 Beta Diversity\nBeta diversity describes the between-sample diversity. The diversity can be calculated in different ways, and the choice of method is dependent on the data and the question at hand. Common methods include Bray-Curtis dissimilarity, UniFrac Distance, and Jaccard similarity index (sometimes called Jaccard similarity coefficient).\nA crude way to calculate beta diversity is to simply count the number of organisms which exists in one sample, but not the other. The mathematical expression is:\n\\[\n\\beta =  (\\alpha_{sample_{1}} - c) + (\\alpha_{sample_{2}} - c)\n\\]\nWhere \\(\\alpha_{sample_{1}}\\) and \\(\\alpha_{sample_{2}}\\) are the number of unique organisms in sample 1 and 2 (the richness), respectively. \\(c\\) is the number of organisms which exists in both samples. As the number of shared organisms are subtracted from the richness for each sample, the resulting value is the number of unique organisms in each sample, and the sum increases with the number of unique organisms in the two samples. Ultimately, this means a high value of \\(\\beta\\) indicates a high dissimilarity, i.e. a high diversity.\nAs an example, consider the following two samples (the top two rows of the test set):\n\ncount_matrix_test |&gt;\n  slice_head(n = 2)\n\n# A tibble: 2 × 5\n  Sample      Actinomyces Adlercreutzia Agathobacter Akkermansia\n  &lt;chr&gt;             &lt;int&gt;         &lt;int&gt;        &lt;int&gt;       &lt;int&gt;\n1 SRR14214860           0            11          190         486\n2 SRR14214861           0            12            0           5\n\n\nSample 1 (SRR14214860 ) has a richness of 3 and sample 2 (SRR14214861) has a richness of 2. They share 2 genera. The beta diversity would be calculated as:\n\\[\n\\beta = (3 - 2) + (2 - 2) = 1\n\\]\n\n4.2.1 Jaccard Similarity Index\nThe Jaccard index is another way to calculate beta diversity where only the presence/absence of an organism is taken into account. It is calculated as the number of shared organisms (intersection) divided by the total number organisms (union) in the two samples. The equation is as follows::\n\\[\nJ(A, B) = \\frac{|A \\cap B|}{|A \\cup B|}\n\\]\nFrom the equation we see why it is called a similarity index. The union (denominator) is just the number of observed organisms, and the intersection (numerator) is the number of shared organisms. The Jaccard index then represent the fraction of the observed organisms which were observed in both samples, i.e. the similarity of the two samples. The value ranges from 0 to 1, where 0 indicates that the two samples do not share any organisms, and 1 indicates that the two samples share the exact same organisms.\nWhen comparing the two first samples in the subset, it is apparent that they share 2 genera, and a total of 3 genera is present between the two samples. The calculation gives:\n\\[\nJ(SRR14214860, SRR14214861) = \\frac{2}{3} \\approx 0.67\n\\]\nTo calculate the Jaccard index by using the vegan package, the argument binary is set to TRUE to indicate that the data is binary, i.e. presence/absence of organisms instead of abundance. It is also important to note, that the vegan package actually calculates the dissimilarities, \\(1 - \\frac{|A \\cap B|}{|A \\cup B|}\\) so a higher value indicates a higher dissimilarity. The Jaccard index is calculated for the test data as follows:\n\ncount_matrix_test |&gt;\n  slice_head(n = 2) |&gt;\n  column_to_rownames(var = \"Sample\")|&gt;\n  vegdist(method = \"jaccard\",\n          binary = TRUE)\n\n            SRR14214860\nSRR14214861   0.3333333\n\n\nThe result is in agreement with the manual calculation since the vegan package calculates the dissimilarity. When inspecting the two samples, they also appear somewhat similar, as the majority of the organisms are present in both sample (even though talking about a majority is a bit misleading when only four organisms are compared). The abundances of the organisms are quite different, and to take that into account, another method is needed.\n\n\n4.2.2 Bray-Curtis Dissimilarity\nThis measure is widely used to calculate beta diversity. Bray-Curtis dissimilarity takes the composition and abundance of the two compared samples into account. It is also important to note that it is a dissimilarity measure, so a higher value indicates a higher dissimilarity. The value ranges from 0 to 1. A value of 0 indicates that the two samples are identical, while a value of 1 indicates that the two samples do not share any organisms. The metric is calculated as:\n\\[\nBC_{jk} = \\frac{\\sum_{i=1}^{R} |S_{ij} - S_{ik}|}{\\sum_{i=1}^{R} (S_{ij} + S_{ik})}\n\\]\nWhere \\(S_{ij}\\) and \\(S_{ik}\\) are the abundances of the \\(i\\)’th organism in sample \\(j\\) and \\(k\\), respectively. \\(R\\) is the number of organisms (Bray and Curtis 1957). With the test data, the Bray-Curtis dissimilarity is calculated as follows:\n\\[\nBC_{SRR14214860, SRR14214861} = \\frac{|0 - 0| + |11 - 12| + |190 - 0| + |486 - 5|}\n                                     {(0 + 0) + (11 + 12) + (190 + 0) + (486 + 5)}\n                                     = 0.955\n\\]\nBy using the vegan package the Bray-Curtis dissimilarity can be calculated as follows:\n\ncount_matrix_test |&gt;\n  slice_head(n = 2) |&gt;\n  column_to_rownames(var = \"Sample\")|&gt;\n  vegdist(method = \"bray\")\n\n            SRR14214860\nSRR14214861   0.9545455\n\n\nThe results are in agreement. The value is close to 1, indicating a high dissimilarity between the two samples. Interestingly, this is a different find compared to the Jaccard index, which indicated a higher similarity between the two samples. The abundances of the organisms in the two samples are quite different, and the Bray-Curtis dissimilarity takes that into account.\n\n\n4.2.3 UniFrac Distance\nThe method is based on the phylogenetic tree of the organisms and thereby take the phylogenetic distances into account. UniFrac is shorthand notation for Unique Fraction and is calculated as the sum of unique branch lengths in the phylogenetic tree over the sum of shared branch lengths:\n\\[\nU = \\frac{sum \\; of \\; unique \\; branches' \\; length}{sum \\; of \\; all \\; branches'  \\; length}\n\\]\nThe possible values range from 0 to 1. In the case where the two compared samples share all organisms, the value of the numerator would sum to 0, and the UniFrac would be 0. If the two samples do not share any organisms, the value of the numerator would sum to the total length of the phylogenetic tree (the denominator), and the UniFrac would be 1.\nImagine a phylogenetic tree made from two samples, red and blue. Four different organisms were found in total as seen in the below figure. A red colour indicate an organism found in the red sample, a blue colour indicate an organism found in the blue sample and a purple colour indicate an organism was found in both samples.\n\n\n\n\n\n\nFigure 4.2: Phylogenetic tree with cohorts\n\n\n\nFollowing the equation above, the UniFrac distance would be calculated as:\n\\[\nU = \\frac{4 + 2 + 4}{15 + 6 + 4 + 3 + 1 + 2 + 4} = 0.286\n\\]\nThe value indicates that the blue and red samples are quite similar.\nThe method is divided into two different types, unweighted and weighted. It is the unweighted method which is described above. The weighted method takes the abundance of the organisms into account. The weighted method biases towards the most abundant organisms, while the unweighted method biases more towards the rare organisms. A third version exists, called Generalized UniFrac. This metric has another parameter, usually called \\(\\alpha\\), which can be used to control the balance between the unweighted and weighted methods.",
    "crumbs": [
      "Applied Biostatistical Methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Diversity Measures</span>"
    ]
  },
  {
    "objectID": "stat02_wilcoxon.html",
    "href": "stat02_wilcoxon.html",
    "title": "5  Wilcoxon Test",
    "section": "",
    "text": "Set seed and load packages.\n\n\nShow the code\nset.seed(1337)\n\nlibrary(\"tidymodels\")\ntidymodels::tidymodels_prefer()\nlibrary(\"vegan\")\n\n\nLoad data.\n\n\nShow the code\ncount_matrix &lt;- readr::read_rds(\"https://github.com/WilliamH-R/BioStatistics/raw/main/data/count_matrix/count_matrix.rds\") |&gt; \n  select(-\"NA\")\n\nmeta &lt;- read.csv(file = \"data/metadata.txt\") |&gt; \n  as_tibble() |&gt;\n  select(Run, chem_administration, ETHNICITY, geo_loc_name,\n         Host_age, host_body_mass_index, Host_disease, host_phenotype, host_sex) |&gt; \n  rename(Sample = Run,\n         Treatment = chem_administration,\n         Ethnicity = ETHNICITY,\n         Location = geo_loc_name,\n         Age = Host_age,\n         BMI = host_body_mass_index,\n         Disease_severity = Host_disease,\n         EDSS = host_phenotype,\n         Sex = host_sex) |&gt;\n  mutate(Patient_status = case_when(Disease_severity == \"1HealthyControl\" ~ \"Healthy\",\n                                    TRUE ~ \"MS\"),\n         EDSS = as.factor(EDSS),\n         EDSS = case_when(is.na(EDSS) & Disease_severity == \"1HealthyControl\" ~ \"-1\",\n                          is.na(EDSS) & Disease_severity != \"1HealthyControl\" ~ \"Unknown\",\n                          TRUE ~ EDSS),\n         EDSS = as.factor(EDSS))\n\n\nAfter calculating some metric for two groups, e.g. the Shannon Index as a measure of diversity introduced in Section 4.1.2, one is left with two groups of values. Simply stating the numerical difference between the groups is not enough to determine if the difference is statistically significant. To do this, hypothesis testing is used. The null hypothesis is that there is no difference between the groups, and the alternative hypothesis is that there is a difference.\nA plethora of statistical tests exists. The choice of test depends on the type of data. For example, if the data is normally distributed, the t-test can be used to compare the means of two groups. If the data is not normally distributed, the Wilcoxon test is an option. Below, the Shannon Index for the full data set have been calculated and plotted stratified by the patient status (Healthy vs Multiple Sclerosis (MS)). As can be seen from the plots, the Shannon Index does not seem to follow a normal distribution for both groups.\n\nshannon &lt;- count_matrix |&gt;\n  column_to_rownames(var = \"Sample\") |&gt;\n  diversity(index = \"shannon\") |&gt; \n  as_tibble(rownames = \"Sample\") |&gt; \n  rename(Shannon = value) |&gt; \n  left_join(meta,\n            by = \"Sample\")\n\nshannon |&gt;\n  ggplot(aes(x = Shannon,\n             fill = Patient_status)) +\n  geom_bar(stat = \"bin\",\n           binwidth = 0.1) +\n  facet_wrap(~ Patient_status) +\n  scale_y_continuous(expand = c(0, 0,\n                                0.01, 0.01)) +\n  theme(text=element_text(size=13))\n\n\n\n\n\n\n\n\nThe Wilcoxon test is a non-parametric test, which means it do not assume that the samples have been taken from a specific distribution, e.g. the normal distribution. Two different Wilcoxon tests exists, one for comparing dependent samples and one for comparing independent samples. Samples would be dependent if they were e.g. taken from the same patient at different times. Since the samples compared here originates from different patients, the samples are independent. Hence, the optimal test to use is the Wilcoxon Rank Sum test, also called the Mann-Whitney U test.\nThe test is rank-based which means all the data is first ranked and then used to calculate the test statistic. After ranking the data, the sum of the ranks for the two groups are calculated. U is then calculated as:\n\\[\nU = min(U_1, \\; U_2) = min(R_1 - \\frac{n_1(n_1+1)} {2}, \\; R_2 - \\frac{n_2(n_2+1)} {2})\n\\]\nWhere \\(R_1\\) and \\(R_2\\) is the sum of the ranks for the first and second group, respectively. Likewise, \\(n_1\\) and \\(n_2\\) is the number of samples in the first and second group, respectively. The U statistic is then used to find the p-value, which is used to determine if the difference between the two groups is statistically significant. The p-value is the probability of observing a U statistic as extreme as the one observed, given that the null hypothesis of no difference between \\(U_1\\) and \\(U_2\\) is true. The Wilcoxon Rank Sum test for the Shannon Index is shown below. As the p-value is much lower than the usual significance level of 0.05, we can conclude that the Shannon Index is significantly different between the two groups. Here, the two groups are the healthy and MS patients.\n\nwilcox.test(Shannon ~ Patient_status,\n            data = shannon |&gt; filter(Sex == \"male\")) |&gt; \n  broom::tidy()\n\n# A tibble: 1 × 4\n  statistic p.value method                                           alternative\n      &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                                            &lt;chr&gt;      \n1       450 0.00928 Wilcoxon rank sum test with continuity correcti… two.sided  \n\n\n\n6 Session Info\n\nsessioninfo::session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.3 (2024-02-29 ucrt)\n os       Windows 11 x64 (build 22631)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_United Kingdom.utf8\n ctype    English_United Kingdom.utf8\n tz       Europe/Copenhagen\n date     2024-05-30\n pandoc   3.1.11 @ C:/Program Files/RStudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package      * version    date (UTC) lib source\n backports      1.4.1      2021-12-13 [1] CRAN (R 4.3.1)\n broom        * 1.0.5      2023-06-09 [1] CRAN (R 4.3.3)\n cachem         1.0.8      2023-05-01 [1] CRAN (R 4.3.3)\n class          7.3-22     2023-05-03 [2] CRAN (R 4.3.3)\n cli            3.6.2      2023-12-11 [1] CRAN (R 4.3.3)\n cluster        2.1.6      2023-12-01 [2] CRAN (R 4.3.3)\n codetools      0.2-19     2023-02-01 [2] CRAN (R 4.3.3)\n colorspace     2.1-0      2023-01-23 [1] CRAN (R 4.3.3)\n conflicted     1.2.0      2023-02-01 [1] CRAN (R 4.3.3)\n data.table     1.15.4     2024-03-30 [1] CRAN (R 4.3.3)\n dials        * 1.2.1      2024-02-22 [1] CRAN (R 4.3.3)\n DiceDesign     1.10       2023-12-07 [1] CRAN (R 4.3.3)\n digest         0.6.35     2024-03-11 [1] CRAN (R 4.3.3)\n dplyr        * 1.1.4      2023-11-17 [1] CRAN (R 4.3.2)\n evaluate       0.23       2023-11-01 [1] CRAN (R 4.3.3)\n fansi          1.0.6      2023-12-08 [1] CRAN (R 4.3.3)\n farver         2.1.1      2022-07-06 [1] CRAN (R 4.3.3)\n fastmap        1.1.1      2023-02-24 [1] CRAN (R 4.3.3)\n foreach        1.5.2      2022-02-02 [1] CRAN (R 4.3.3)\n furrr          0.3.1      2022-08-15 [1] CRAN (R 4.3.3)\n future         1.33.2     2024-03-26 [1] CRAN (R 4.3.3)\n future.apply   1.11.2     2024-03-28 [1] CRAN (R 4.3.3)\n generics       0.1.3      2022-07-05 [1] CRAN (R 4.3.3)\n ggplot2      * 3.5.1      2024-04-23 [1] CRAN (R 4.3.3)\n globals        0.16.3     2024-03-08 [1] CRAN (R 4.3.3)\n glue           1.7.0      2024-01-09 [1] CRAN (R 4.3.3)\n gower          1.0.1      2022-12-22 [1] CRAN (R 4.3.1)\n GPfit          1.0-8      2019-02-08 [1] CRAN (R 4.3.3)\n gtable         0.3.5      2024-04-22 [1] CRAN (R 4.3.3)\n hardhat        1.3.1      2024-02-02 [1] CRAN (R 4.3.3)\n hms            1.1.3      2023-03-21 [1] CRAN (R 4.3.3)\n htmltools      0.5.8.1    2024-04-04 [1] CRAN (R 4.3.3)\n htmlwidgets    1.6.4      2023-12-06 [1] CRAN (R 4.3.3)\n infer        * 1.0.7      2024-03-25 [1] CRAN (R 4.3.3)\n ipred          0.9-14     2023-03-09 [1] CRAN (R 4.3.3)\n iterators      1.0.14     2022-02-05 [1] CRAN (R 4.3.3)\n jsonlite       1.8.8      2023-12-04 [1] CRAN (R 4.3.3)\n knitr          1.46       2024-04-06 [1] CRAN (R 4.3.3)\n labeling       0.4.3      2023-08-29 [1] CRAN (R 4.3.1)\n lattice      * 0.22-5     2023-10-24 [2] CRAN (R 4.3.3)\n lava           1.8.0      2024-03-05 [1] CRAN (R 4.3.3)\n lhs            1.1.6      2022-12-17 [1] CRAN (R 4.3.3)\n lifecycle      1.0.4      2023-11-07 [1] CRAN (R 4.3.3)\n listenv        0.9.1      2024-01-29 [1] CRAN (R 4.3.3)\n lubridate      1.9.3      2023-09-27 [1] CRAN (R 4.3.3)\n magrittr       2.0.3      2022-03-30 [1] CRAN (R 4.3.3)\n MASS           7.3-60.0.1 2024-01-13 [2] CRAN (R 4.3.3)\n Matrix         1.6-5      2024-01-11 [2] CRAN (R 4.3.3)\n memoise        2.0.1      2021-11-26 [1] CRAN (R 4.3.3)\n mgcv           1.9-1      2023-12-21 [2] CRAN (R 4.3.3)\n modeldata    * 1.3.0      2024-01-21 [1] CRAN (R 4.3.3)\n munsell        0.5.1      2024-04-01 [1] CRAN (R 4.3.3)\n nlme           3.1-164    2023-11-27 [2] CRAN (R 4.3.3)\n nnet           7.3-19     2023-05-03 [2] CRAN (R 4.3.3)\n parallelly     1.37.1     2024-02-29 [1] CRAN (R 4.3.3)\n parsnip      * 1.2.1      2024-03-22 [1] CRAN (R 4.3.3)\n permute      * 0.9-7      2022-01-27 [1] CRAN (R 4.3.3)\n pillar         1.9.0      2023-03-22 [1] CRAN (R 4.3.3)\n pkgconfig      2.0.3      2019-09-22 [1] CRAN (R 4.3.3)\n prodlim        2023.08.28 2023-08-28 [1] CRAN (R 4.3.3)\n purrr        * 1.0.2      2023-08-10 [1] CRAN (R 4.3.3)\n R6             2.5.1      2021-08-19 [1] CRAN (R 4.3.3)\n Rcpp           1.0.12     2024-01-09 [1] CRAN (R 4.3.3)\n readr          2.1.5      2024-01-10 [1] CRAN (R 4.3.3)\n recipes      * 1.0.10     2024-02-18 [1] CRAN (R 4.3.3)\n rlang          1.1.3      2024-01-10 [1] CRAN (R 4.3.3)\n rmarkdown      2.26       2024-03-05 [1] CRAN (R 4.3.3)\n rpart          4.1.23     2023-12-05 [2] CRAN (R 4.3.3)\n rsample      * 1.2.1      2024-03-25 [1] CRAN (R 4.3.3)\n rstudioapi     0.16.0     2024-03-24 [1] CRAN (R 4.3.3)\n scales       * 1.3.0      2023-11-28 [1] CRAN (R 4.3.3)\n sessioninfo    1.2.2      2021-12-06 [1] CRAN (R 4.3.3)\n survival       3.5-8      2024-02-14 [2] CRAN (R 4.3.3)\n tibble       * 3.2.1      2023-03-20 [1] CRAN (R 4.3.3)\n tidymodels   * 1.2.0      2024-03-25 [1] CRAN (R 4.3.3)\n tidyr        * 1.3.1      2024-01-24 [1] CRAN (R 4.3.3)\n tidyselect     1.2.1      2024-03-11 [1] CRAN (R 4.3.3)\n timechange     0.3.0      2024-01-18 [1] CRAN (R 4.3.3)\n timeDate       4032.109   2023-12-14 [1] CRAN (R 4.3.2)\n tune         * 1.2.1      2024-04-18 [1] CRAN (R 4.3.3)\n tzdb           0.4.0      2023-05-12 [1] CRAN (R 4.3.3)\n utf8           1.2.4      2023-10-22 [1] CRAN (R 4.3.3)\n vctrs          0.6.5      2023-12-01 [1] CRAN (R 4.3.3)\n vegan        * 2.6-4      2022-10-11 [1] CRAN (R 4.3.3)\n withr          3.0.0      2024-01-16 [1] CRAN (R 4.3.3)\n workflows    * 1.1.4      2024-02-19 [1] CRAN (R 4.3.3)\n workflowsets * 1.1.0      2024-03-21 [1] CRAN (R 4.3.3)\n xfun           0.43       2024-03-25 [1] CRAN (R 4.3.3)\n yaml           2.3.8      2023-12-11 [1] CRAN (R 4.3.2)\n yardstick    * 1.3.1      2024-03-21 [1] CRAN (R 4.3.3)\n\n [1] C:/Users/Willi/AppData/Local/R/win-library/4.3\n [2] C:/Program Files/R/R-4.3.3/library\n\n──────────────────────────────────────────────────────────────────────────────",
    "crumbs": [
      "Applied Biostatistical Methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Wilcoxon Test</span>"
    ]
  },
  {
    "objectID": "stat03_mult_testing.html",
    "href": "stat03_mult_testing.html",
    "title": "6  Multiple Testing Correction",
    "section": "",
    "text": "6.1 FWER Correction (Bonferroni)\nThe Bonferroni correction is a conservative method, which means that it is less likely to find significant results. To apply Bonferroni correction, the significance level is divided by the number of tests, which would then also affect the FWER (the probability of finding at least one false positive among the significant results). The Bonferroni adjusted significance level is then:\nalpha_bonferroni &lt;- alpha / nrow(p_values_per_group)\nalpha_bonferroni\n\n[1] 0.003125\nUsing this, we actually find no significant results:\np_values_per_group |&gt; \n  filter(p_value &lt; alpha_bonferroni)\n\n# A tibble: 0 × 3\n# ℹ 3 variables: Group Name &lt;chr&gt;, Group Value &lt;chr&gt;, p_value &lt;dbl&gt;\nThe FWER becomes:\n\\[\nFWER = 1 - (1 - \\frac{\\alpha}{m})^{m} = 1 - (1 - \\frac{0.05}{16})^{16} = 0.049\n\\]\nThe FWER is reduced from 56% to 4.9%, close to the original significance level of 5%. It is then expected to find fewer false positives, also called Type I errors. However, the chance of finding false negatives also increases (referred to as Type II errors). In this case, finding false negatives means accepting the null hypothesis when it was actually false, i.e. not finding a significant result when one exists. In some cases, using a very conservative method is preferred, especially when the cost of a false positive is high.",
    "crumbs": [
      "Applied Biostatistical Methods",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Multiple Testing Correction</span>"
    ]
  },
  {
    "objectID": "stat03_mult_testing.html#false-discovery-rate-correction-benjamini-hochberg",
    "href": "stat03_mult_testing.html#false-discovery-rate-correction-benjamini-hochberg",
    "title": "6  Multiple Testing Correction",
    "section": "6.2 False Discovery Rate Correction (Benjamini-Hochberg)",
    "text": "6.2 False Discovery Rate Correction (Benjamini-Hochberg)\nAnother way to control for multiple testing is through the False Discovery Rate (FDR). FDR is the proportion of false positives among the significant results:\n\\[\nFDR = \\frac{FP}{FP + TP}\n\\]\nWhere \\(FP = False Positive\\) and \\(TP = True Positive\\). FDR is then an expression for the proportion of false positives among the significant results.\nIn some cases, a less conservative method than the Bonferroni correction is preferred allowing for more significant hits. This comes at a cost of an increase in false positives. The Benjamini-Hochberg Procedure is such a method - a type of False Discovery Rate (FDR) correction.\nTwo ways to implement the method exists. Either the significance level is adjusted, or the p-values are adjusted, both giving the same significant hits. The latter is the most common way, but both are introduced.\nIn general, each p-value is compared to a moving significance level instead of compared to a static value of \\(\\alpha\\), e.g. \\(0.05\\). The moving significance level is dependent on the rank of the p-value and the number of tests. The lowest p-value gets rank 1, the second lowest rank 2, and so on. With a higher rank, the significance level increases, allowing for more significant hits.\n\n6.2.1 Adjusting the Significance Level\nAll p-values are sorted in ascending order and ranked as mentioned above. A critical value for each rank is calculated by:\n\\[\n\\text{Critical Value} = \\frac{k}{m} \\cdot \\alpha\n\\]\nWhere \\(k\\) is the rank of the considered p-value, \\(m\\) is the number of tests and \\(\\alpha\\) is the significance level. The largest value of \\(k\\) where the p-value is less than the critical value is found:\n\\[\n\\max_{k} \\left( p_{(k)} \\leq Critical \\; Value \\right) = \\max_{k} \\left( p_{(k)} \\leq \\frac{k}{m} \\cdot \\alpha \\right)\n\\]\nAll p-values with a rank less than or equal to the largest \\(k\\) are then considered significant. Note that is is irrelevant if a p-value of lower rank than the rank found from the above, is larger than the critical value. It is still considered significant.\nAs for the Bonferroni correction, no significant results are found:\n\np_values_per_group |&gt; \n  arrange(p_value) |&gt; \n  mutate(rank = row_number(),\n         critical_value = (rank / nrow(p_values_per_group)) * alpha,\n         significant = p_value &lt;= critical_value)\n\n# A tibble: 16 × 6\n   `Group Name` `Group Value` p_value  rank critical_value significant\n   &lt;chr&gt;        &lt;chr&gt;           &lt;dbl&gt; &lt;int&gt;          &lt;dbl&gt; &lt;lgl&gt;      \n 1 Age_group    30-36.6        0.0192     1        0.00312 FALSE      \n 2 BMI_group    25.77-28.51    0.0249     2        0.00625 FALSE      \n 3 Age_group    63.2+          0.0444     3        0.00938 FALSE      \n 4 BMI_group    23.77-25.77    0.0449     4        0.0125  FALSE      \n 5 Age_group    51.2-53        0.128      5        0.0156  FALSE      \n 6 Age_group    55.7-59.3      0.133      6        0.0188  FALSE      \n 7 BMI_group    33.31+         0.239      7        0.0219  FALSE      \n 8 Age_group    48.6-51.2      0.246      8        0.025   FALSE      \n 9 Age_group    36.6-41        0.277      9        0.0281  FALSE      \n10 BMI_group    17-21.7        0.317     10        0.0312  FALSE      \n11 Age_group    53-55.7        0.562     11        0.0344  FALSE      \n12 Age_group    41-45          0.681     12        0.0375  FALSE      \n13 BMI_group    21.7-23.77     0.843     13        0.0406  FALSE      \n14 Age_group    45-48.6        0.901     14        0.0438  FALSE      \n15 BMI_group    28.51-33.31    0.955     15        0.0469  FALSE      \n16 Age_group    59.3-63.2      0.978     16        0.05    FALSE      \n\n\n\n\n6.2.2 Adjusting the p-values\nThe p-values are again sorted and ranked exactly as before. Apply the following formula to each p-value as an intermediary step:\n\\[\np_{k,adj} = p_{k} \\cdot \\frac{m}{k}\n\\]\nWhere \\(m\\) is the number of tests, \\(k\\) is the rank of the considered p-value and \\(p_{k}\\) is the p-value of said rank. The adjusted p-values can then be found.\nStarting with the highest numbered rank, i.e. largest p-value, the adjusted p-value is simply the intermediary value found from the above. For the second highest rank, the adjusted p-value is the minimum of the intermediary value and the adjusted p-value of previously calculated rank. This is repeated for all ranks. The adjusted p-values are then compared to the significance level \\(\\alpha\\), and all p-values less than the significance level are considered significant. Once again, no significant results are found:\n\np_values_per_group |&gt; \n  mutate(p_value_adjusted = p.adjust(p = p_value,\n                                     method = \"BH\"),\n         significant = p_value_adjusted &lt; 0.05)\n\n# A tibble: 16 × 5\n   `Group Name` `Group Value` p_value p_value_adjusted significant\n   &lt;chr&gt;        &lt;chr&gt;           &lt;dbl&gt;            &lt;dbl&gt; &lt;lgl&gt;      \n 1 Age_group    30-36.6        0.0192            0.180 FALSE      \n 2 Age_group    36.6-41        0.277             0.492 FALSE      \n 3 Age_group    41-45          0.681             0.908 FALSE      \n 4 Age_group    45-48.6        0.901             0.978 FALSE      \n 5 Age_group    48.6-51.2      0.246             0.492 FALSE      \n 6 Age_group    51.2-53        0.128             0.356 FALSE      \n 7 Age_group    53-55.7        0.562             0.818 FALSE      \n 8 Age_group    55.7-59.3      0.133             0.356 FALSE      \n 9 Age_group    59.3-63.2      0.978             0.978 FALSE      \n10 Age_group    63.2+          0.0444            0.180 FALSE      \n11 BMI_group    17-21.7        0.317             0.507 FALSE      \n12 BMI_group    21.7-23.77     0.843             0.978 FALSE      \n13 BMI_group    23.77-25.77    0.0449            0.180 FALSE      \n14 BMI_group    25.77-28.51    0.0249            0.180 FALSE      \n15 BMI_group    28.51-33.31    0.955             0.978 FALSE      \n16 BMI_group    33.31+         0.239             0.492 FALSE",
    "crumbs": [
      "Applied Biostatistical Methods",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Multiple Testing Correction</span>"
    ]
  },
  {
    "objectID": "stat04_OLS.html",
    "href": "stat04_OLS.html",
    "title": "7  Ordinary Least Squares Regression",
    "section": "",
    "text": "Set seed and load packages.\n\n\nShow the code\nset.seed(1337)\n\nlibrary(\"tidymodels\")\ntidymodels::tidymodels_prefer()\n\n\nLoad data.\n\n\nShow the code\nmtcars &lt;- mtcars |&gt; \n  as_tibble()\n\n\nA general use case in statistics is to predict new data given some input data. Endless examples exist, such as predicting the price of a house given the size of the house or predicting the miles pr gallon of a car given the weight of the car. To do so, some model is needed. A wide range of models for predicting data exists. One of the most common models is a linear regression model. Introduced in this chapter is the model called Ordinary Least Squares (OLS).\nThe general idea is to fit a line to the data, such that the sum of the squared residuals is minimized, hence the name least squares. Residuals are the difference between the actual value and the predicted value of the model.\nAnother famously used data set is the mtcars. Below is an example of a scatter plot of the miles pr gallon (mpg) vs the weight of the car (wt). By fitting a linear regression model, the goal is to predict the miles pr gallon based on the weight of the car. For an initial guess, the mean of the miles pr gallon is used, shown as the red dashed line. The residuals are the distance between the actual value (dots) and the predicted value (red dashed line). The sum of the squares of the residuals is then used as a measure of how well the model fits the data. As some predicted values are below the actual value, and some values are above, these can counter balance each other, such that the sum of the residuals is 0. To avoid this, the residuals are squared, such that all residuals are positive. Further, by squaring the residuals the bigger residuals have a larger effect on the sum of the residuals squared. Since the goal is to minimize the sum of the residuals squared, the bigger residuals are more important to reduce.\n\nmpg_mean &lt;- mtcars |&gt; \n  pull(mpg) |&gt; \n  mean()\n\nmtcars |&gt; \n  ggplot(aes(x = wt,\n             y = mpg)) +\n  geom_point(size = 3,\n             colour = \"steelblue\") +\n  geom_hline(yintercept = mpg_mean,\n             colour = \"firebrick\",\n             linetype = \"dashed\") +\n  labs(title = \"Miles pr Gallon vs Weight\",\n       x = \"Weight (1000 lbs)\",\n       y = \"Miles pr Gallon\",\n       colour = \"Miles per gallon\") +\n  theme(text=element_text(size=13))\n\n\n\n\n\n\n\n\nIn this simple case, the model is fitting the line to the data by changing the slope and intercept of the line. The combination which minimizes the sum of the squared residuals (SSR) is chosen. This results in a three dimensional space, where the slope and intercept are plotted along the x and y axis, and the sum of the squared residuals are plotted along the z-axis. Some combination of slope and intercept results in a minimum in the 3D plane. This combination can be found by taking the derivative of the sum of the squared residuals with respect to the slope and intercept, and set it equal to zero.\nThe model can be generalized to more than one feature, such that the model can predict the response based on multiple features. Usually, the simple linear case is written as \\(y = ax + b\\) where \\(a\\) is the slope and \\(b\\) is the intercept. When using multiple features, it is generalized to \\(y = \\beta_0 + \\beta_{1}x_1 + \\beta_{2}x_2 + ... + \\beta_{n}x_n\\). In matrix form it can be written as \\(y = X\\beta\\) where \\(X\\) is a matrix with \\(n\\) rows (number of observations) and \\(p\\) columns (number of features). \\(\\beta\\) is a vector of length \\(p\\) which includes a parameter for each feature.\nAs mentioned earlier, the goal is to minimize the sum of the squared residuals. On matrix form, SSR is written as:\n\\[\nSSR = ||y-X\\beta||_{2}^{2}\n\\]\nWhere the subscript 2 denotes the L2 norm, i.e. Euclidean distance. The term \\(X\\beta\\) is the predicted value of the model as we saw from \\(y = X\\beta\\) in the above. SSR is also written as:\n\\[\nSSR = \\sum_{i=1}^{n} (y_i - X_{i}\\beta)^2 =  \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n\\]\nWhere \\(\\hat{y}_i\\) is the predicted value of the model for the \\(i\\)’th sample. Since the goal is to minimize the SSR, the \\(\\beta\\)s should be chosen such that the SSR is minimized:\n\\[\n\\beta_{OLS} = \\min_{\\beta} SSR = \\min_{\\beta} ||y-X\\beta||_{2}^{2}\n\\]\nAs mentioned in the above, the \\(\\beta\\)s can be found by taking the derivative of the SSR with respect to \\(\\beta\\) and set it equal to zero. Since this is generalized to multiple features, the gradient is taken with respect to the vector \\(\\beta\\) instead. The equation is then:\n\\[\n\\nabla_\\beta||y-X\\beta||_{2}^{2} = 0\n\\]\n\n8 Session Info\n\nsessioninfo::session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.3 (2024-02-29 ucrt)\n os       Windows 11 x64 (build 22631)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_United Kingdom.utf8\n ctype    English_United Kingdom.utf8\n tz       Europe/Copenhagen\n date     2024-05-30\n pandoc   3.1.11 @ C:/Program Files/RStudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package      * version    date (UTC) lib source\n backports      1.4.1      2021-12-13 [1] CRAN (R 4.3.1)\n broom        * 1.0.5      2023-06-09 [1] CRAN (R 4.3.3)\n cachem         1.0.8      2023-05-01 [1] CRAN (R 4.3.3)\n class          7.3-22     2023-05-03 [2] CRAN (R 4.3.3)\n cli            3.6.2      2023-12-11 [1] CRAN (R 4.3.3)\n codetools      0.2-19     2023-02-01 [2] CRAN (R 4.3.3)\n colorspace     2.1-0      2023-01-23 [1] CRAN (R 4.3.3)\n conflicted     1.2.0      2023-02-01 [1] CRAN (R 4.3.3)\n data.table     1.15.4     2024-03-30 [1] CRAN (R 4.3.3)\n dials        * 1.2.1      2024-02-22 [1] CRAN (R 4.3.3)\n DiceDesign     1.10       2023-12-07 [1] CRAN (R 4.3.3)\n digest         0.6.35     2024-03-11 [1] CRAN (R 4.3.3)\n dplyr        * 1.1.4      2023-11-17 [1] CRAN (R 4.3.2)\n evaluate       0.23       2023-11-01 [1] CRAN (R 4.3.3)\n fansi          1.0.6      2023-12-08 [1] CRAN (R 4.3.3)\n farver         2.1.1      2022-07-06 [1] CRAN (R 4.3.3)\n fastmap        1.1.1      2023-02-24 [1] CRAN (R 4.3.3)\n foreach        1.5.2      2022-02-02 [1] CRAN (R 4.3.3)\n furrr          0.3.1      2022-08-15 [1] CRAN (R 4.3.3)\n future         1.33.2     2024-03-26 [1] CRAN (R 4.3.3)\n future.apply   1.11.2     2024-03-28 [1] CRAN (R 4.3.3)\n generics       0.1.3      2022-07-05 [1] CRAN (R 4.3.3)\n ggplot2      * 3.5.1      2024-04-23 [1] CRAN (R 4.3.3)\n globals        0.16.3     2024-03-08 [1] CRAN (R 4.3.3)\n glue           1.7.0      2024-01-09 [1] CRAN (R 4.3.3)\n gower          1.0.1      2022-12-22 [1] CRAN (R 4.3.1)\n GPfit          1.0-8      2019-02-08 [1] CRAN (R 4.3.3)\n gtable         0.3.5      2024-04-22 [1] CRAN (R 4.3.3)\n hardhat        1.3.1      2024-02-02 [1] CRAN (R 4.3.3)\n htmltools      0.5.8.1    2024-04-04 [1] CRAN (R 4.3.3)\n htmlwidgets    1.6.4      2023-12-06 [1] CRAN (R 4.3.3)\n infer        * 1.0.7      2024-03-25 [1] CRAN (R 4.3.3)\n ipred          0.9-14     2023-03-09 [1] CRAN (R 4.3.3)\n iterators      1.0.14     2022-02-05 [1] CRAN (R 4.3.3)\n jsonlite       1.8.8      2023-12-04 [1] CRAN (R 4.3.3)\n knitr          1.46       2024-04-06 [1] CRAN (R 4.3.3)\n labeling       0.4.3      2023-08-29 [1] CRAN (R 4.3.1)\n lattice        0.22-5     2023-10-24 [2] CRAN (R 4.3.3)\n lava           1.8.0      2024-03-05 [1] CRAN (R 4.3.3)\n lhs            1.1.6      2022-12-17 [1] CRAN (R 4.3.3)\n lifecycle      1.0.4      2023-11-07 [1] CRAN (R 4.3.3)\n listenv        0.9.1      2024-01-29 [1] CRAN (R 4.3.3)\n lubridate      1.9.3      2023-09-27 [1] CRAN (R 4.3.3)\n magrittr       2.0.3      2022-03-30 [1] CRAN (R 4.3.3)\n MASS           7.3-60.0.1 2024-01-13 [2] CRAN (R 4.3.3)\n Matrix         1.6-5      2024-01-11 [2] CRAN (R 4.3.3)\n memoise        2.0.1      2021-11-26 [1] CRAN (R 4.3.3)\n modeldata    * 1.3.0      2024-01-21 [1] CRAN (R 4.3.3)\n munsell        0.5.1      2024-04-01 [1] CRAN (R 4.3.3)\n nnet           7.3-19     2023-05-03 [2] CRAN (R 4.3.3)\n parallelly     1.37.1     2024-02-29 [1] CRAN (R 4.3.3)\n parsnip      * 1.2.1      2024-03-22 [1] CRAN (R 4.3.3)\n pillar         1.9.0      2023-03-22 [1] CRAN (R 4.3.3)\n pkgconfig      2.0.3      2019-09-22 [1] CRAN (R 4.3.3)\n prodlim        2023.08.28 2023-08-28 [1] CRAN (R 4.3.3)\n purrr        * 1.0.2      2023-08-10 [1] CRAN (R 4.3.3)\n R6             2.5.1      2021-08-19 [1] CRAN (R 4.3.3)\n Rcpp           1.0.12     2024-01-09 [1] CRAN (R 4.3.3)\n recipes      * 1.0.10     2024-02-18 [1] CRAN (R 4.3.3)\n rlang          1.1.3      2024-01-10 [1] CRAN (R 4.3.3)\n rmarkdown      2.26       2024-03-05 [1] CRAN (R 4.3.3)\n rpart          4.1.23     2023-12-05 [2] CRAN (R 4.3.3)\n rsample      * 1.2.1      2024-03-25 [1] CRAN (R 4.3.3)\n rstudioapi     0.16.0     2024-03-24 [1] CRAN (R 4.3.3)\n scales       * 1.3.0      2023-11-28 [1] CRAN (R 4.3.3)\n sessioninfo    1.2.2      2021-12-06 [1] CRAN (R 4.3.3)\n survival       3.5-8      2024-02-14 [2] CRAN (R 4.3.3)\n tibble       * 3.2.1      2023-03-20 [1] CRAN (R 4.3.3)\n tidymodels   * 1.2.0      2024-03-25 [1] CRAN (R 4.3.3)\n tidyr        * 1.3.1      2024-01-24 [1] CRAN (R 4.3.3)\n tidyselect     1.2.1      2024-03-11 [1] CRAN (R 4.3.3)\n timechange     0.3.0      2024-01-18 [1] CRAN (R 4.3.3)\n timeDate       4032.109   2023-12-14 [1] CRAN (R 4.3.2)\n tune         * 1.2.1      2024-04-18 [1] CRAN (R 4.3.3)\n utf8           1.2.4      2023-10-22 [1] CRAN (R 4.3.3)\n vctrs          0.6.5      2023-12-01 [1] CRAN (R 4.3.3)\n withr          3.0.0      2024-01-16 [1] CRAN (R 4.3.3)\n workflows    * 1.1.4      2024-02-19 [1] CRAN (R 4.3.3)\n workflowsets * 1.1.0      2024-03-21 [1] CRAN (R 4.3.3)\n xfun           0.43       2024-03-25 [1] CRAN (R 4.3.3)\n yaml           2.3.8      2023-12-11 [1] CRAN (R 4.3.2)\n yardstick    * 1.3.1      2024-03-21 [1] CRAN (R 4.3.3)\n\n [1] C:/Users/Willi/AppData/Local/R/win-library/4.3\n [2] C:/Program Files/R/R-4.3.3/library\n\n──────────────────────────────────────────────────────────────────────────────",
    "crumbs": [
      "Applied Biostatistical Methods",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Ordinary Least Squares Regression</span>"
    ]
  },
  {
    "objectID": "stat05_log_reg.html",
    "href": "stat05_log_reg.html",
    "title": "8  Logistic Regression",
    "section": "",
    "text": "Set seed and load packages.\n\n\nShow the code\nset.seed(1337)\n\nlibrary(\"tidymodels\")\ntidymodels::tidymodels_prefer()\nlibrary(\"MASS\")\n\n\nLoad data.\n\n\nShow the code\ndata(\"iris\")\niris &lt;- iris |&gt;\n  tibble::as_tibble() |&gt; \n  filter(Species != \"setosa\") |&gt; \n  droplevels()\n\n\nWhereas the problem introduces in the beginning of Chapter 7 was to predict a continuous value, the problem in this chapter is to predict a binary value, otherwise the problem is the same. Examples include predicting if a patient has a disease or not, if a customer will buy a product or not or if a flower is a certain species or not.\nOpposite Ordinary Least Squares (OLS), which predicts a continuous value, logistic regression predicts a probability between two classes (usually referred to as a TRUE/FALSE). This is done by fitting a logistic function, also called a sigmoid function, which has the form \\(p = \\frac{1}{1 + e^{-y}}\\) and have a characteristic S-shape. When plotting these probabilites, the y-axis ranges from 0 to 1 reflecting the probability of the class measured along the y-axis.\nWhen applying logistic regression, the probabilites are transformed into a continuous scale with the logit function which has the form \\(log(odds) = log(\\frac{p}{1-p})\\). The logit function converts the probabilities into log-odds. For example, a probability of 0.5 is 0 log-odds, the center on both y-axes. A probability of 0.75 is 1.099 log-odds and so forth. The points that were classified as TRUE, i.e. 1 on the y-axis, is transformed to positive infinity on the new y-axis as:\n\\[\nlog(odds) = log(\\frac{p}{1-p}) = log(\\frac{1}{1-1}) = log(\\frac{1}{0}) = log(1) - log(0) = \\infty\n\\]\nThe points that were classified as FALSE, i.e. 0 on the previous y-axis, is transformed to negative infinity on the new y-axis as:\n\\[\nlog(odds) = log(\\frac{p}{1-p}) = log(\\frac{0}{1-0}) = log(\\frac{0}{1}) = log(0) - log(1) = -\\infty\n\\]\nSo, the interval of 0.5 to 1 on the old axis as been transformed into an interval from 0 to positive infinity and vice versa. The data now exists on a continuous scale from negative to positive infinity, and a straight line can be fitted to the data.\nFor finding the coefficients for the fitted line, logistic regression cannot apply sum of squared residuals as OLS did, as the distance between a point and a candidate line tends to be infinity given the transformation mentioned above. Instead, maximum likelihood is used to find the best fit line. The term refers to maximizing the likelihood of observing the data given the model. The likelihood is the product of the probabilities of the data points, so a good fit maximizes the product of the probabilities.\nAfter finding a candidate line on the log-odds scale (the continuous scale), the data points are projected onto the line. The log-odds values are read off the y-axis and converted back into probabilities with the logistic function. The product of probabilities, the likelihood, is calculated where the product is found differently for the TRUE and FALSE classes:\n\\[\nL(\\beta,x_i,y_i) = \\prod_{i \\; in \\; y = 1}^{n} x_i \\cdot \\prod_{i \\; in \\; y = 0}^{n} (1-x_i)\n\\]\nWhere n is the number of observations, y is the class, and x is individual probabilities. It is the above product that is maximized by trying different lines on the log-odds scale. The line that maximizes the product is the best fit line which can be written as:\n\\[\n\\beta_{LR} = \\max_{\\beta} \\; L(\\beta,x_i,y_i)\n\\]\nThe process results in the best set of coefficients for the linear line that maximizes the product of probabilities, i.e. maximum likelihood. It is possible to convert the linearly fit line back to the original scale with the logistic function by replacing y with the linear equation:\n\\[\np = \\frac{1}{1 + e^{-y}} = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 \\cdot x_1+ \\beta_2 \\cdot x_2 + ... + \\beta_n \\cdot x_n)}}\n\\]\nFor illustrating logistic regression, the iris data set is used to predict the probability of a flower being a virginica based on the petal length. As previously, the setosa class have been removed from the data set to only contain two classes. The species is transformed into a binary response, where versicolor is 0 and virginica is 1. Below the data is visualized where the petal length is seen on the x-axis and the probability of being a virginica is seen on the y-axis.\n\niris_prob &lt;- iris |&gt;\n  mutate(Species_prob = case_when(Species == \"versicolor\" ~ 0,\n                                  Species == \"virginica\" ~ 1))\n\niris_prob_plot &lt;- iris_prob |&gt;\n  ggplot(aes(x = Petal.Length,\n             y = Species_prob)) +\n  geom_point(aes(col = Species),\n             size = 3) +\n  labs(x = \"Petal Length\",\n       y = \"Probability of virginica\") +\n  theme(text=element_text(size=13))\niris_prob_plot\n\n\n\n\n\n\n\n\nThe probabilities are then transformed to log(odds) with the logit function. In this data set, all values were either 1 or 0, which is why the log(odds) values are either positive or negative infinity as seen below:\n\niris_logit_plot &lt;- iris_prob |&gt; \n  mutate(Species_logit = log(Species_prob / (1 - Species_prob))) |&gt; \n  mutate(Species_logit = case_when(Species_logit == Inf ~ \"Inf\",\n                                   Species_logit == -Inf ~ \"-Inf\",\n                                   TRUE ~ as.character(Species_logit)\n                                   )) |&gt;\n  ggplot(aes(x = Petal.Length,\n             y = Species_logit)) +\n  geom_point(aes(col = Species),\n             size = 3) +\n  scale_y_discrete(limits = c(\"-Inf\", \"-2\", \"-1\", \"0\", \"1\", \"2\", \"Inf\")) +\n  labs(x = \"Petal Length\",\n       y = \"log(odds) of virginica\") +\n  theme(text=element_text(size=13))\niris_logit_plot\n\n\n\n\n\n\n\n\nAs a candidate fit, see the red dashed lined below. The data points are projected onto the line and the log(odds) values are read off the y-axis. The log(odds) values are then converted back into probabilities with the logistic function and the product for the TRUE and FALSE classes are found. The product of the two classes is then calculated. The candidate line that maximizes the product is the best fit line.\n\niris_logit_plot +\n  geom_abline(intercept = 0,\n              slope = 1,\n              linetype = \"dashed\", \n              color = \"red\")\n\n\n\n\n\n\n\n\nUltimately, the optimal sigmoid shaped line is found:\n\niris_prob_plot +\n  geom_smooth(method = \"glm\",\n              formula = y ~ x,\n              method.args = list(family = \"binomial\"),\n              se = FALSE)\n\n\n\n\n\n\n\n\n\n9 Session Info\n\nsessioninfo::session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.3 (2024-02-29 ucrt)\n os       Windows 11 x64 (build 22631)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_United Kingdom.utf8\n ctype    English_United Kingdom.utf8\n tz       Europe/Copenhagen\n date     2024-05-30\n pandoc   3.1.11 @ C:/Program Files/RStudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package      * version    date (UTC) lib source\n backports      1.4.1      2021-12-13 [1] CRAN (R 4.3.1)\n broom        * 1.0.5      2023-06-09 [1] CRAN (R 4.3.3)\n cachem         1.0.8      2023-05-01 [1] CRAN (R 4.3.3)\n class          7.3-22     2023-05-03 [2] CRAN (R 4.3.3)\n cli            3.6.2      2023-12-11 [1] CRAN (R 4.3.3)\n codetools      0.2-19     2023-02-01 [2] CRAN (R 4.3.3)\n colorspace     2.1-0      2023-01-23 [1] CRAN (R 4.3.3)\n conflicted     1.2.0      2023-02-01 [1] CRAN (R 4.3.3)\n data.table     1.15.4     2024-03-30 [1] CRAN (R 4.3.3)\n dials        * 1.2.1      2024-02-22 [1] CRAN (R 4.3.3)\n DiceDesign     1.10       2023-12-07 [1] CRAN (R 4.3.3)\n digest         0.6.35     2024-03-11 [1] CRAN (R 4.3.3)\n dplyr        * 1.1.4      2023-11-17 [1] CRAN (R 4.3.2)\n evaluate       0.23       2023-11-01 [1] CRAN (R 4.3.3)\n fansi          1.0.6      2023-12-08 [1] CRAN (R 4.3.3)\n farver         2.1.1      2022-07-06 [1] CRAN (R 4.3.3)\n fastmap        1.1.1      2023-02-24 [1] CRAN (R 4.3.3)\n foreach        1.5.2      2022-02-02 [1] CRAN (R 4.3.3)\n furrr          0.3.1      2022-08-15 [1] CRAN (R 4.3.3)\n future         1.33.2     2024-03-26 [1] CRAN (R 4.3.3)\n future.apply   1.11.2     2024-03-28 [1] CRAN (R 4.3.3)\n generics       0.1.3      2022-07-05 [1] CRAN (R 4.3.3)\n ggplot2      * 3.5.1      2024-04-23 [1] CRAN (R 4.3.3)\n globals        0.16.3     2024-03-08 [1] CRAN (R 4.3.3)\n glue           1.7.0      2024-01-09 [1] CRAN (R 4.3.3)\n gower          1.0.1      2022-12-22 [1] CRAN (R 4.3.1)\n GPfit          1.0-8      2019-02-08 [1] CRAN (R 4.3.3)\n gtable         0.3.5      2024-04-22 [1] CRAN (R 4.3.3)\n hardhat        1.3.1      2024-02-02 [1] CRAN (R 4.3.3)\n htmltools      0.5.8.1    2024-04-04 [1] CRAN (R 4.3.3)\n htmlwidgets    1.6.4      2023-12-06 [1] CRAN (R 4.3.3)\n infer        * 1.0.7      2024-03-25 [1] CRAN (R 4.3.3)\n ipred          0.9-14     2023-03-09 [1] CRAN (R 4.3.3)\n iterators      1.0.14     2022-02-05 [1] CRAN (R 4.3.3)\n jsonlite       1.8.8      2023-12-04 [1] CRAN (R 4.3.3)\n knitr          1.46       2024-04-06 [1] CRAN (R 4.3.3)\n labeling       0.4.3      2023-08-29 [1] CRAN (R 4.3.1)\n lattice        0.22-5     2023-10-24 [2] CRAN (R 4.3.3)\n lava           1.8.0      2024-03-05 [1] CRAN (R 4.3.3)\n lhs            1.1.6      2022-12-17 [1] CRAN (R 4.3.3)\n lifecycle      1.0.4      2023-11-07 [1] CRAN (R 4.3.3)\n listenv        0.9.1      2024-01-29 [1] CRAN (R 4.3.3)\n lubridate      1.9.3      2023-09-27 [1] CRAN (R 4.3.3)\n magrittr       2.0.3      2022-03-30 [1] CRAN (R 4.3.3)\n MASS         * 7.3-60.0.1 2024-01-13 [2] CRAN (R 4.3.3)\n Matrix         1.6-5      2024-01-11 [2] CRAN (R 4.3.3)\n memoise        2.0.1      2021-11-26 [1] CRAN (R 4.3.3)\n mgcv           1.9-1      2023-12-21 [2] CRAN (R 4.3.3)\n modeldata    * 1.3.0      2024-01-21 [1] CRAN (R 4.3.3)\n munsell        0.5.1      2024-04-01 [1] CRAN (R 4.3.3)\n nlme           3.1-164    2023-11-27 [2] CRAN (R 4.3.3)\n nnet           7.3-19     2023-05-03 [2] CRAN (R 4.3.3)\n parallelly     1.37.1     2024-02-29 [1] CRAN (R 4.3.3)\n parsnip      * 1.2.1      2024-03-22 [1] CRAN (R 4.3.3)\n pillar         1.9.0      2023-03-22 [1] CRAN (R 4.3.3)\n pkgconfig      2.0.3      2019-09-22 [1] CRAN (R 4.3.3)\n prodlim        2023.08.28 2023-08-28 [1] CRAN (R 4.3.3)\n purrr        * 1.0.2      2023-08-10 [1] CRAN (R 4.3.3)\n R6             2.5.1      2021-08-19 [1] CRAN (R 4.3.3)\n Rcpp           1.0.12     2024-01-09 [1] CRAN (R 4.3.3)\n recipes      * 1.0.10     2024-02-18 [1] CRAN (R 4.3.3)\n rlang          1.1.3      2024-01-10 [1] CRAN (R 4.3.3)\n rmarkdown      2.26       2024-03-05 [1] CRAN (R 4.3.3)\n rpart          4.1.23     2023-12-05 [2] CRAN (R 4.3.3)\n rsample      * 1.2.1      2024-03-25 [1] CRAN (R 4.3.3)\n rstudioapi     0.16.0     2024-03-24 [1] CRAN (R 4.3.3)\n scales       * 1.3.0      2023-11-28 [1] CRAN (R 4.3.3)\n sessioninfo    1.2.2      2021-12-06 [1] CRAN (R 4.3.3)\n survival       3.5-8      2024-02-14 [2] CRAN (R 4.3.3)\n tibble       * 3.2.1      2023-03-20 [1] CRAN (R 4.3.3)\n tidymodels   * 1.2.0      2024-03-25 [1] CRAN (R 4.3.3)\n tidyr        * 1.3.1      2024-01-24 [1] CRAN (R 4.3.3)\n tidyselect     1.2.1      2024-03-11 [1] CRAN (R 4.3.3)\n timechange     0.3.0      2024-01-18 [1] CRAN (R 4.3.3)\n timeDate       4032.109   2023-12-14 [1] CRAN (R 4.3.2)\n tune         * 1.2.1      2024-04-18 [1] CRAN (R 4.3.3)\n utf8           1.2.4      2023-10-22 [1] CRAN (R 4.3.3)\n vctrs          0.6.5      2023-12-01 [1] CRAN (R 4.3.3)\n withr          3.0.0      2024-01-16 [1] CRAN (R 4.3.3)\n workflows    * 1.1.4      2024-02-19 [1] CRAN (R 4.3.3)\n workflowsets * 1.1.0      2024-03-21 [1] CRAN (R 4.3.3)\n xfun           0.43       2024-03-25 [1] CRAN (R 4.3.3)\n yaml           2.3.8      2023-12-11 [1] CRAN (R 4.3.2)\n yardstick    * 1.3.1      2024-03-21 [1] CRAN (R 4.3.3)\n\n [1] C:/Users/Willi/AppData/Local/R/win-library/4.3\n [2] C:/Program Files/R/R-4.3.3/library\n\n──────────────────────────────────────────────────────────────────────────────",
    "crumbs": [
      "Applied Biostatistical Methods",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "stat06_enet.html",
    "href": "stat06_enet.html",
    "title": "9  Elastic Net",
    "section": "",
    "text": "9.1 Ridge Regression\nRecall from the OLS section (Chapter 7) that the goal is to minimize the sum of squared residuals (SSR).\n\\[\n\\beta_{OLS} = \\min_{\\beta} ||y-X\\beta||_{2}^{2}\n\\]\nRidge is essentially OLS, except for an added penalty term to the minimization of the SSR:\n\\[\n\\beta_{Ridge} = \\min_{\\beta} ||y-X\\beta||_{2}^{2} + \\lambda||\\beta||_{2}^{2}\n\\]\nFinding the coefficients for the linear model now becomes a compromise between minimizing the SSR and minimizing the value of the coefficients. \\(\\lambda\\) is a hyperparameter that controls the trade-off between the two. When \\(\\lambda = 0\\), Ridge regression is the same as OLS. When \\(\\lambda\\) is very large, the coefficients are driven towards zero as the penalty term \\(\\lambda||\\beta||_{2}^{2}\\) becomes large compared to the residuals. The coefficients can never be set to zero, but they can be driven asymptotically close to zero. The reason is, when the coefficients are close to zero, the model prefers fitting to the data instead of further reducing the size of the coefficients. See a more detailed explanation in Chapter 7.\nIntroducing the penalty term makes the fit to the data worse, but it can improve the generalization to new data. The coefficients obtained through Ridge tends to be small compared to OLS, so the model tends to be less sensitive to changes in the data. In machine learning terms, this is called introducing bias (inability to fit data perfectly) to reduce variance (the model fits better to new data). The bias-variance trade-off is a fundamental concept in machine learning.\nFor visualization, remember the mtcars data set used previously. The following code chunk shows a 3D scatter plot of the weight of the car (wt) vs the horse power (hp) vs the miles per gallon (mpg). The color of the dots represent the horse power. When orienting the plot correctly, it is possible to see a linear correlation between the features.\nmtcars |&gt; \n  plot_ly(x = ~wt,\n          y = ~hp,\n          z = ~mpg,\n          color = ~hp,\n          type = \"scatter3d\",\n          mode = \"markers\") |&gt; \n  plotly::layout(\n    scene = list(\n      xaxis = list(title = \"Weight\"),\n      yaxis = list(title = \"Horse Power\"),\n      zaxis = list(title = \"Miles per Gallon\"),\n      camera = list(\n        eye = list(x = 1.5, y = -2, z = 0.5)\n      )),\n    showlegend = FALSE)\nTwo coefficients are used to try and model the data. The coefficients are the slope of the line for the weight and horse power. Large absolute values for the coefficients resukts in steep slopes, which in turn suggests that the response (mpg) is sensitive to the features (wt and hp). When presented to new data which is different from the training data, the model predicts wildly different responses due to the sensitivity to changes. The model then generalizes poorly to new data and is said to have high variance. By increasing lambda, the slope is reduced, and the model becomes less sensitive to the features. This is the bias-variance trade-off in action as seen in the following code chunks.\n# CV folds object\nmtcars_split &lt;- initial_split(mtcars)\nmtcars_train &lt;- training(mtcars_split)\nmtcars_test &lt;- testing(mtcars_split)\nmtcars_folds &lt;- vfold_cv(mtcars_train,\n                         v = 5)\n\n# Setup model specs\nridge_spec &lt;- linear_reg(penalty = tune(),\n                         mixture = 0) |&gt; \n  set_engine(engine = \"glmnet\") |&gt; \n  set_mode(\"regression\")\n\nOLS_spec &lt;- linear_reg() |&gt;  # added OLS to have lambda = 0\n  set_engine(engine = \"lm\") |&gt; \n  set_mode(\"regression\")\n\n# Model recipe, no preprocessing\nridge_recipe &lt;- recipe(mpg ~ wt + hp,\n                       data = mtcars_train)\n\n\n# Combine to worflow\nridge_wflow &lt;- workflow_set(\n  preproc = list(recipe = ridge_recipe),\n  models = list(OLS = OLS_spec,\n                Ridge = ridge_spec)\n  )\n\n# Change values of hyperparameters to search across\nridge_params &lt;- ridge_spec |&gt;\n  extract_parameter_set_dials() |&gt;\n  update(penalty = penalty(c(-1, 1)))\n\n# Add params to workflow\nridge_wflow &lt;-\n  ridge_wflow |&gt;\n  option_add(id = \"recipe_Ridge\",\n             param_info = ridge_params) |&gt;\n  option_add(id = \"recipe_Ridge\",\n             control = control_grid(extract = function(x) x))\n\n# Run through grid\nridge_grid_results &lt;- ridge_wflow |&gt; \n  workflow_map(\n    seed = 1337,\n    resamples = mtcars_folds,\n    grid = 5\n  )\nThe following code chunk shows the results of the grid search. The plot shows the root mean square error (RMSE) for the Ridge regression model against the different lambdas. The RMSE increases as lambda increases suggesting a low lambda is preferred.\nautoplot(ridge_grid_results,\n         id = \"recipe_Ridge\",\n         metric = \"rmse\")\nIt is also possible to visualize the actual coefficients for the Ridge regression model. In the above, the lambda values were treated as hyperparameters and was optimized. If the goal is to visualize the coefficients for a specific lambda value, each value of lambda needs to be treated as its own model. The following code chunk shows how to do this.\n# Define a grid of lambda values\nlambda_grid &lt;- 10^seq(-2, 2, length.out = 10)\n\n# Create each function\nridge_models &lt;- lambda_grid |&gt;\n  map(function(lambda) {\n    \n    linear_reg(penalty = lambda,\n               mixture = 0) |&gt; \n      set_engine(\"glmnet\") |&gt; \n      fit(mpg ~ wt + hp,\n          data = mtcars)\n})\nThe coefficients are then plotted against the lambda values. The coefficients are reduced as lambda increases. The coefficients are not set to zero, but they are driven towards zero (and might visually appear to be zero).\n# Unnest coefficients\nridge_coefficients &lt;- bind_rows(map_dfr(ridge_models, ~tidy(.x))) %&gt;%\n  mutate(lambda = rep(lambda_grid, each = 3))\n\n# Visualize coefficients\nridge_coefficients |&gt;\n  ggplot(aes(x = lambda,\n             y = estimate,\n             color = term)) +\n  geom_line(linewidth = 1) +\n  scale_x_log10() +\n  labs(title = \"Ridge Regression Coefficients vs. Lambda\",\n       x = \"Lambda\",\n       y = \"Coefficient Estimate\") +\n  theme(text=element_text(size=13))\nIt is possible plot each of the fitted models on top of the 3D scatterplot to see how the coefficients change with lambda. Due to the limitations of 3D-plotting with plotly, legends for each lambda value are not shown. The lambda values can instead be found be hovering each of the planes in the plot. It is seen that the slopes are reduced as lambda increases.\nShow the code\n# Make predictions for each function\nridge_predictions &lt;- ridge_models |&gt;\n  map2(lambda_grid, function(model, lambda) {\n    model |&gt; \n      predict(mtcars) |&gt;\n      bind_cols(lambda = lambda)\n  }) |&gt; \n  bind_rows()\n\n# Bind col predictions to data set\nmtcars_train_w_pred &lt;- mtcars |&gt; \n  bind_cols(ridge_predictions |&gt;\n              pivot_wider(names_from = \"lambda\",\n                          values_from = .pred,\n                          names_prefix = \"lambda_\",\n                          values_fn = list) |&gt;\n              unnest(everything())) |&gt; \n  pivot_longer(cols = starts_with(\"lambda_\"),\n               names_to = \"lambda\",\n               values_to = \"prediction\") |&gt; \n  mutate(lambda = as.numeric(stringr::str_remove(lambda, \"lambda_\")),\n         lambda = round(lambda, 3))\n\n# Create visualization\nmtcars_3d_scatter &lt;- mtcars |&gt; \n  plot_ly(x = ~wt,\n          y = ~hp,\n          z = ~mpg,\n          color = ~hp,\n          type = \"scatter3d\",\n          mode = \"markers\")\n\nplot_w_predictions &lt;- mtcars |&gt;\n  plot_ly(\n    x = ~ wt,\n    y = ~ hp,\n    z = ~ mpg,\n    type = \"scatter3d\",\n    mode = \"markers\"\n  ) |&gt;\n  plotly::layout(\n    scene = list(\n      xaxis = list(title = \"Weight\"),\n      yaxis = list(title = \"Horse Power\"),\n      zaxis = list(title = \"Miles per Gallon\"),\n      camera = list(\n        eye = list(x = 1.5, y = -2, z = 0.5)\n      )),\n    showlegend = FALSE)\n\nfor (lambda in lambda_grid) {\n  mtcars_train_w_pred_filt &lt;- mtcars_train_w_pred |&gt;\n    filter(lambda == !!round(lambda, 3))\n\n  plot_w_predictions &lt;- plot_w_predictions |&gt;\n    add_trace(data = mtcars_train_w_pred_filt,\n              x = ~wt,\n              y = ~hp,\n              z = ~prediction,\n              color = ~lambda,\n              type = \"mesh3d\",\n              name = stringr::str_c(\"Lambda: \", lambda))\n} \nplot_w_predictions",
    "crumbs": [
      "Applied Biostatistical Methods",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Elastic Net</span>"
    ]
  },
  {
    "objectID": "stat06_enet.html#ridge-regression",
    "href": "stat06_enet.html#ridge-regression",
    "title": "9  Elastic Net",
    "section": "",
    "text": "9.1.1 Applying Ridge Regression to Logistic Regression\nRecall from Chapter 8 on Logistic Regression, that the algorithm seek to maximize the likelihoods:\n\\[\n\\beta_{LR} = \\max_{\\beta} \\; L(\\beta,x_i,y_i)\n\\]\nWhere \\(L(\\beta,x_i,y_i)\\) is the likelihood function mentioned in the chapter, \\(\\beta\\) are the coefficients, ‘y’ are the classes, and ‘x’ are individual probabilities. Similarly to OLS, a penalization term is included when applying Ridge Regression. Since Logistic Regression seek to maximize likelihoods, whereas OLS seek to minimize residuals, the included penalization term is subtracted from the expression:\n\\[\n\\beta_{LR, Ridge} = \\max_{\\beta} \\; L(\\beta,x_i,y_i) - \\lambda||\\beta||_{2}^{2}\n\\]",
    "crumbs": [
      "Applied Biostatistical Methods",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Elastic Net</span>"
    ]
  },
  {
    "objectID": "stat06_enet.html#lasso-regression",
    "href": "stat06_enet.html#lasso-regression",
    "title": "9  Elastic Net",
    "section": "9.2 Lasso Regression",
    "text": "9.2 Lasso Regression\nLasso is similar to Ridge, except the penalty uses the absolute values of the coefficients instead of squaring them. This enables the Lasso Regression to set some coefficients to 0, resulting in a less complex model as it contains fewer features. Again, the \\(\\lambda\\) hyperparameter is used to control the penalization. \\(\\lambda\\) ranges from 0 to \\(+\\infty\\), where higher values of \\(\\lambda\\) gives a more penalized model, i.e. contain fewer features. The Lasso applied to OLS is given by:\n\\[\n\\beta_{Lasso} = \\min_{\\beta} ||y-X\\beta||_{2}^{2} + \\lambda||\\beta||_{1}\n\\]\nBy removing features, the algorithm can be used for dimensionality reduction. To exemplify, the below code chunk tries to predict mpg given all features using Lasso Regression. The expectation is, that some features contain too little information about the response, and is removed at higher values of \\(\\lambda\\). Since we are not comparing different models, a much simpler approach can be applied:\n\nset.seed(1337)\n# Setup model specs\nlasso_spec &lt;- linear_reg(penalty = tune(),\n                         mixture = 1) |&gt; \n  set_engine(engine = \"glmnet\") |&gt; \n  set_mode(\"regression\")\n\n\n# Change values of hyperparameters to search across\nlasso_params &lt;- lasso_spec |&gt;\n  extract_parameter_set_dials() |&gt;\n  update(penalty = penalty(c(-1, 0.1)))\n\n# Tune the grid and find optimal lambda\nlasso_grid_results &lt;- lasso_spec |&gt;\n  tune_grid(preprocessor = mpg ~ .,\n            resamples = mtcars_folds,\n            param_info = lasso_params,\n            metrics = metric_set(rmse),\n            grid = 100) |&gt;\n  select_best(metric = \"rmse\")\n\n\n# Update our model with the tuned parameter\nlasso_tuned &lt;- lasso_spec |&gt;\n  finalize_model(lasso_grid_results)\n\n# Fit model to data and get terms\nlasso_tuned |&gt;\n  fit(mpg ~ .,\n      data = mtcars_train) |&gt; \n  tidy() |&gt; \n  filter(estimate != 0)\n\n# A tibble: 7 × 3\n  term        estimate penalty\n  &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)  11.9      0.150\n2 hp           -0.0209   0.150\n3 drat          2.24     0.150\n4 wt           -1.83     0.150\n5 qsec          0.501    0.150\n6 am            3.23     0.150\n7 carb         -0.295    0.150\n\n\nAs can be seen from the final output, the Lasso Regression has removed some features from the model. Six features remains in the model (and the intercept), whereas the last five are set to zero.\n\n9.2.1 Applying Lasso Regression to Logistic Regression\nAs for Ridge, Lasso can be used with logistic regression as well by subtracting the L1 norm of the coefficients.\n\\[\n\\beta_{LR, Lasso} = \\max_{\\beta} \\; L(\\beta,x_i,y_i) - \\lambda||\\beta||_{1}\n\\]",
    "crumbs": [
      "Applied Biostatistical Methods",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Elastic Net</span>"
    ]
  },
  {
    "objectID": "stat07_svm.html",
    "href": "stat07_svm.html",
    "title": "10  Support Vector Machines",
    "section": "",
    "text": "10.1 The Kernel Trick\nCases where data is not linearly separable with an acceptable classification error could arise. The plot below is an example of such a case:\ntest_data &lt;- tibble::tibble(class = c(rep(\"Sick\", 4),\n                                      rep(\"Healthy\", 5),\n                                      rep(\"Sick\", 4)),\n                            value = c(3, 7, 11, 17,\n                                      25, 26, 27, 31, 32,\n                                      41, 43, 53, 55))\n\ntest_data |&gt; \n  ggplot(aes(x = value,\n             y = 0,\n             color = class)) +\n  geom_point(size = 3) +\n  theme(text=element_text(size=13))\nThe data can be transformed by adding a new dimension, e.g. the square of the original value. The data is then linearly separable:\ntest_data |&gt; \n  ggplot(aes(x = value,\n             y = value^2,\n             color = class)) +\n  geom_point(size = 3) +\n  geom_abline(intercept = -850,\n              slope = 61,\n              color = \"blue\",\n              linewidth = 1) +\n  theme(text=element_text(size=13))\nApplying a transformation of this type is computationally expensive, especially with large data sets. A trick can be applied through kernels, hence The Kernel Trick, by exploiting dot products and thereby reduce the computational cost.\nAn example of a kernel is the polynomial kernel:\n\\[\nK(x,y) = \\phi(x)^T \\phi(y) =  (c + x^Ty)^d\n\\]\nWhere \\(K(x,y)\\) is a vector of transformed values for each observation in \\(x\\) and \\(y\\), \\(\\phi\\) is a feature mapping, \\(d\\) is the degree of the polynomial and \\(c\\) is a hyperparameter. \\(c\\) has a regularizing effect where larger values reduces the effect of \\(x^Ty\\).\nThe idea is, that instead of transforming \\(x\\) and \\(y\\) with \\(\\phi\\) and then calculating the dot product, the dot product can be calculated directly on the input space. This saves computational power as \\(\\phi\\) maps the data into a higher dimension, which is computationally expensive. The mathematical benefit of the Kernel Trick is exemplified below where \\(c = 1\\) for simplicity. The feature mapping \\(\\phi\\) needed to transform the data can be found by expanding the polynomial kernel:\n\\[\nK(x,y) = (1 + x^Ty)^2  = (1 + x_1y_1 + x_2y_2)^2 \\rightarrow\n\\] \\[\nK(x,y) = 1 + 2x_1y_1 + 2x_2y_2 + 2x_1y_1x_2y_2 + x_1^2y_1^2 + x_2^2y_2^2\n\\]\nTo get the above expanded expression by the dot product \\(\\phi(x)^T \\phi(y)\\), the feature mapping \\(\\phi\\) is:\n\\[\nK(x,y) = (1 + \\sqrt{2}x_1 + \\sqrt{2}x_2 + \\sqrt{2}x_1x_2 + x_1^2 + x_2^2)\n         (1 + \\sqrt{2}y_1 + \\sqrt{2}y_2 + \\sqrt{2}y_1y_2 + y_1^2 + y_2^2)^T\n\\]\nSo the feature mapping \\(\\phi\\) is:\n\\[\n\\phi(x)^T = (1 + \\sqrt{2}x_1 + \\sqrt{2}x_2 + \\sqrt{2}x_1x_2 + x_1^2 + x_2^2)\n\\]\n\\[\n\\phi(y) = (1 + \\sqrt{2}y_1 + \\sqrt{2}y_2 + \\sqrt{2}y_1y_2 + y_1^2 + y_2^2)\n\\]\nUltimately, the Kernel Trick allows for calculating the dot product in the input \\((x^Ty)\\) space without transforming the data into a higher dimension, here six dimensions, and then calculating the dot product.",
    "crumbs": [
      "Applied Biostatistical Methods",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Support Vector Machines</span>"
    ]
  },
  {
    "objectID": "stat08_bias_var.html",
    "href": "stat08_bias_var.html",
    "title": "11  Bias and Variance",
    "section": "",
    "text": "12 Introduction to the Bias-Variance Tradeoff\nIn supervised machine learning, the ultimate goal is usually to create models which can predict the response of unseen data. This is the ability of a model to generalize well to new data and is measured by the Generalization Error. The ability to generalize can be decomposed into the balance between bias and variance of a model. Therefore, understanding bias and variance is crucial in machine learning since it aids in developing models that generalize well to new data.\nThe bias of a model is measured by how close the predictions for different training sets are to the true values. A model with low bias make predictions close to the true values, whereas a model with high bias make predictions that are far from the true values. High bias usually occurs when a model is too simple, and cannot capture the underlying complex structure of the data. For example, a linear model will have high bias when the true relationship between the features and the target is non-linear. Opposite, low bias occurs when a model is complex enough to capture the underlying structure of the data, but can also occur when the model is too complex and overfits the data. To measure whether the model has overfit, the variance is introduced.\nThe variance of a model is measured by how much the predictions vary for different training sets, i.e. how much the model simply just memorizes the training data and fits too closely to the data points. A model with low variance makes similar predictions for different training sets, whereas a model with high variance makes different predictions for different training sets. High variance usually occurs when a model is too complex and fits the noise of the data. A polynomial model of high degree is an example of a model with high variance, as it models tightly to the data points. A low variance model generalizes well to new data, as it makes similar predictions for different training sets, but it tends to make incorrect predictions, i.e. have high bias.\nModels with low bias (precise on average) tends to have high variance (inconsistent across training sets) and vice versa. An optimal model has a both low bias and variance, but since it is difficult to obtain, a good balance between bias and variance is usually sought. This is called the Bias-Variance Tradeoff.",
    "crumbs": [
      "Applied Biostatistical Methods",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Bias and Variance</span>"
    ]
  },
  {
    "objectID": "stat08_bias_var.html#ridge-regression-example",
    "href": "stat08_bias_var.html#ridge-regression-example",
    "title": "11  Bias and Variance",
    "section": "12.1 Ridge Regression Example",
    "text": "12.1 Ridge Regression Example\nRidge Regression is a good example of the Bias-Variance Tradeoff. When the penalty is set to 0, the Ridge model is the OLS solution. Assuming that there is a linear relationship between the features and the response, the OLS model has low bias as it on average makes predictions close to the true values. However, the OLS model has high variance as it is sensitive to the noise in the data. By applying Ridge penalty, the coefficients of the model is reduced, and is thereby less sensitive to the noise in the data - lower variance and a lower chance of overfitting. Meanwhile, it also forces the model to fit the data less closely, which increases the bias of the model.",
    "crumbs": [
      "Applied Biostatistical Methods",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Bias and Variance</span>"
    ]
  },
  {
    "objectID": "stat08_bias_var.html#the-bootstrap-method",
    "href": "stat08_bias_var.html#the-bootstrap-method",
    "title": "11  Bias and Variance",
    "section": "12.2 The Bootstrap Method",
    "text": "12.2 The Bootstrap Method\nA detail which proves difficult to understand, is that an obtained data set, e.g. the iris data set is in itself a sample from some total population. It is not feasible to measure all iris flowers, and as such the true relationship between the features and the target is unknown. Therefore, the bias and variance of a model is not directly measurable, as it is not possible to compare the predictions of the model to the true values. Instead, it is assumed that the obtained data set is a good representation of the total population, i.e. they have similar distributions. Bias and variance is estimated by fitting a model multiple times to different training sets, but only one data set is available. Cross-validation is a method for estimating the bias and variance of a model, as it simulates different training sets. It is done by splitting the data set into multiple training and validation sets, again assuming each fold is a good representation of the total population. Another method is the Bootstrap method which is applied in the below examples. A Bootstrap sample is a sample drawn with replacement from the original data set. Usually, the sample drawn is of the same size as the total data set. With replacement means that the same observation can be drawn multiple times, which is how the Bootstrap samples differs from the original data set.\nTo exemplify what is meant by the Bootstrap method, a sample of 10 numbers is drawn from the numbers 1 to 10. The sample is drawn with replacement, meaning that the same number can be drawn multiple times:\n\nsample_10 &lt;- tibble(1:10)\nsample_frac(sample_10,\n            1,\n            replace = TRUE)\n\n# A tibble: 10 × 1\n   `1:10`\n    &lt;int&gt;\n 1     10\n 2      6\n 3      1\n 4     10\n 5      3\n 6     10\n 7      4\n 8      4\n 9     10\n10      5\n\n\nTo visualize the Bootstrap method, 10000 samples are drawn from a normal distribution with mean 0 and standard deviation 1, and plotted. The histogram is expected to follow the known bell shape of the normal distribution:\n\nsamples_10k &lt;- rnorm(10000) |&gt; \n  as_tibble()\n\nsamples_10k |&gt;\n  ggplot(aes(x = value)) +\n  geom_histogram(bins = 50,\n                 fill = \"steelblue\") +\n  scale_y_continuous(expand = c(0, 0,\n                                0.01, 0.01)) +\n  labs(title = \"Normal Distribution\",\n       x = \"Value\",\n       y = \"Count\") +\n  theme(text=element_text(size=13))\n\n\n\n\n\n\n\n\nBootstrapping the samples should result in a similar distribution, but contains different values, i,e. different data sets but drawn from the same distribution:\n\n# Bootstrap sample\nbootstrap_10k &lt;- samples_10k |&gt; \n  sample_frac(1,\n              replace = TRUE)\n\n\nbootstrap_10k |&gt;\n  ggplot(aes(x = value)) +\n  geom_histogram(bins = 50,\n                 fill = \"steelblue\") +\n  scale_y_continuous(expand = c(0, 0,\n                                0.01, 0.01)) +\n  labs(title = \"Bootstrap Sample\",\n       x = \"Value\",\n       y = \"Count\") +\n  theme(text=element_text(size=13))",
    "crumbs": [
      "Applied Biostatistical Methods",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Bias and Variance</span>"
    ]
  },
  {
    "objectID": "stat08_bias_var.html#bias-and-variance-example",
    "href": "stat08_bias_var.html#bias-and-variance-example",
    "title": "11  Bias and Variance",
    "section": "12.3 Bias and Variance Example",
    "text": "12.3 Bias and Variance Example\nTo visualize the meaning of bias and variance, a data set has been randomly generated with a sine function and some noise (random jitter). Polynomials of different degrees are then fitted to the data. For properly visualizing the terms, models are fitted to multiple samples taken from the data. Therefore, the data set is resampled with the Bootstrap method to simulate different training sets but with the same distribution.\nThe expectation is, that a polynomial of low degree makes a bad prediction on average, but is consistent across different training sets, i.e. high bias and low variance. A polynomial of high degree makes a good prediction on average, but is inconsistent across different training sets, i.e. low bias and high variance. The optimal model is a polynomial of moderate degree, which makes good predictions and is consistent across different training sets.\nThe data generated show clear signs of following a sine function, but with some noise. The black line is the true sine function, and the blue points are the generated data.\n\nggplot(df_poly, aes(x = X, y = y)) +\n  geom_point(aes(color = \"Data points\"),\n             size = 2) +\n  stat_function(aes(color = \"True sine function\"),\n                fun = sin,\n                linewidth = 1) +\n  labs(title = \"Generated Data\",\n       x = \"X\",\n       y = \"y\",\n       color = \"Legend\") +\n  scale_color_manual(values = c(\"Data points\" = \"steelblue\",\n                                \"True sine function\" = \"black\")) +\n  theme(text=element_text(size=13))\n\n\n\n\n\n\n\n\nA polynomial of degree 1 (a linear prediction), obviously do not capture the complexity of the data, but is consistent across different training sets, i.e. high bias and low variance.\n\ndf_poly |&gt; \n  plot_multiple_poly_models(degree = 1,\n                            n_models = 30)\n\n\n\n\n\n\n\n\nA polynomial of degree 20 captures the complexity of the data, but also models the noise which was added, but the average of the models are close to the true sine function. The model is overfitting the data, thereby creating variance between different Bootstrap samples, i.e. low bias and high variance.\n\ndf_poly |&gt; \n  plot_multiple_poly_models(degree = 20,\n                            n_models = 30)\n\n\n\n\n\n\n\n\nA polynomial of degree 4 captures the complexity of the data without fitting too tightly to the noise. The average of the models are close to the true sine function, and the models are consistent across different training sets, i.e. low bias and low variance. Hence, it is the optimal model out of degree 1, 20 and 4.\n\ndf_poly |&gt; \n  plot_multiple_poly_models(degree = 4,\n                            n_models = 10)",
    "crumbs": [
      "Applied Biostatistical Methods",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Bias and Variance</span>"
    ]
  },
  {
    "objectID": "pre00_intro.html",
    "href": "pre00_intro.html",
    "title": "Preprocessing of Data",
    "section": "",
    "text": "It is rarely seen that data is ready for analysis as soon as it is collected. Data preprocessing is a crucial step in the modeling process. It involves cleaning, transforming, and encoding data to optimize it for further analysis. There is no one-step process to data preprocessing. It is an iterative process that involves multiple steps, and is highly dependent on the model you are building, the data you are working with and the objective you are trying to achieve.\nSome issues that often needs to be addressed during data preprocessing are missing values, outliers, scaling and encoding categorical data, whereas some other issues are more specific to the model you are building, such as dimensionality reduction and data augmentation.\nMissing data: The absence of a value in a feature is a common issue, and could arise from various reasons such as malfunction in measurement equipment or respondents not answering a question. Many models cannot handle missing data, and it is important to address this issue before proceeding with the analysis. One way would be to remove rows or columns with missing values, but this could lead to loss of valuable information. Otherwise, the missing values could be imputed with e.g. the mean.\nOutliers: Outliers are values that are somehow different from the rest of the data. It can be discussed if some value is an outlier or just an extreme value. Outliers are usually values that should not be able to occur, and would therefore have a negative impact on the model. It is important to identify and remove outliers before proceeding with the analysis.\nData Transformation: For some data sets, the data could be skewed, which means that the data is not normally distributed, but some models assume that the data is normally distributed. A solution could be to take the logarithm or square root of the data. In the field of metagenomics, data is usually compositional, which means that the data is in the form of proportions, and the sum of the data is constant. To move the data from the simplex to the Euclidean space, the data could be transformed with the centered log-ratio transformation.\nScaling: Many machine learning models are sensitive to the scale of the data. For example, Principal Component Analysis finds variance along axis, and if the values along one axis are much larger than the values along another axis, the variance will be dominated by the first axis which is showcased in the chapter on PCA (12  Principal Component Analysis). This could lead to misleading results. It is therefore important to scale the data before applying such models. A common way to scale is by standardizing, which means that the data is transformed to have a mean of 0 and a standard deviation of 1. This is achieved by subtracting the mean and dividing by the standard deviation.\nEncode categorical data: Categorical data could easily be string values, and many models cannot handle this. The issue is solved by encoding the data, e.g. with one-hot encoding. A categorical value with k categories is transformed into k binary features, where one is 1 and the others are 0.\nDimensionality reduction: Data of high dimensionality, usually with more features than observations can impose challenges for the analysis. Some models simply do not work with high-dimensional data such as OLS, whereas others could be computationally expensive or lead to overfitting. The more features, the more observations are needed to fill out the feature space. Dimensionality reduction is a way to reduce the number of features in the data, while still preserving the most important information. Principal Component Analysis and Uniform Manifold Approximation and Projection are common methods for dimensionality reduction. A simple feature selection also results in dimensionality reduction. All solution would also improve the computational costs.\nData Augmentation: When working with small data sets, the model could be overfitting. Data augmentation is used to increase the size of the data set by creating new data points from the existing data. When working with image data, the data set could be increased by rotating, flipping, or zooming the images. This is done to increase the size of the data set, which could lead to a more robust model.\nIn the two final chapters of this book (15  Linear Modelling and 16  Non-Linear Modelling), the disease status of individuals is predicted based on their gut microbiota. Such data is usually high-dimensional, and some of the data preprocessing steps mentioned above are crucial to obtain a good model. There is usually no missing values given the pipeline it was generated from, but the data is compositional and needs to be transformed. The data is also high-dimensional, and dimensionality reduction techniques are used to extract the important information from the data. Hence, the next chapters is on different dimensionality reduction techniques which are also available in the tidymodels package.",
    "crumbs": [
      "Preprocessing of Data"
    ]
  },
  {
    "objectID": "pre01_pca.html",
    "href": "pre01_pca.html",
    "title": "12  Principal Component Analysis",
    "section": "",
    "text": "12.1 Explanation and Example\nWorking with high dimensional data carries its own challenges such as the curse of dimensionality, overfitting, and computational complexity. Data sets of high dimensions are hard to visualize, and the data is often noisy. They are usually encountered in fields such as metagenomics, other omics fields and image processing. In the worse cases, the number of features far exceed the number of observations which can severely affect the performance of machine learning models. Ways of dealing with high dimensional data includes feature selection and dimensionality reduction. On such solution is Principal Component Analysis (PCA).\nMultiple reasons for applying PCA exists. One of the most common reasons is to visualize high-dimensional data in lower dimensions. Another reason could be to decorrelate features or to reduce the noise in the data by only combining and retaining the features that contain most of the variance.\nWhen having more than 3 features, it is not possible to visualize them in a single plot. To overcome this, PCA can reduce the dimensionality of the data to lower dimensions (e.g. 2 or 3) while preserving the variance of the data. Each Principal Component (PC) is a linear combination of the original features, and can now be considered as a feature on its own replacing the originals. A data set with many features, e.g. the 123 features in the count matrix, can be visualised with a 2D plot. Loosing that many dimensions is almost bound to loose some information, but a PCA optimizes towards retaining as much as possible.\nTo exemplify and showcasing the method and the underlying math, the mtcars data set is used. For this example, two dimensions wt (weight) and mpg (miles pr. gallon) are used. The points are coloured by cyl (cylinders) to show how the data is grouped in the 2D space. The goal is to retain the information, but reduce the dimensions to a single line that best describes the data. This line is called the first Principal Component (PC1).\nmtcars |&gt; \n  ggplot(aes(x = wt,\n             y = mpg,\n             col = cyl)) +\n  geom_point(size = 3) +\n  labs(title = \"Miles pr Gallon vs Weight\",\n       x = \"Weight (1000 lbs)\",\n       y = \"Miles pr Gallon\",\n       colour = \"Cylinders\") +\n  scale_colour_viridis_c() +\n  theme(text=element_text(size=13))\nThe data is centered, i.e. the mean of each feature is subtracted from each observation within that feature.\n\\[\nX_{i,j \\;centered} = X_{i,j} - \\mu_{j}\n\\]\nWhere X is the data matrix, i is the observation and j is the feature and \\(\\mu\\) the mean. By subtracting the mean, all data observations are centered around the origin. This makes some mathematical operations easier, such as rotating the coordinate system. Centering the data do not affect the correlation between observations. The resulting plot is then:\nmtcars_centered_plot &lt;- mtcars |&gt; \n  ggplot(aes(x = wt - mean(wt),\n             y = mpg - mean(mpg),\n             col = cyl - mean(cyl))) +\n  geom_point(size = 3) +\n  labs(title = \"Miles pr Gallon vs Weight\",\n       x = \"Weight (1000 lbs)\",\n       y = \"Miles pr Gallon\",\n       colour = \"Cylinders\") +\n  scale_colour_viridis_c() +\n  theme(text=element_text(size=13))\nmtcars_centered_plot\nA line is to be fit to the data. This line should maximize the variance of the data, i.e. maximize the sum of squared distances from the origin to the points projected on to the line. A rough sketch of the scenario can be seen below. A proposed best fit line that maximizes the variance of the data is drawn in red. The dashed green line is a point projected onto the proposed best fit line. The dashed blue line is the distance from the point on the line to the origin. It is the length of the blue line that is maximized across all data points.\nShow the code\nmtcars_centered_plot +\n  geom_abline(intercept = 0, slope = -6, colour = \"red\") +\n  geom_segment(aes(x = -1, y = 12.5, xend = -1.35, yend = 8.4),\n               colour = \"green\",\n               linetype = \"dashed\",\n               size = 1) +\n  geom_segment(aes(x = -1.35, y = 8.4, xend = 0, yend = 0), \n               colour = \"steelblue\",\n               linetype = \"dashed\",\n               size = 1)\nThe line that maximizes the sum of squared distances is PC1. All future Principal Components are orthogonal to the previous and run through the origin, which means no optimization for subsequent PCs. To visualize PC1, all data points are projected onto the line, and the line is rotated to be horizontal and act as a new x-axis. The resulting plot is:\nShow the code\npca &lt;- mtcars |&gt; \n  select(wt, mpg) |&gt;\n  prcomp(center = TRUE) |&gt; \n  tidy(matrix = \"x\") |&gt; \n  pivot_wider(names_from = \"PC\",\n              names_prefix = \"PC\",\n              values_from = \"value\") |&gt;\n  rename(car = row)\n\npca_mtcars &lt;- pca |&gt; \n  left_join(mtcars |&gt;\n              rownames_to_column(var = \"car\"),\n            by = \"car\")\n\npca_mtcars |&gt;\n  ggplot(aes(x = PC1,\n             y = 0,\n             col = cyl)) +\n  geom_point(size = 3) +\n  labs(title = \"Miles pr Gallon vs Weight\",\n       x = \"PC1\",\n       y = \"\",\n       colour = \"Cylinders\") +\n  scale_colour_viridis_c() +\n  theme(text=element_text(size=13))\nAs can be observed from plotting PC1, the groupings of the data is conserved, but the dimensions are reduced to a single line. The same method can be applied to high dimensional data, such as the count matrix.",
    "crumbs": [
      "Preprocessing of Data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Principal Component Analysis</span>"
    ]
  },
  {
    "objectID": "pre01_pca.html#mathematical-explanation",
    "href": "pre01_pca.html#mathematical-explanation",
    "title": "12  Principal Component Analysis",
    "section": "12.2 Mathematical Explanation",
    "text": "12.2 Mathematical Explanation\n\n12.2.1 Covariance Matrix\nTo mathematically describe the method the covariance matrix, often denoted \\(\\Sigma\\), of the data is calculated. The covariance matrix is a square matrix where the diagonal contains the variance of each feature, and the off-diagonal contains the covariance between each feature:\n\\[\n\\Sigma =  \\begin{bmatrix}\nVar(x_1) & Cov(x_2, x_1) \\\\\nCov(x_1, x_2) & Var(x_2)\n\\end{bmatrix}\n\\]\nWhere \\(x_1\\) and \\(x_2\\) are columns in a data set \\(X\\). The variance in a feature is the spread of the data along the axis, and the covariance is the spread of the data between the axes. The covariance matrix is symmetric, meaning the covariance between \\(x_1\\) and \\(x_2\\) is the same as the covariance between \\(x_2\\) and \\(x_1\\). The variance of each feature is calculated as:\n\\[\nVar(x) = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\mu_x)^2\n\\]\nRemember that the data was centered by subtracting the mean, \\(\\mu_x\\). As a consequence, the mean of all columns of \\(X\\) is 0, and the expression can be reduced to:\n\\[\nVar(x) = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i)^2\n\\] The covariance between two features is calculated as:\n\\[\nCov(x_1, x_2) = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_{1i} - \\mu_{x_1})(x_{2i} - \\mu_{x_2})\n\\]\nWhere \\(x_{1i}\\) and \\(x_{2i}\\) are the \\(i\\)’th observations in columns \\(x_1\\) and \\(x_2\\), respectively. Again, the mean is zero, and the expression can be reduced to:\n\\[\nCov(x_1, x_2) = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_{1i} \\cdot x_{2i})\n\\]\nThe covariance of the mtcars data set used in the example is then:\n\nmtcars |&gt;\n  select(wt, mpg) |&gt; \n  cov()\n\n           wt       mpg\nwt   0.957379 -5.116685\nmpg -5.116685 36.324103\n\n\n\n\n12.2.2 Eigenvectors and Eigenvalues\nAs the covariance matrix show how the features varies within themselves and between each other, the eigenvectors of such a matrix spans the directions in the feature space where the data have the most variance. By convention, eigenvectors have unit length (length 1), and as such it is their associated eigenvalues that quantify the length of the eigenvector, and thereby the variance explained by each eigenvector.\nTo find the eigenvectors, the eigenvalues are found first by solving the equation:\n\\[\ndet(\\Sigma - \\lambda I) = 0\n\\]\nWhere \\(\\Sigma\\) is the covariance matrix, \\(\\lambda\\) is the eigenvalue, and \\(I\\) is the identity matrix. As the covariance matrix in the example is a 2x2 matrix, the identity matrix is likewise a 2x2 matrix and two eigenvalues are found. The eigenvalues are then found by solving the equation:\n\\[\ndet\\left(\n\\begin{bmatrix}\n0.96 & -5.12 \\\\\n-5.12 & 36.32\n\\end{bmatrix}\n-\n\\begin{bmatrix}\n\\lambda & 0 \\\\\n0 & \\lambda\n\\end{bmatrix}\n\\right)\n=\ndet\\left(\n\\begin{bmatrix}\n(0.96 - \\lambda) & -5.12 \\\\\n-5.12 & (36.32 - \\lambda)\n\\end{bmatrix}\n\\right) = 0\n\\]\nIt can be further simplified to:\n\\[\n(0.96 - \\lambda)(36.32 - \\lambda) - (-5.12 \\cdot -5.12) =\n\\]\n\\[\n34.87 - 0.96\\lambda - 36.32\\lambda + \\lambda^2 - 26.21 =\n\\]\n\\[\n8.66 - 37.28\\lambda + \\lambda^2 = 0\n\\]\nSolving the quadratic equation, the \\(\\lambda\\) values are:\n\\[\n\\lambda_1 = 37.05, \\; \\; \\lambda_2 = 0.234\n\\]\nThe eigenvectors are then found by solving the equation:\n\\[\n\\Sigma \\cdot v = \\lambda \\cdot v\n\\]\nWhere \\(\\Sigma\\) is the covariance matrix, \\(v\\) is the eigenvector, and \\(\\lambda\\) is the eigenvalue. For the two eigenvectors, the equations are:\n\\[\n\\begin{bmatrix}\n0.96 & -5.12 \\\\\n-5.12 & 36.32\n\\end{bmatrix}\n\\cdot v_1 = 37.05 \\cdot v_1\n\\]\n\\[\n\\begin{bmatrix}\n0.96 & -5.12 \\\\\n-5.12 & 36.32\n\\end{bmatrix}\n\\cdot v_2 = 0.234 \\cdot v_2\n\\]\nThe two vectors are then:\n\\[\nv_1 = \\begin{bmatrix}\n-0.14 \\\\\n1\n\\end{bmatrix}, \\; \\; v_2 = \\begin{bmatrix}\n7.05 \\\\\n1\n\\end{bmatrix}\n\\]\nIn unit length, the eigenvectors are:\n\\[\nv_1 = \\begin{bmatrix}\n-0.14 \\\\\n1\n\\end{bmatrix}, \\; \\; v_2 = \\begin{bmatrix}\n1 \\\\\n0.14\n\\end{bmatrix}\n\\] ### Plotting of Eigenvectors\nTwo eigenvectors can be plotted on top of the data to show the direction of the variance in the data. The green line is the first eigenvector, i.e. has the largest eigenvalue, and the blue line is the second eigenvector. An immediate observation is, that these two lines are not orthogonal, which is a requirement for eigenvectors. This is due to the scaling of the plot.\n\nmtcars_centered_plot +\n  geom_segment(aes(x = 0, y = 0, xend = -0.14, yend = 1),\n               colour = \"green\",\n               size = 1) +\n  geom_segment(aes(x = 0, y = 0, xend = 1, yend = 0.14), \n               colour = \"steelblue\",\n               size = 1)\n\n\n\n\n\n\n\n\nTo visually show that the two eigenvectors are indeed orthogonal, the plot can either be scaled to have the same aspect ratio on both axis, or to limit the axis to the same range:\n\nmtcars_centered_plot +\n  geom_segment(aes(x = 0, y = 0, xend = -0.14, yend = 1),\n               colour = \"green\",\n               size = 1) +\n  geom_segment(aes(x = 0, y = 0, xend = 1, yend = 0.14), \n               colour = \"steelblue\",\n               size = 1) +\n  coord_fixed()\n\n\n\n\n\n\n\nmtcars_centered_plot +\n  geom_segment(aes(x = 0, y = 0, xend = -0.14, yend = 1),\n               colour = \"green\",\n               size = 1) +\n  geom_segment(aes(x = 0, y = 0, xend = 1, yend = 0.14), \n               colour = \"steelblue\",\n               size = 1) +\n  lims(x = c(-2, 2), y = c(-2, 2))\n\n\n\n\n\n\n\n\n\n\n12.2.3 Importance of Scaling\nThe issue of scaling goes to show how important it is, that values are on the same scale before applying PCA. If the values are not on the same scale, the PCA will be biased towards the features with the largest values. This can be seen in the example, where the wt feature has a larger range than the mpg feature. The eigenvectors are then biased towards the wt feature. The data can be scaled by both centering and dividing by the standard deviation, which is called standardization. The data is then on the same scale, and the PCA is not biased towards any feature:\n\\[\nX_{i,j \\;standardized} = \\frac{X_{i,j} - \\mu_{j}}{sd_j}\n\\]\nWhere X is the data matrix, i is the observation and j is the feature, \\(\\mu\\) is the mean and \\(sd\\) is the standard deviation. Using standardized data, the eigenvectors and their eigenvalues are:\n\\[\nv_1 =\n\\begin{bmatrix}\n0.707 \\\\\n-0.707\n\\end{bmatrix},\n\\; \\; \\lambda_1 = 1.87\\\\\n\\]\n\\[\nv_2 =\n\\begin{bmatrix}\n0.707 \\\\\n0.707\n\\end{bmatrix},\n\\; \\; \\lambda_2 = 0.13\n\\]\nThe eigenvectors are now orthogonal, and the first eigenvector (green) is the one that maximizes the variance of the data. The eigenvectors are scaled by their associated eigenvalue:\n\nmtcars |&gt; \n  ggplot(aes(x = (wt - mean(wt))/sd(wt),\n             y = (mpg - mean(mpg))/sd(mpg),\n             col = (cyl - mean(cyl)/sd(cyl)))) +\n  geom_point(size = 3) +\n  geom_segment(aes(x = 0, y = 0, xend = 0.707*1.87, yend = -0.707*1.87),\n               colour = \"green\",\n               size = 1) +\n  geom_segment(aes(x = 0, y = 0, xend = 0.707*0.13, yend = 0.707*0.13), \n               colour = \"steelblue\",\n               size = 1) +\n  labs(title = \"Miles pr Gallon vs Weight\",\n       x = \"Weight (1000 lbs)\",\n       y = \"Miles pr Gallon\",\n       colour = \"Cylinders\") +\n  scale_colour_viridis_c() +\n  theme(text=element_text(size=13))\n\n\n\n\n\n\n\n\n\n\n12.2.4 Explained Variance\nUsing all PCs, one for each feature, it is possible to fully reconstruct the data since the combined variance of all PCs is equal to the total variance of the data. Therefore, it is also possible to calculate the proportion of variance explained by each PC. For the case of the scaled data, the two eigenvalues were \\(\\lambda_1 = 1.87\\) and \\(\\lambda_2 = 0.13\\). The proportion of variance explained by each PC is then:\n\\[\n\\frac{\\lambda_1}{\\lambda_1 + \\lambda_2} = 0.94, \\; \\; \\frac{\\lambda_2}{\\lambda_1 + \\lambda_2} = 0.06\n\\]\nThe proportion of variance explained by each PC is usually plotted to get an idea of how many PCs are needed to explain the data. The plot is called a scree plot, and can be seen below:\n\nvar_explained &lt;- tibble(prop_explained = c(0.94, 0.06),\n                        PC = c(\"PC1\", \"PC2\"))\n\nvar_explained |&gt;\n  ggplot(aes(x = PC,\n             y = prop_explained)) +\n  geom_col(fill = \"steelblue\") +\n  geom_text(aes(label = prop_explained),\n            vjust = -0.5) +\n  scale_y_continuous(expand = c(0, 0,\n                                0.01, 0.1)) +\n  labs(title = \"Proportion of Variance Explained\",\n       x = \"Principal Component\",\n       y = \"Proportion Explained\") +\n  theme(text=element_text(size=13))\n\n\n\n\n\n\n\n\n\n\n12.2.5 Projection of Data Onto PCs\nThe original data can be projected onto the PCA space by multiplying the data matrix with the eigenvectors. The projection ensures a change of basis, such that PC1 is the new x-axis, PC2 is the new y-axis, and so forth. The projection happens by:\n\\[\nS = XU\n\\]\nWhere \\(S\\) is the projected data, \\(X\\) is the original data matrix, and \\(U\\) is the matrix of eigenvectors. The matrix \\(U\\) then essentially works as a rotation matrix, rotating the data into the PCA space. When projecting unto fewer dimensions, the data is reduced to the number of eigenvectors (PCs) used in \\(U\\).\nAs an example, the first observation in the unscaled mtcars data set is projected onto the PCA space. The projection happens by:\n\\[\nS =\n\\begin{bmatrix}\n-0.6 & 0.91\n\\end{bmatrix} \\cdot\n\\begin{bmatrix}\n-0.14 & -0.99 \\\\\n0.99 & -0.14\n\\end{bmatrix} =\n\\begin{bmatrix}\n0.98 & 0.46\n\\end{bmatrix}\n\\]\nThe data point \\((-0.6, 0.91)\\) is then rotated such that the coordinates are \\((0.98, 0.46)\\) in the PCA space. What is important to note is how the values of the rotation matrix are used to rotate the data point.\nEach data point in the original space are on the form \\((x, y) = (wt, mpg)\\), so the first coordinate denotes wt and the second denotes mpg. Similarly, each PC consists of two coordinates. The first is multiplied by the first coordinate of the data point (here wt), and the second is multiplied by the second coordinate of the data point (here mpg) as per the definition of a matrix product. This implies, that the values of the eigenvectors are the weights of the data points in the PCA space. The first eigenvector has the values \\((-0.14, 0.99)\\), which means that the first coordinate of the data point is multiplied by \\(-0.14\\) and the second coordinate is multiplied by \\(0.99\\), i.e. the second coordinate is weighted more than the first. This can be related back to the fact, that mpg has a larger range than wt, and as such the first PC primarily explains mpg. As mentioned, the data is not scaled, and so this is a biased conclusion.\nThe important implication of this is, that a row in the rotation matrix contains values describing how each PC weights the original features. A common method is to plot the loadings, i.e. the rows of the rotation matrix, in the PCA space. The loadings are equal to the eigenvector matrix \\(U\\), but is at times scaled by the square root of the eigenvalues. It is done to encapsulate the difference in variance each PC explains. The loadings are then used to interpret the data in the PCA space, as they show which features that drive the variance within each PC. By using the R function prcomp the loadings is found as rotation. As can be observed, the loading for mpg points in the direction of PC1 where the biggest variance (spread) is observed. Once again, this is a biased observation, as the data is not scaled.\n\n\nShow the code\nloadings &lt;- mtcars |&gt; \n  select(wt, mpg) |&gt;\n  prcomp(center = TRUE) |&gt; \n  tidy(matrix = \"rotation\") |&gt; \n  pivot_wider(names_from = \"PC\",\n              names_prefix = \"PC\",\n              values_from = \"value\")\n\narrow_style &lt;- arrow(angle = 20,\n                     ends = \"first\",\n                     type = \"closed\",\n                     length = grid::unit(8, \"pt\"))\n\npca_mtcars |&gt; \n  ggplot() +\n  geom_point(aes(x = PC1,\n             y = PC2),\n             size = 3,\n             color = \"steelblue\") +\n  geom_segment(aes(x = PC1, y = PC2),\n               data = loadings,\n               xend = 0,\n               yend = 0,\n               arrow = arrow_style) +\n  geom_text(aes(x = PC1,\n                y = PC2,\n                label = column),\n            data = loadings,\n            nudge_y = -0.2,\n            color = \"orange\") +\n  labs(title = \"PC2 vs PC1\",\n       x = \"PC1\",\n       y = \"PC2\") +\n  coord_fixed() +\n  theme(text=element_text(size=13))",
    "crumbs": [
      "Preprocessing of Data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Principal Component Analysis</span>"
    ]
  },
  {
    "objectID": "pre01_pca.html#count-matrix",
    "href": "pre01_pca.html#count-matrix",
    "title": "12  Principal Component Analysis",
    "section": "12.3 Count Matrix",
    "text": "12.3 Count Matrix\nTo apply PCA to an example of higher dimensions, the count matrix is used. The built in function prcomp is used to calculate the PCs along with the loadings and eigenvalues (explained variance).\n\npca_count_matrix &lt;- count_matrix_clr |&gt; \n  column_to_rownames(var = \"Sample\") |&gt;\n  prcomp(center = TRUE,\n         scale = TRUE)\n\nThe proportion of variance explained by each PC is calculated and plotted. Here, the first PC explains approximately 6% of the variance.\n\npca_count_matrix |&gt;\n  tidy(matrix = \"eigenvalues\") |&gt;\n  ggplot(aes(x = reorder(PC, -percent),\n             y = percent)) +\n  geom_col(fill = \"steelblue\") +\n  scale_y_continuous(expand = c(0, 0)) +\n  labs(title = \"Proportion of Variance Explained\",\n       x = \"Principal Component\",\n       y = \"Proportion Explained\") +\n  theme(axis.text.x = element_blank(),\n        axis.ticks.x = element_blank()) +\n  theme(text=element_text(size=13))\n\n\n\n\n\n\n\n\nA cummulative plot over the PCs can be used to determine how many PCs are needed to explain the data. The green dashed line is set at 70% of the variance explained, and the red dashed line is set at the 49th PC. The plot then shows that 49 PCs are needed to explain 70% of the variance.\n\npca_count_matrix |&gt;\n  tidy(matrix = \"eigenvalues\") |&gt;\n  ggplot(aes(x = reorder(PC, -percent),\n             y = cumulative)) +\n  geom_col(fill = \"steelblue\") +\n  geom_hline(yintercept = 0.7,\n             linetype = \"dashed\",\n             colour = \"green\",\n             size = 1) +\n  geom_vline(xintercept = \"49\",\n             linetype = \"dashed\",\n             colour = \"red\",\n             size = 1) +\n  scale_y_continuous(expand = c(0, 0)) +\n  labs(title = \"Cummulative Proportion of Variance Explained\",\n       x = \"Principal Component\",\n       y = \"Cummulative Proportion Explained\") +\n  theme(axis.text.x = element_blank(),\n        axis.ticks.x = element_blank()) +\n  theme(text=element_text(size=13))\n\n\n\n\n\n\n\n\nIt is for quite obvious reasons not possible to plot the first 49 PCs in a single plot. Instead, the first two PCs are plotted to show the data in a 2D space. The data is colored by the Patient_status response, which shows that the data is not easily separable in the 2D space. Since the first two PCs only explain approximately 1% of the variance, it is expected that the data is not separable. It is then also an assumption, that the data is even separable in the high dimensional space which is unknown. If the data is separable, it is expected that the separation is more pronounced in the higher dimensions as more variance is explained.\n\npca_count_matrix |&gt;\n  tidy(matrix = \"x\") |&gt; \n  pivot_wider(names_from = \"PC\",\n              names_prefix = \"PC\",\n              values_from = \"value\") |&gt; \n  rename(Sample = row) |&gt; \n  left_join(meta,\n            by = \"Sample\") |&gt;\n  ggplot(aes(x = PC1,\n             y = PC2,\n             colour = factor(Patient_status))) +\n  geom_point(size = 3) +\n  labs(title = \"Principal Component Analysis\",\n       x = \"PC1\",\n       y = \"PC2\",\n       colour = \"Patient Status\") +\n  theme(text=element_text(size=13))",
    "crumbs": [
      "Preprocessing of Data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Principal Component Analysis</span>"
    ]
  },
  {
    "objectID": "pre02_umap.html",
    "href": "pre02_umap.html",
    "title": "13  Uniform Manifold Approximation and Projection",
    "section": "",
    "text": "Set seed and load packages.\n\n\nShow the code\nset.seed(1337)\n\nlibrary(\"tidymodels\")\ntidymodels::tidymodels_prefer()\nlibrary(\"uwot\")\n\n\nLoad data.\n\n\nShow the code\ndata(\"iris\")\niris &lt;- iris |&gt;\n  tibble::as_tibble()\n\ncount_matrix_clr &lt;- readr::read_rds(\"https://github.com/WilliamH-R/BioStatistics/raw/main/data/count_matrix/count_matrix_clr.rds\") |&gt; \n  select(-\"NA\")\n\nmeta &lt;- read.csv(file = \"data/metadata.txt\") |&gt; \n  as_tibble() |&gt;\n  select(Run, chem_administration, ETHNICITY, geo_loc_name,\n         Host_age, host_body_mass_index, Host_disease, host_phenotype, host_sex) |&gt; \n  rename(Sample = Run,\n         Treatment = chem_administration,\n         Ethnicity = ETHNICITY,\n         Location = geo_loc_name,\n         Age = Host_age,\n         BMI = host_body_mass_index,\n         Disease_severity = Host_disease,\n         EDSS = host_phenotype,\n         Sex = host_sex) |&gt;\n  mutate(Patient_status = case_when(Disease_severity == \"1HealthyControl\" ~ \"Healthy\",\n                                    TRUE ~ \"MS\"),\n         EDSS = as.factor(EDSS),\n         EDSS = case_when(is.na(EDSS) & Disease_severity == \"1HealthyControl\" ~ \"-1\",\n                          is.na(EDSS) & Disease_severity != \"1HealthyControl\" ~ \"Unknown\",\n                          TRUE ~ EDSS),\n         EDSS = as.factor(EDSS))\n\n\nWhile Uniform Manifold Approximation and Projection (UMAP) can resolve some of the same issues as a PCA (Chapter 12), e.g. dimensionality reduction, UMAP does so in a non-linear fashion unlike PCA. The model finds cluster in a high-dimensional space and project them onto a low dimension manifold trying to maintain the clustering.\nSimilarity scores are calculated between all points, and based on these are clustered in a k-nearest neighbor fashion. A low dimensional representation - usually 2 dimensional such that it can be visualized - is initialized and points are moved around taking the similarity scores into account. The process is repeated until the low dimensional representation is stable.\nUMAP is especially popular for visualizing high-dimensional data. As an example, the iris data set is visualized below using UMAP. The input data contain all four features of the iris data set, and the result is a two-dimensional representation of the data. The points are colored by the species of the iris, and as can be observed, the method found a decent clustering of the species.\n\niris_umap &lt;- iris |&gt;\n  uwot::umap(n_neighbors = 20,\n             min_dist = 0.2,\n             metric = \"euclidean\") |&gt;\n  as.data.frame() |&gt;\n  as_tibble() |&gt;\n  bind_cols(iris |&gt; select(Species))\n\niris_umap |&gt; \n  ggplot(aes(x = V1,\n             y = V2,\n             col = Species)) +\n  geom_point(size = 3) +\n  theme(text=element_text(size=13))\n\n\n\n\n\n\n\n\nThe following section includes more details on how UMAP works. As mentioned, the first step is to find the distance between all observations in the high-dimensional space (first step in Figure 13.1). The default distance metric is often Euclidean, but other metrics can be used.\n\n\n\n\n\n\nFigure 13.1: Visualisation of the UMAP algorithm\n\n\n\nFor each observation, place it in the origin of a two-dimensional space and plot the remaining observations along the x-axis based on their distance to the observation in the origin. An exponential decay function is fitted such that the sum of y-values of the K nearest observations is equal to \\(log2(K)\\), and the y-value of all other observations is approximately zero (second step in Figure 13.1). K is a hyperparameter which control the resolution of the clustering. A low value of K results in clusters capturing the details of the data as only the nearest few observations are considered. Opposite, a high value of K results in larger clusters that might lose some details, but better captures the overall structure of the data. The exponential function is on the form:\n\\[\nS = \\exp(-\\frac{d_{A,B}-d_{NN}}{\\sigma})\n\\]\nWhere \\(d_{A,B}\\) is the distance between observation A and B (the observation in the origin and the considered observation), \\(d_{NN}\\) is the distance to the nearest neighbor to the origin, and \\(\\sigma\\) is a parameter that is adjusted such that the sum of the y-values (\\(S\\)) of the K nearest observations are equal to \\(\\log_2(K)\\). The value of \\(\\sigma\\) is then likely to change for each repetition of placing a new observation in the origin. The score \\(S\\), which is also the y-value mentioned above, is then the similarity score between the two observations.\nA similarity score then exists for each pair of observations. However, the similarity score from observation A to B is not necessarily the same as from B to A. This ambiguity occurs if e.g. observation C is actually closer to B, but on the opposite side of A on the x-axis. Then, from the perspective of A, B is the nearest observation, but from the perspective of B, C is the nearest observation. To make the similarity scores symmetric, the following formula is used:\n\\[\nS_{sym} = (S_{A,B} + S_{B,A}) - S_{A,B}S_{B,A}\n\\]\nAs mentioned, placing a point in the origin and calculating the similarity score by fitting the exponential is done for all observations. Afterwards, a symmetric similarity score exists for each pair of points in the data set.\nA low dimensional representation of the data is initialized (third step in Figure 13.1). Using the clusters from the K nearest neighbors, two observations are chosen at random in a random cluster. Two observations with high similarity scores are prioritized in the otherwise random selection. One of the two observations, also chosen at random, is to be moved towards the other. Before moving, an additional observation is chosen at random from a different cluster. The observation to be moved is moved closer to the other observation in the same cluster and further away from the additional observation, e.g. moving observation C towards observation B and away from observation E in Figure 13.1. The distance to move the observation is determined by calculating a similarity score in the low dimensional manifold by fitting two curves (represented by the red and greed curve in Figure 13.1). The formula for the curve is:\n\\[\nS_{low} = \\frac{1}{1 + \\alpha d_{low}^{2\\beta}}\n\\]\nWhere \\(d_{low}\\) is the distance between the two observations in the low dimensional space, and \\(\\alpha\\) and \\(\\beta\\) are hyperparameters that control the shape of the distribution. If the two observations are on top of each other, the distance is zero and the similarity score is one. The similarity score is calculated twice, once for the pair of observations in the same cluster, and once for the observation to be moved and the additional observation. This becomes an optimization problem where the observation is moved towards the maximum of the distribution for the other observation in the pair, and away from the maximum of the distribution for the additional observation. The cost function to be minimized is:\n\\[\nC(S_{pair}, S_{add}) = \\log\\left(\\frac{1}{S_{pair}}\\right) + \\log\\left(\\frac{1}{1 - S_{add}}\\right)\n\\]\nWhere \\(S_{pair}\\) is the similarity score for the pair of observations, and \\(S_{add}\\) is the similarity score for the observation to be moved and the additional observation. When moving the point in the wrong direction, or too far in the correct direction, the value of the cost function starts to increase again. The observation is moved in the direction of the negative derivative of the cost function one step at a time, a process called stochastic gradient descent. This process is repeated until the low dimensional representation is stable, and the final representation is the output of the UMAP algorithm (fourth step in Figure 13.1).\nThe UMAP algorithm is applied to the count matrix with clr transformed values. The data is visualized in a two-dimensional space and colored by the patient status. No apparent clustering is observed.\n\ncount_matrix_umap &lt;- count_matrix_clr |&gt;\n  column_to_rownames(var = \"Sample\") |&gt;\n  uwot::umap(n_neighbors = 4,\n             min_dist = 0.01,\n             n_components = 2,\n             metric = \"euclidean\") |&gt;\n  as.data.frame() |&gt;\n  rownames_to_column(var = \"Sample\") |&gt;\n  left_join(meta,\n            by = \"Sample\")\n\ncount_matrix_umap |&gt; \n  ggplot(aes(x = V1,\n             y = V2,\n             col = Patient_status)) +\n  geom_point(size = 3) +\n  theme(text=element_text(size=13))\n\n\n\n\n\n\n\n\n\n14 Session Info\n\nsessioninfo::session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.3 (2024-02-29 ucrt)\n os       Windows 11 x64 (build 22631)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_United Kingdom.utf8\n ctype    English_United Kingdom.utf8\n tz       Europe/Copenhagen\n date     2024-05-30\n pandoc   3.1.11 @ C:/Program Files/RStudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package      * version    date (UTC) lib source\n backports      1.4.1      2021-12-13 [1] CRAN (R 4.3.1)\n broom        * 1.0.5      2023-06-09 [1] CRAN (R 4.3.3)\n cachem         1.0.8      2023-05-01 [1] CRAN (R 4.3.3)\n class          7.3-22     2023-05-03 [2] CRAN (R 4.3.3)\n cli            3.6.2      2023-12-11 [1] CRAN (R 4.3.3)\n codetools      0.2-19     2023-02-01 [2] CRAN (R 4.3.3)\n colorspace     2.1-0      2023-01-23 [1] CRAN (R 4.3.3)\n conflicted     1.2.0      2023-02-01 [1] CRAN (R 4.3.3)\n data.table     1.15.4     2024-03-30 [1] CRAN (R 4.3.3)\n dials        * 1.2.1      2024-02-22 [1] CRAN (R 4.3.3)\n DiceDesign     1.10       2023-12-07 [1] CRAN (R 4.3.3)\n digest         0.6.35     2024-03-11 [1] CRAN (R 4.3.3)\n dplyr        * 1.1.4      2023-11-17 [1] CRAN (R 4.3.2)\n evaluate       0.23       2023-11-01 [1] CRAN (R 4.3.3)\n fansi          1.0.6      2023-12-08 [1] CRAN (R 4.3.3)\n farver         2.1.1      2022-07-06 [1] CRAN (R 4.3.3)\n fastmap        1.1.1      2023-02-24 [1] CRAN (R 4.3.3)\n FNN            1.1.4      2024-01-12 [1] CRAN (R 4.3.3)\n foreach        1.5.2      2022-02-02 [1] CRAN (R 4.3.3)\n furrr          0.3.1      2022-08-15 [1] CRAN (R 4.3.3)\n future         1.33.2     2024-03-26 [1] CRAN (R 4.3.3)\n future.apply   1.11.2     2024-03-28 [1] CRAN (R 4.3.3)\n generics       0.1.3      2022-07-05 [1] CRAN (R 4.3.3)\n ggplot2      * 3.5.1      2024-04-23 [1] CRAN (R 4.3.3)\n globals        0.16.3     2024-03-08 [1] CRAN (R 4.3.3)\n glue           1.7.0      2024-01-09 [1] CRAN (R 4.3.3)\n gower          1.0.1      2022-12-22 [1] CRAN (R 4.3.1)\n GPfit          1.0-8      2019-02-08 [1] CRAN (R 4.3.3)\n gtable         0.3.5      2024-04-22 [1] CRAN (R 4.3.3)\n hardhat        1.3.1      2024-02-02 [1] CRAN (R 4.3.3)\n hms            1.1.3      2023-03-21 [1] CRAN (R 4.3.3)\n htmltools      0.5.8.1    2024-04-04 [1] CRAN (R 4.3.3)\n htmlwidgets    1.6.4      2023-12-06 [1] CRAN (R 4.3.3)\n infer        * 1.0.7      2024-03-25 [1] CRAN (R 4.3.3)\n ipred          0.9-14     2023-03-09 [1] CRAN (R 4.3.3)\n irlba          2.3.5.1    2022-10-03 [1] CRAN (R 4.3.3)\n iterators      1.0.14     2022-02-05 [1] CRAN (R 4.3.3)\n jsonlite       1.8.8      2023-12-04 [1] CRAN (R 4.3.3)\n knitr          1.46       2024-04-06 [1] CRAN (R 4.3.3)\n labeling       0.4.3      2023-08-29 [1] CRAN (R 4.3.1)\n lattice        0.22-5     2023-10-24 [2] CRAN (R 4.3.3)\n lava           1.8.0      2024-03-05 [1] CRAN (R 4.3.3)\n lhs            1.1.6      2022-12-17 [1] CRAN (R 4.3.3)\n lifecycle      1.0.4      2023-11-07 [1] CRAN (R 4.3.3)\n listenv        0.9.1      2024-01-29 [1] CRAN (R 4.3.3)\n lubridate      1.9.3      2023-09-27 [1] CRAN (R 4.3.3)\n magrittr       2.0.3      2022-03-30 [1] CRAN (R 4.3.3)\n MASS           7.3-60.0.1 2024-01-13 [2] CRAN (R 4.3.3)\n Matrix       * 1.6-5      2024-01-11 [2] CRAN (R 4.3.3)\n memoise        2.0.1      2021-11-26 [1] CRAN (R 4.3.3)\n modeldata    * 1.3.0      2024-01-21 [1] CRAN (R 4.3.3)\n munsell        0.5.1      2024-04-01 [1] CRAN (R 4.3.3)\n nnet           7.3-19     2023-05-03 [2] CRAN (R 4.3.3)\n parallelly     1.37.1     2024-02-29 [1] CRAN (R 4.3.3)\n parsnip      * 1.2.1      2024-03-22 [1] CRAN (R 4.3.3)\n pillar         1.9.0      2023-03-22 [1] CRAN (R 4.3.3)\n pkgconfig      2.0.3      2019-09-22 [1] CRAN (R 4.3.3)\n prodlim        2023.08.28 2023-08-28 [1] CRAN (R 4.3.3)\n purrr        * 1.0.2      2023-08-10 [1] CRAN (R 4.3.3)\n R6             2.5.1      2021-08-19 [1] CRAN (R 4.3.3)\n Rcpp           1.0.12     2024-01-09 [1] CRAN (R 4.3.3)\n readr          2.1.5      2024-01-10 [1] CRAN (R 4.3.3)\n recipes      * 1.0.10     2024-02-18 [1] CRAN (R 4.3.3)\n rlang          1.1.3      2024-01-10 [1] CRAN (R 4.3.3)\n rmarkdown      2.26       2024-03-05 [1] CRAN (R 4.3.3)\n rpart          4.1.23     2023-12-05 [2] CRAN (R 4.3.3)\n rsample      * 1.2.1      2024-03-25 [1] CRAN (R 4.3.3)\n rstudioapi     0.16.0     2024-03-24 [1] CRAN (R 4.3.3)\n scales       * 1.3.0      2023-11-28 [1] CRAN (R 4.3.3)\n sessioninfo    1.2.2      2021-12-06 [1] CRAN (R 4.3.3)\n survival       3.5-8      2024-02-14 [2] CRAN (R 4.3.3)\n tibble       * 3.2.1      2023-03-20 [1] CRAN (R 4.3.3)\n tidymodels   * 1.2.0      2024-03-25 [1] CRAN (R 4.3.3)\n tidyr        * 1.3.1      2024-01-24 [1] CRAN (R 4.3.3)\n tidyselect     1.2.1      2024-03-11 [1] CRAN (R 4.3.3)\n timechange     0.3.0      2024-01-18 [1] CRAN (R 4.3.3)\n timeDate       4032.109   2023-12-14 [1] CRAN (R 4.3.2)\n tune         * 1.2.1      2024-04-18 [1] CRAN (R 4.3.3)\n tzdb           0.4.0      2023-05-12 [1] CRAN (R 4.3.3)\n utf8           1.2.4      2023-10-22 [1] CRAN (R 4.3.3)\n uwot         * 0.2.2      2024-04-21 [1] CRAN (R 4.3.3)\n vctrs          0.6.5      2023-12-01 [1] CRAN (R 4.3.3)\n withr          3.0.0      2024-01-16 [1] CRAN (R 4.3.3)\n workflows    * 1.1.4      2024-02-19 [1] CRAN (R 4.3.3)\n workflowsets * 1.1.0      2024-03-21 [1] CRAN (R 4.3.3)\n xfun           0.43       2024-03-25 [1] CRAN (R 4.3.3)\n yaml           2.3.8      2023-12-11 [1] CRAN (R 4.3.2)\n yardstick    * 1.3.1      2024-03-21 [1] CRAN (R 4.3.3)\n\n [1] C:/Users/Willi/AppData/Local/R/win-library/4.3\n [2] C:/Program Files/R/R-4.3.3/library\n\n──────────────────────────────────────────────────────────────────────────────",
    "crumbs": [
      "Preprocessing of Data",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Uniform Manifold Approximation and Projection</span>"
    ]
  },
  {
    "objectID": "apply00_intro.html",
    "href": "apply00_intro.html",
    "title": "Modeling of Microbiome Data",
    "section": "",
    "text": "In the first chapter, the concepts of tidymodeling were introduced. In the second chapter, the statistical concepts that are often used in Bioinformatics and Biostatistics were introduced. The third chapter touched upon different preprocessing techniques to apply. This chapter combines the first three chapters to build and evaluate models for a real-world dataset. As mentioned in the preface, the dataset used is 16S rRNA amplicon reads used to investigate the microbiome of MS patients (Cox et al. 2021). Here, the data is used to predict the disease status an individual given the microbiome.\n\n\n\n\nCox, Laura M., Amir Hadi Maghzi, Shirong Liu, Stephanie K. Tankou, Fyonn H. Dhang, Valerie Willocq, Anya Song, et al. 2021. “Gut Microbiome in Progressive Multiple Sclerosis.” Annals of Neurology 89 (June): 1195–1211. https://doi.org/10.1002/ANA.26084.",
    "crumbs": [
      "Modeling of Microbiome Data"
    ]
  },
  {
    "objectID": "apply01_model_diversity.html",
    "href": "apply01_model_diversity.html",
    "title": "14  Diversity Measures",
    "section": "",
    "text": "14.1 Species Richness\nAs mentioned previously, Richness is the number of unique organisms in a sample. Here, the number of unique genera in each sample is calculated and stratified by disease severity. The violin plot shows the distribution of the richness values for each disease severity.\nBy visual inspection, it would seem that patients with MS, either relapsing-remitting or progressive, have a higher richness compared to healthy controls. Interestingly, a sample seem to have a richness of 0. This could indicate that either something went wrong in the sequencing process or in the subsequent data processing moving from raw sequence reads to a count matrix.\nrichness &lt;- count_matrix |&gt;\n  column_to_rownames(var = \"Sample\") |&gt; \n  specnumber() |&gt;\n  as_tibble(rownames = \"Sample\") |&gt;\n  rename(Richness = value) |&gt; \n  left_join(meta,\n            by = \"Sample\") |&gt;\n  group_by(Disease_severity) |&gt;\n  mutate(count = n()) |&gt;\n  mutate(Disease_severity_n = stringr::str_c(Disease_severity, \"\\n\", \"n=\", count)) |&gt;\n  ungroup()\n\nrichness |&gt;\n  ggplot(aes(x = Disease_severity_n,\n             y = Richness,\n             fill = Disease_severity)) +\n  geom_violin() +\n  geom_boxplot(width = 0.1) +\n  labs(title = \"Richness by Disease Severity\",\n       x = \"Disease Severity\",\n       y = \"Richness\") +\n  theme(legend.position = \"none\") +\n  theme(text=element_text(size=13))\nTo test if the difference in richness between the groups is statistically significant, a Wilcoxon rank sum test is performed. The test is performed for each combination of the three groups. The p-values are adjusted for multiple testing using Bonferroni correction.\nTo calculate the p-values, the combinations of the disease severities are found:\ncombinations &lt;- combn(x = richness |&gt;\n                            distinct(Disease_severity) |&gt;\n                            pull(Disease_severity),\n                      m = 2,\n                      simplify = FALSE)\ncombinations\n\n[[1]]\n[1] \"2RelapsingRemittingMS\" \"1HealthyControl\"      \n\n[[2]]\n[1] \"2RelapsingRemittingMS\" \"3ProgressiveMS\"       \n\n[[3]]\n[1] \"1HealthyControl\" \"3ProgressiveMS\"\nThe p-values are calculated via the function pairwise_wilcox_test() which can be found in the helper function section in the beginning of the document. For each of the combinations, the input data is subsetted to only include the samples in the combination. Then, a Wilcoxon rank sum test is performed. The significance level is per default 0.05, and the significance level can be adjusted with Bonferroni. As can be seen from the results, there is a significant difference in the richness between the healthy controls and each of the MS groups.\nrichness |&gt;\n  pairwise_wilcox_test(combinations = combinations,\n                       metric_col_name = \"Richness\",\n                       sig_level = 0.05,\n                       bonferroni = TRUE)\n\n# A tibble: 3 × 3\n  Combination                              p_value Significant\n  &lt;chr&gt;                                      &lt;dbl&gt; &lt;chr&gt;      \n1 2RelapsingRemittingMS vs 1HealthyControl 0.00250 Yes        \n2 2RelapsingRemittingMS vs 3ProgressiveMS  0.994   No         \n3 1HealthyControl vs 3ProgressiveMS        0.0124  Yes",
    "crumbs": [
      "Modeling of Microbiome Data",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Diversity Measures</span>"
    ]
  },
  {
    "objectID": "apply01_model_diversity.html#shannon-index",
    "href": "apply01_model_diversity.html#shannon-index",
    "title": "14  Diversity Measures",
    "section": "14.2 Shannon Index",
    "text": "14.2 Shannon Index\nBoth the richness and the evenness of a sample is considered when calculating the Shannon Index. If a group has a generally high Shannon Index, it means that the group has a high number of unique genera and that they are evenly distributed. The violin plot shows the distribution of the Shannon Index values for each disease severity.\nSimilarly to the richness, it would seem that patients with MS, either relapsing-remitting or progressive, have a higher Shannon Index compared to healthy controls, i.e. more unique taxa and more evenly distributed taxa.\n\nshannon &lt;- count_matrix |&gt;\n  column_to_rownames(var = \"Sample\") |&gt;\n  diversity(index = \"shannon\") |&gt; \n  as_tibble(rownames = \"Sample\") |&gt; \n  rename(Shannon = value) |&gt; \n  left_join(meta,\n            by = \"Sample\") |&gt;\n  group_by(Disease_severity) |&gt;\n  mutate(count = n()) |&gt;\n  mutate(Disease_severity_n = stringr::str_c(Disease_severity, \"\\n\", \"n=\", count)) |&gt;\n  ungroup()\n\nshannon |&gt;\n  ggplot(aes(x = Disease_severity_n,\n             y = Shannon,\n             fill = Disease_severity)) +\n  geom_violin() +\n  geom_boxplot(width = 0.1) +\n  labs(title = \"Shannon Index by Disease Severity\",\n       x = \"Disease Severity\",\n       y = \"Shannon Index\") +\n  theme(legend.position = \"none\") +\n  theme(text=element_text(size=13))\n\n\n\n\n\n\n\n\nAgain, p-values are calculated via the function pairwise_wilcox_test() which can be found in the helper function section in the beginning of the document. As can be seen from the results, there is a significant difference in the Shannon Index between the healthy controls and each of the MS groups.\n\nshannon |&gt;\n  pairwise_wilcox_test(combinations = combinations,\n                       metric_col_name = \"Shannon\",\n                       sig_level = 0.05,\n                       bonferroni = TRUE)\n\n# A tibble: 3 × 3\n  Combination                              p_value Significant\n  &lt;chr&gt;                                      &lt;dbl&gt; &lt;chr&gt;      \n1 2RelapsingRemittingMS vs 1HealthyControl 0.00715 Yes        \n2 2RelapsingRemittingMS vs 3ProgressiveMS  0.870   No         \n3 1HealthyControl vs 3ProgressiveMS        0.0113  Yes",
    "crumbs": [
      "Modeling of Microbiome Data",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Diversity Measures</span>"
    ]
  },
  {
    "objectID": "apply01_model_diversity.html#pielou-index-evenness",
    "href": "apply01_model_diversity.html#pielou-index-evenness",
    "title": "14  Diversity Measures",
    "section": "14.3 Pielou Index (Evenness)",
    "text": "14.3 Pielou Index (Evenness)\nEven though richness is needed to calculated the Shannon Index which in turn is needed to calculate the Pielou Index, the richness is not of importance. The independence comes from the division in the formula as the Pielou Index compares the Shannon Index of the sample to Shannon Index of a perfectly even sample of the same size. Then, the Pielou Index is a measure of how evenly the species (here genera) are distributed in a sample.\nOpposite richness and Shannon Index, Pielou Index seems to be lowest for the relapsing-remitting MS group whereas the healthy controls and progressive MS have similar values. This could indicate that the organisms in the relapsing-remitting MS group are not as evenly distributed as in the other groups. Further, this also indicate that the higher Shannon Index in the relapsing-remitting group is due to a higher number of unique organisms and not due to a more even distribution of the organisms.\n\npielou &lt;- shannon |&gt;\n  mutate(Pielou = Shannon / log(count))\n\npielou |&gt;\n  ggplot(aes(x = Disease_severity_n,\n             y = Pielou,\n             fill = Disease_severity)) +\n  geom_violin() +\n  geom_boxplot(width = 0.1) +\n  labs(title = \"Pielou Index by Disease Severity\",\n       x = \"Disease Severity\",\n       y = \"Pielou Index\") +\n  theme(legend.position = \"none\") +\n  lims(y = c(0, 1)) +\n  theme(text=element_text(size=13))\n\n\n\n\n\n\n\n\nAgain, p-values are calculated via the function pairwise_wilcox_test() which can be found in the helper function section in the beginning of the document. As can be seen from the results, there is a significant difference in the Pielou Index between the relapsing-remitting group vs both of the other groups.\n\npielou |&gt;\n  pairwise_wilcox_test(combinations = combinations,\n                       metric_col_name = \"Pielou\",\n                       sig_level = 0.05,\n                       bonferroni = TRUE)\n\n# A tibble: 3 × 3\n  Combination                               p_value Significant\n  &lt;chr&gt;                                       &lt;dbl&gt; &lt;chr&gt;      \n1 2RelapsingRemittingMS vs 1HealthyControl 3.10e-23 Yes        \n2 2RelapsingRemittingMS vs 3ProgressiveMS  3.12e-35 Yes        \n3 1HealthyControl vs 3ProgressiveMS        5.15e- 2 No",
    "crumbs": [
      "Modeling of Microbiome Data",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Diversity Measures</span>"
    ]
  },
  {
    "objectID": "apply02_model_lin.html",
    "href": "apply02_model_lin.html",
    "title": "15  Linear Modelling",
    "section": "",
    "text": "15.1 Data Wrangling\nThe data is joined with the metadata as the Patient_status is used as the response. Only the columns that are needed for the model are kept. Notice inner_join() is used such that only rows for which both the count matrix and metadata are available are kept.\ncount_matrix_clr &lt;- count_matrix_clr |&gt;\n  inner_join(meta,\n             by = \"Sample\") |&gt;\n  select(-c(Sample, Treatment, Ethnicity, Location,\n            Age, BMI, Disease_severity, EDSS, Sex)) |&gt;\n  relocate(Patient_status)",
    "crumbs": [
      "Modeling of Microbiome Data",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Linear Modelling</span>"
    ]
  },
  {
    "objectID": "apply02_model_lin.html#cross-validation",
    "href": "apply02_model_lin.html#cross-validation",
    "title": "15  Linear Modelling",
    "section": "15.2 Cross-Validation",
    "text": "15.2 Cross-Validation\nThe data is split into a training and testing set using cross-validation to avoid overfitting. This is especially needed as hyperparameters are to be tuned.\nThe data is split:\n\ncount_matrix_clr_split &lt;- initial_split(count_matrix_clr,\n                                        prop = params$percentage_train,\n                                        strata = Patient_status)\ncount_matrix_clr_train &lt;- training(count_matrix_clr_split)\ncount_matrix_clr_test &lt;- testing(count_matrix_clr_split)\n\nThe CV object is created:\n\ncount_matrix_clr_folds &lt;- vfold_cv(count_matrix_clr_train,\n                                   v = params$n_folds)",
    "crumbs": [
      "Modeling of Microbiome Data",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Linear Modelling</span>"
    ]
  },
  {
    "objectID": "apply02_model_lin.html#recipes",
    "href": "apply02_model_lin.html#recipes",
    "title": "15  Linear Modelling",
    "section": "15.3 Recipes",
    "text": "15.3 Recipes\nThe recipes contains two pieces of important information. The first is the formula that describes the relationship between the features and the response. In this case, which genera to use for predicting disease. The second is the pre-processing steps that are applied to the data before the model is built. This could include steps such as scaling, log-transform or imputing missing values. Here, the data is already clr-transformed.\nTwo recipes are created. One without any pre-processing steps and one where PCA is applied for dimensionality reduction. In both cases, all features are used to try and predict the response Patient_status.\n\nstand_recipe &lt;- recipe(Patient_status ~ .,\n                       data = count_matrix_clr_train)\n\npca_recipe &lt;- stand_recipe |&gt; \n  step_pca(all_predictors(),\n           num_comp = 7)",
    "crumbs": [
      "Modeling of Microbiome Data",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Linear Modelling</span>"
    ]
  },
  {
    "objectID": "apply02_model_lin.html#model-specifications",
    "href": "apply02_model_lin.html#model-specifications",
    "title": "15  Linear Modelling",
    "section": "15.4 Model Specifications",
    "text": "15.4 Model Specifications\nTo model the data, a specification is needed. This includes the model to use, the engine to use, mode of prediction and choosing hyperparameters. Since the response is binary, a logistic regression model is used. The model is later tuned using a grid search over the hyperparameters penalty and mixture. Models with exclusively Ridge and Lasso regularization are built, as well as a model with a mixture of both.\n\nlog_spec &lt;- logistic_reg(penalty = tune(),\n                         mixture = tune()) |&gt;\n  set_engine(\"glmnet\") |&gt;\n  set_mode(\"classification\")\n\nlog_ridge_spec &lt;- logistic_reg(penalty = tune(),\n                               mixture = 0) |&gt;\n  set_engine(\"glmnet\") |&gt;\n  set_mode(\"classification\")\n\nlog_lasso_spec &lt;- logistic_reg(penalty = tune(),\n                               mixture = 1) |&gt;\n  set_engine(\"glmnet\") |&gt;\n  set_mode(\"classification\")",
    "crumbs": [
      "Modeling of Microbiome Data",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Linear Modelling</span>"
    ]
  },
  {
    "objectID": "apply02_model_lin.html#change-hyperparameter-values",
    "href": "apply02_model_lin.html#change-hyperparameter-values",
    "title": "15  Linear Modelling",
    "section": "15.5 Change Hyperparameter Values",
    "text": "15.5 Change Hyperparameter Values\nThe hyperparameters penalty and mixture are extracted from the model specification to see the possible values that can be used in the grid search.\nFor the mixture, values between \\(0.05\\) and \\(1\\) is used which seem to be reasonable values.\n\nlog_spec |&gt; \n  extract_parameter_set_dials() |&gt; \n  extract_parameter_dials(\"mixture\")\n\nProportion of Lasso Penalty (quantitative)\nRange: [0.05, 1]\n\n\nFor the penalty, values between \\(10^{-10}\\) and \\(1\\) are used per default. The range is increased to \\(5\\cdot10^1\\). The updated range is saved in a new object, but the information is not added to the models until the workflow is created further down. As seen from the output, the information do not pertain to a specific model, but simply contain ranges for different hyperparameters.\n\nlog_param_ranges &lt;- log_spec |&gt; \n  extract_parameter_set_dials() |&gt;\n  update(penalty = penalty(c(params$penalty_lower,\n                             params$penalty_higher)))\n\nlog_penalty_param_ranges &lt;- log_lasso_spec |&gt; \n  extract_parameter_set_dials() |&gt;\n  update(penalty = penalty(c(params$penalty_lower,\n                             params$penalty_higher)))\n\nlog_param_ranges\n\nCollection of 2 parameters for tuning\n\n identifier    type    object\n    penalty penalty nparam[+]\n    mixture mixture nparam[+]\n\nlog_penalty_param_ranges\n\nCollection of 1 parameters for tuning\n\n identifier    type    object\n    penalty penalty nparam[+]",
    "crumbs": [
      "Modeling of Microbiome Data",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Linear Modelling</span>"
    ]
  },
  {
    "objectID": "apply02_model_lin.html#create-the-workflow-set",
    "href": "apply02_model_lin.html#create-the-workflow-set",
    "title": "15  Linear Modelling",
    "section": "15.6 Create the Workflow Set",
    "text": "15.6 Create the Workflow Set\nThe recipes are combined with the model specifications to create a workflow set. The workflow set is used to fit the models to the data and evaluate the models. Notice it is called a workflow set, as it contains multiple workflows. Each row of the workflow set is a workflow of its own which further contain the recipe and model.\nA workflow set for all combinations of recipes and models is created:\n\nworkflow_set &lt;- workflow_set(\n  preproc = list(stand = stand_recipe,\n                 pca = pca_recipe),\n  models = list(log = log_spec,\n                log_ridge = log_ridge_spec,\n                log_lasso = log_lasso_spec)\n)\n\nworkflow_set\n\n# A workflow set/tibble: 6 × 4\n  wflow_id        info             option    result    \n  &lt;chr&gt;           &lt;list&gt;           &lt;list&gt;    &lt;list&gt;    \n1 stand_log       &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n2 stand_log_ridge &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n3 stand_log_lasso &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n4 pca_log         &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n5 pca_log_ridge   &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n6 pca_log_lasso   &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n\n\nThe parameter objects which contain the hyperparameter ranges are added to the workflow (note the option column is changed):\n\nworkflow_set &lt;- workflow_set |&gt;\n  option_add(id = \"stand_log\",\n             param_info = log_param_ranges) |&gt;\n  option_add(id = \"pca_log\",\n             param_info = log_param_ranges) |&gt;\n  option_add(id = \"stand_log_ridge\",\n             param_info = log_penalty_param_ranges) |&gt;\n  option_add(id = \"stand_log_lasso\",\n             param_info = log_penalty_param_ranges) |&gt;\n  option_add(id = \"pca_log_ridge\",\n             param_info = log_penalty_param_ranges) |&gt;\n  option_add(id = \"pca_log_lasso\",\n             param_info = log_penalty_param_ranges)\nworkflow_set\n\n# A workflow set/tibble: 6 × 4\n  wflow_id        info             option    result    \n  &lt;chr&gt;           &lt;list&gt;           &lt;list&gt;    &lt;list&gt;    \n1 stand_log       &lt;tibble [1 × 4]&gt; &lt;opts[1]&gt; &lt;list [0]&gt;\n2 stand_log_ridge &lt;tibble [1 × 4]&gt; &lt;opts[1]&gt; &lt;list [0]&gt;\n3 stand_log_lasso &lt;tibble [1 × 4]&gt; &lt;opts[1]&gt; &lt;list [0]&gt;\n4 pca_log         &lt;tibble [1 × 4]&gt; &lt;opts[1]&gt; &lt;list [0]&gt;\n5 pca_log_ridge   &lt;tibble [1 × 4]&gt; &lt;opts[1]&gt; &lt;list [0]&gt;\n6 pca_log_lasso   &lt;tibble [1 × 4]&gt; &lt;opts[1]&gt; &lt;list [0]&gt;",
    "crumbs": [
      "Modeling of Microbiome Data",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Linear Modelling</span>"
    ]
  },
  {
    "objectID": "apply02_model_lin.html#tune-hyperparameters",
    "href": "apply02_model_lin.html#tune-hyperparameters",
    "title": "15  Linear Modelling",
    "section": "15.7 Tune Hyperparameters",
    "text": "15.7 Tune Hyperparameters\nAll workflows in the workflow set contains hyperparameters. These are tuned with a search grid using the function tune_grid(). As workflow_set contain multiple workflows, the tune_grid() function can be mapped over the workflows to tune all hyperparameters using workflow_map(), a purrr-like map function.\nSeveral settings for tuning the grid exists. This includes e.g. whether or not to parallelize, what output to save and how verbose the output should be, i.e. how much should be printed to std.out. The settings are set to:\n\ngrid_settings &lt;-\n   control_grid(\n      save_pred = TRUE,\n      parallel_over = \"everything\",\n      save_workflow = TRUE,\n      extract = function(x) x\n   )\n\nThe grid search is performed for a grid of size 100, which means 100 different combinations of the hyperparameters are tried for each model.\n\ngrid_results &lt;- workflow_set |&gt;\n  workflow_map(\n    fn = \"tune_grid\",\n    seed = 1337,\n    resamples = count_matrix_clr_folds,\n    grid = params$grid_size,\n    control = grid_settings\n  )\n\ntidymodels comes with a lot of convenience functions. One of these is autoplot() that can, among other things, plot the results of the grid search. The best result of each workflow is selected based on the AUC.\n\nautoplot(\n  grid_results,\n  rank_metric = \"roc_auc\",\n  metric = \"roc_auc\",\n  type = \"wflow_id\"\n) +\n  geom_point(aes(y = mean),\n             size = 2) +\n  lims(y = c(0.4, 1)) +\n  theme(legend.position = \"none\") +\n  facet_wrap(~ wflow_id,\n             ncol = 2) +\n  theme(text=element_text(size=13))\n\n\n\n\n\n\n\n\nThe issue with wrappers, e.g. the convenience function autoplot(), is that they do not always provide the flexibility needed. A cleaner plot is created by extracting the data and manually plotting it. The plot shows the AUC for each workflow, with error bars showing the standard error.\n\ngrid_results |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"roc_auc\") |&gt; \n  group_by(wflow_id) |&gt;\n  arrange(desc(mean)) |&gt;\n  mutate(rank = row_number()) |&gt;\n  ungroup() |&gt;\n  ggplot(aes(x = rank,\n             y = mean,\n             col = wflow_id)) +\n  geom_point(size = 2) +\n  geom_errorbar(aes(ymin = mean - std_err,\n                    ymax = mean + std_err)) +\n  labs(x = \"Rank\",\n       y = \"AUC\") +\n  facet_wrap(~ wflow_id,\n             ncol = 2,\n             scales = \"free_y\") +\n  lims(y = c(0.4, 1)) +\n  theme(legend.position = \"none\",\n        text=element_text(size=13))\n\n\n\n\n\n\n\n\nA couple of interesting insights can be gained from looking at the optimal hyperparameters (see table below). When comparing the two ENet models (stand_log vs pca_log), the mixture is higher for pca_log, i.e. preferring Lasso over Ridge. This is opposite expectations as PCA should reduce the number of features, which in turn should reduce the need for Lasso regularization. However, penalty is several orders of magnitude lower for pca_log meaning the Lasso regularization is less strict. When comparing the full Ridge and Lasso models between the two preprocessing steps, the penalty is lower when PCA preprocessing is performed. This is expected as PCA reduces the number of features reducing the need for regularization.\n\nworkflow_ids &lt;- grid_results |&gt;\n  pull(wflow_id)\n\n\ntuning_params_result &lt;- workflow_ids |&gt;\n  purrr::map(function(id) {\n    grid_results |&gt;\n      extract_workflow_set_result(id) |&gt;\n      select_best(metric = \"roc_auc\") |&gt; \n      mutate(wflow_id = id)\n    }) |&gt;\n  bind_rows() |&gt;\n  mutate(mixture = case_when(stringr::str_detect(string = wflow_id,\n                                                 pattern = \"lasso\") ~ 1,\n                             stringr::str_detect(string = wflow_id,\n                                                 pattern = \"ridge\") ~ 0,\n                             TRUE ~ mixture)) |&gt;\n  select(wflow_id, penalty, mixture)\n  \ntuning_params_result\n\n# A tibble: 6 × 3\n  wflow_id         penalty mixture\n  &lt;chr&gt;              &lt;dbl&gt;   &lt;dbl&gt;\n1 stand_log       1.24e- 1  0.0869\n2 stand_log_ridge 2.72e+ 0  0     \n3 stand_log_lasso 4.08e- 2  1     \n4 pca_log         2.24e- 6  0.356 \n5 pca_log_ridge   1.26e-10  0     \n6 pca_log_lasso   1.26e-10  1",
    "crumbs": [
      "Modeling of Microbiome Data",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Linear Modelling</span>"
    ]
  },
  {
    "objectID": "apply02_model_lin.html#finalize-workflow",
    "href": "apply02_model_lin.html#finalize-workflow",
    "title": "15  Linear Modelling",
    "section": "15.8 Finalize Workflow",
    "text": "15.8 Finalize Workflow\nAs the hyperparameters have been tuned, the final workflow can be created which is done by adding the hyperparameters to the workflow.\n\nworkflow_set_final &lt;- workflow_ids |&gt;\n  purrr::map(function(id) {\n    grid_results |&gt;\n      extract_workflow(id) |&gt; \n      finalize_workflow(tuning_params_result |&gt; \n                          filter(wflow_id == id))\n    })\nnames(workflow_set_final) &lt;- workflow_ids\n\nTo exemplify, the pca_log workflow is shown from before finalizing and after finalizing. As can be seen, the hyperparameters are added to the workflow such that they have a value instead of tune().\n\n# Before finalizing\ngrid_results |&gt;\n  extract_workflow(\"pca_log\")\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_pca()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = tune()\n  mixture = tune()\n\nComputational engine: glmnet \n\n# After finalizing\nworkflow_set_final$pca_log\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_pca()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = 2.24450165395719e-06\n  mixture = 0.35620289735531\n\nComputational engine: glmnet",
    "crumbs": [
      "Modeling of Microbiome Data",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Linear Modelling</span>"
    ]
  },
  {
    "objectID": "apply02_model_lin.html#fit-and-predict-for-the-final-workflow",
    "href": "apply02_model_lin.html#fit-and-predict-for-the-final-workflow",
    "title": "15  Linear Modelling",
    "section": "15.9 Fit and Predict for the Final Workflow",
    "text": "15.9 Fit and Predict for the Final Workflow\nWith the hyperparameters set, the workflows can be fitted to the data, and the models can be evaluated. The last_fit() function is used to fit the model to the data. The split argument is set to the cross-validation object created earlier fitting to the entire training data.\n\nworkflow_set_fit &lt;- workflow_ids |&gt;\n  purrr::map(function(id) {\n    workflow_set_final[[id]] |&gt;\n      last_fit(split = count_matrix_clr_split)\n  })\nnames(workflow_set_fit) &lt;- workflow_ids",
    "crumbs": [
      "Modeling of Microbiome Data",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Linear Modelling</span>"
    ]
  },
  {
    "objectID": "apply02_model_lin.html#visualize-performance",
    "href": "apply02_model_lin.html#visualize-performance",
    "title": "15  Linear Modelling",
    "section": "15.10 Visualize Performance",
    "text": "15.10 Visualize Performance\nDifferent options exist for visualizing the performance of the models. One option is to look at the confusion matrices. The example below shows the confusion matrix for the stand_log model. From the table it can be seen that the model always predict MS, regardless of the true status.\n\nworkflow_set_fit$stand_log |&gt;\n  collect_predictions()|&gt; \n  conf_mat(truth = Patient_status,\n           estimate = .pred_class)\n\n          Truth\nPrediction Healthy MS\n   Healthy       0  0\n   MS           10 82\n\n\nAnother more visual approach is to plot the ROC-curve for each model. First, the specificity and sensitivity is calculated for each model with a convenience function roc_curve():\n\nroc_auc_results &lt;- workflow_ids |&gt;\n  map(function(id) {\n    workflow_set_fit[[id]] |&gt;\n      collect_predictions() |&gt;\n      roc_curve(truth = Patient_status,\n                .pred_MS,\n                event_level = \"second\") |&gt;\n      mutate(wflow_id = id)\n  }) |&gt; bind_rows()\n\nroc_auc_results\n\n# A tibble: 564 × 4\n   .threshold specificity sensitivity wflow_id \n        &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;    \n 1   -Inf             0         1     stand_log\n 2      0.707         0         1     stand_log\n 3      0.771         0         0.988 stand_log\n 4      0.778         0         0.976 stand_log\n 5      0.797         0         0.963 stand_log\n 6      0.801         0         0.951 stand_log\n 7      0.803         0         0.939 stand_log\n 8      0.813         0         0.927 stand_log\n 9      0.824         0.1       0.927 stand_log\n10      0.832         0.2       0.927 stand_log\n# ℹ 554 more rows\n\n\nFinally, the ROC-curve is plotted stratified by the workflow ID. The ROC-curve is a plot of the sensitivity against 1 - specificity. Sensitivity is the proportion of true positives out of all positives, i.e. true positive rate. In this case, the proportion of correctly classified MS patients out of all MS patients. Specificity is the proportion of true negatives out of all negatives. In this case, the proportion of correctly classified healthy patients out of all healthy patients. Since the x-axis plots 1 - specificity, it is rather the proportion of false positives out of all negatives, i.e. the false positive rate (also called false discovery rate).\nDetermining the best model given the ROC-curve is not trivial as it depends on how many false positives one is willing to accept. If the model is to decide on a risky treatment, one might want to avoid false positives at all costs, e.g. a threshold of 0.01, which would choose stand_log_lasso as it has the highest sensitivity at that threshold. Increasing the threshold to 0.25 would instead choose stand_log.\n\nroc_auc_results |&gt;\n  ggplot(aes(x = 1 - specificity,\n             y = sensitivity,\n             col = wflow_id)) + \n  geom_path(lwd = 1,\n            alpha = 0.5) +\n  geom_abline(lty = 3) + \n  coord_equal() + \n  scale_color_viridis_d() +\n  theme(text=element_text(size=13))\n\n\n\n\n\n\n\n\nOtherwise, the general rule is to choose the model with the highest AUC since it is the model that performs best across all thresholds. stand_log_ridge performs slightly better than the other models.\n\nworkflow_ids |&gt;\n  purrr::map(function(id) {\n    grid_results |&gt;\n      extract_workflow_set_result(id) |&gt;\n      show_best(metric = \"roc_auc\",\n                n = 1) |&gt; \n      mutate(wflow_id = id)\n    }) |&gt;\n  bind_rows() |&gt;\n  ggplot(aes(x = wflow_id,\n             y = mean,\n             fill = wflow_id)) +\n  geom_col() +\n  geom_label(aes(label = round(mean, 3)),\n             position = position_stack(vjust = 0.9),\n             show.legend = FALSE) +\n  scale_y_continuous(expand = c(0, 0,\n                                0.01, 0.01)) +\n  labs(title = \"AUC\",\n       x = \"Workflow ID\",\n       y = \"AUC by Workflows\") +\n  theme(axis.text.x = element_blank(),\n        axis.ticks.x = element_blank(),\n        theme(text=element_text(size=13)))",
    "crumbs": [
      "Modeling of Microbiome Data",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Linear Modelling</span>"
    ]
  },
  {
    "objectID": "apply02_model_lin.html#fine-tuning-hyperparameters",
    "href": "apply02_model_lin.html#fine-tuning-hyperparameters",
    "title": "15  Linear Modelling",
    "section": "15.11 Fine Tuning Hyperparameters",
    "text": "15.11 Fine Tuning Hyperparameters\nThe hyperparameters were tuned using a grid search of size 100. The best result of each workflow is chosen solely on the best value of AUC. It is possible, that a slightly worse performing model is preferred due to it being simpler. Models with a larger penalty are simpler as the coefficients are closer to zero, or less coefficients are present. A “one standard error rule” can be used to choose the simplest model that is within one standard error of the best model.\n\n\nShow the code\n# All results\nridge_results &lt;- grid_results |&gt;\n  extract_workflow_set_result(\"stand_log_ridge\") |&gt;\n  show_best(metric = \"roc_auc\",\n            n = params$grid_size)\n\n# Numerically best\nn_best &lt;- ridge_results |&gt;\n  filter(mean == max(mean)) |&gt;\n  filter(penalty == min(penalty))\n\n# 1-std error rule best\none_std_err_best &lt;- grid_results |&gt;\n  extract_workflow_set_result(\"stand_log_ridge\") |&gt;\n  select_by_one_std_err(p = desc(penalty),\n                        metric = \"roc_auc\") |&gt;\n  left_join(ridge_results |&gt;\n              select(.config,\n                     mean),\n            by = \".config\")\n\nridge_results |&gt;\n  ggplot(aes(x = penalty,\n             y = mean)) +\n  geom_point(aes(col = \"Data Points\")) +\n  geom_point(aes(col = \"Numerical Best\"),\n             data = n_best) +\n  geom_point(aes(col = \"One std err Best\"),\n             data = one_std_err_best) +\n  geom_errorbar(aes(ymin = mean - std_err,\n                    ymax = mean + std_err),\n                col = \"steelblue\") +\n  scale_x_log10() +\n  scale_color_manual(values = c(\"Data Points\" = \"steelblue\",\n                                \"Numerical Best\" = \"red\",\n                                \"One std err Best\" = \"green\")) +\n  labs(title = \"Standard Preprocessing, Ridge\",\n       x = \"Penalty\",\n       y = \"AUC\") +\n  lims(y = c(0.4, 1)) +\n  theme(text=element_text(size=13))\n\n\n\n\n\n\n\n\n\nThe practice is applied to all the models by extracting, finalizing, fitting, predicting and then visualizing as done in the above.\n\n\nShow the code\n# Extract hyperparameters\ntuning_params_result_fine &lt;- workflow_ids |&gt;\n  purrr::map(function(id) {\n    grid_results |&gt;\n      extract_workflow_set_result(id) |&gt;\n      select_by_one_std_err(p = desc(penalty),\n                            metric = \"roc_auc\") |&gt; \n      mutate(wflow_id = id)\n    }) |&gt;\n  bind_rows() |&gt;\n  mutate(mixture = case_when(stringr::str_detect(string = wflow_id,\n                                                 pattern = \"lasso\") ~ 1,\n                             stringr::str_detect(string = wflow_id,\n                                                 pattern = \"ridge\") ~ 0,\n                             TRUE ~ mixture)) |&gt;\n  select(wflow_id, penalty, mixture)\n\n# Finalize\nworkflow_set_final_fine &lt;- workflow_ids |&gt;\n  purrr::map(function(id) {\n    grid_results |&gt;\n      extract_workflow(id) |&gt; \n      finalize_workflow(tuning_params_result_fine |&gt; \n                          filter(wflow_id == id))\n    })\nnames(workflow_set_final_fine) &lt;- workflow_ids\n\n# Fit\nworkflow_set_fit_fine &lt;- workflow_ids |&gt;\n  purrr::map(function(id) {\n    workflow_set_final_fine[[id]] |&gt;\n      last_fit(split = count_matrix_clr_split)\n  })\nnames(workflow_set_fit_fine) &lt;- workflow_ids\n\n# Predict\nroc_auc_results_fine &lt;- workflow_ids |&gt;\n  map(function(id) {\n    workflow_set_fit_fine[[id]] |&gt;\n      collect_predictions() |&gt;\n      roc_curve(truth = Patient_status,\n                .pred_MS,\n                event_level = \"second\") |&gt;\n      mutate(wflow_id = id)\n  }) |&gt; bind_rows()\n\n# Visualize performance\nroc_auc_results_fine |&gt;\n  ggplot(aes(x = 1 - specificity,\n             y = sensitivity,\n             col = wflow_id)) + \n  geom_path(lwd = 1,\n            alpha = 0.6) +\n  geom_abline(lty = 3) + \n  coord_equal() + \n  scale_color_viridis_d() +\n  theme(text=element_text(size=13))\n\n\n\n\n\n\n\n\n\nFinding the best performing model based on the AUC is nonsensical after applying the one standard error rule, as a worse performing model was deliberately chosen to increase simplicity of the model.\n\n\nShow the code\nworkflow_ids |&gt;\n  purrr::map(function(id) {\n    # Get hyper params of best\n    std_error_best &lt;- grid_results |&gt;\n      extract_workflow_set_result(id) |&gt;\n      select_by_one_std_err(p = desc(penalty),\n                            metric = \"roc_auc\") |&gt; \n      mutate(wflow_id = id)\n    \n    # Get AUC of best\n    auc &lt;- grid_results |&gt;\n      extract_workflow_set_result(id) |&gt;\n      show_best(metric = \"roc_auc\",\n                n = params$grid_size)\n    \n    # Get join keys (depend on model)\n    shared_col_names &lt;- intersect(colnames(std_error_best),\n                                  colnames(auc))\n    \n    std_error_best |&gt;\n      left_join(auc,\n                by = shared_col_names) |&gt;\n      select(wflow_id, mean)\n    }) |&gt; bind_rows() |&gt;\n  ggplot(aes(x = wflow_id,\n             y = mean,\n             fill = wflow_id)) +\n  geom_col() +\n  geom_label(aes(label = round(mean, 3)),\n             position = position_stack(vjust = 0.9),\n             show.legend = FALSE) +\n  scale_y_continuous(expand = c(0, 0)) +\n  labs(title = \"AUC\",\n       x = \"Workflow ID\",\n       y = \"AUC by Workflows\") +\n  theme(axis.text.x = element_blank(),\n        axis.ticks.x = element_blank()) +\n  theme(text=element_text(size=13))",
    "crumbs": [
      "Modeling of Microbiome Data",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Linear Modelling</span>"
    ]
  },
  {
    "objectID": "apply03_model_non_lin.html",
    "href": "apply03_model_non_lin.html",
    "title": "16  Non-Linear Modelling",
    "section": "",
    "text": "16.1 Workflow Set Preparation\nThe count matrix is joined with the metadata such that the patient status is known for each sample.\ncount_matrix_clr &lt;- count_matrix_clr |&gt;\n  inner_join(meta,\n             by = \"Sample\") |&gt;\n  select(-c(Sample, Treatment, Ethnicity, Location,\n            Age, BMI, Disease_severity, EDSS, Sex)) |&gt;\n  relocate(Patient_status)\nThe objects needed for cross-validation are created.\ncount_matrix_clr_split &lt;- initial_split(count_matrix_clr,\n                                        prop = params$percentage_train,\n                                        strata = Patient_status)\ncount_matrix_clr_train &lt;- training(count_matrix_clr_split)\ncount_matrix_clr_test &lt;- testing(count_matrix_clr_split)\n\ncount_matrix_clr_folds &lt;- vfold_cv(count_matrix_clr_train,\n                                   v = params$n_folds)\nAs for the linear model chapter, two recipes are created. One without any pre-processing steps, one with UMAP for feature selection.\nstand_recipe &lt;- recipe(Patient_status ~ .,\n                       data = count_matrix_clr_train)\n\numap_recipe &lt;- stand_recipe |&gt;\n  embed::step_umap(all_predictors(),\n                   num_comp = 7,\n                   neighbors = 4,\n                   min_dist = 0.01)\nThe model specification is created. The SVM model is used with a radial basis function kernel. The hyperparameters cost and rbf_sigma are tuned. cost is similar to \\(\\xi\\) from the chapter on SVM, i.e. how much observations are allowed to cross the margin thereby controlling the Bias-Variance trade off. Low values allows for misclassifications creating a simple model with high bias and low variance. Vice versa for high values which penalizes misclassifications. rbf_sigma scales the distance between points inversely. Low values decreases the distance between points in the high dimensions, high values increases the distance. When the distances are lower, the observations affect each other more, creating a more complex model with high variance and low bias.\nsvm_spec &lt;- svm_rbf(cost = tune(),\n                    rbf_sigma = tune()) |&gt;\n  set_engine(\"kernlab\") |&gt;\n  set_mode(\"classification\")\nThe ranges of the hyperparameters are updated.\nsvm_param_ranges &lt;- svm_spec |&gt; \n  extract_parameter_set_dials() |&gt;\n  update(rbf_sigma = rbf_sigma(c(params$rbf_sigma_lower,\n                                 params$rbf_sigma_higher)))\n\nsvm_param_ranges |&gt;\n  extract_parameter_dials(\"rbf_sigma\")\n\nRadial Basis Function sigma (quantitative)\nTransformer: log-10 [1e-100, Inf]\nRange (transformed scale): [-10, -4]\nThe workflow set is created by combining the recipes and specifications.\n# Create workflow set\nworkflow_set &lt;- workflow_set(\n  preproc = list(stand = stand_recipe,\n                 umap = umap_recipe),\n  models = list(svm = svm_spec)\n)\n\n# Update workflow set with hyperparameter ranges\nworkflow_set &lt;- workflow_set |&gt;\n  option_add(id = \"stand_svm\",\n             param_info = svm_param_ranges) |&gt;\n  option_add(id = \"umap_svm\",\n             param_info = svm_param_ranges)\nworkflow_set\n\n# A workflow set/tibble: 2 × 4\n  wflow_id  info             option    result    \n  &lt;chr&gt;     &lt;list&gt;           &lt;list&gt;    &lt;list&gt;    \n1 stand_svm &lt;tibble [1 × 4]&gt; &lt;opts[1]&gt; &lt;list [0]&gt;\n2 umap_svm  &lt;tibble [1 × 4]&gt; &lt;opts[1]&gt; &lt;list [0]&gt;",
    "crumbs": [
      "Modeling of Microbiome Data",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Non-Linear Modelling</span>"
    ]
  },
  {
    "objectID": "apply03_model_non_lin.html#tune-hyperparameters",
    "href": "apply03_model_non_lin.html#tune-hyperparameters",
    "title": "16  Non-Linear Modelling",
    "section": "16.2 Tune Hyperparameters",
    "text": "16.2 Tune Hyperparameters\nThe same grid search is performed as for the linear models. The grid search is performed for a grid of size 100.\n\n# Set grid search settings\ngrid_settings &lt;-\n  control_grid(\n    save_pred = TRUE,\n    parallel_over = \"everything\",\n    save_workflow = TRUE,\n    extract = function(x) x\n  )\n\n# Perform grid search\ngrid_results &lt;- workflow_set |&gt;\n  workflow_map(\n    fn = \"tune_grid\",\n    seed = 1337,\n    resamples = count_matrix_clr_folds,\n    grid = params$grid_size,\n    control = grid_settings\n  )\n\nPerformance across the grid search is visualized. From the search, it would seem the grid ranges should be increased further as the performance is not plateauing.\n\ngrid_results |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"roc_auc\") |&gt; \n  group_by(wflow_id) |&gt;\n  arrange(desc(mean)) |&gt;\n  mutate(rank = row_number()) |&gt;\n  ungroup() |&gt;\n  ggplot(aes(x = rank,\n             y = mean,\n             col = wflow_id)) +\n  geom_point(size = 2) +\n  geom_errorbar(aes(ymin = mean - std_err,\n                    ymax = mean + std_err)) +\n  theme(legend.position = \"none\") +\n  labs(x = \"Rank\",\n       y = \"AUC\") +\n  facet_wrap(~ wflow_id,\n             ncol = 2, scales = \"free_y\") +\n  lims(y = c(0.3, 1)) +\n  theme(text=element_text(size=13))\n\n\n\n\n\n\n\n\nThe best hyperparameters are extracted and added to the workflow for fitting and predicting.\n\n# Extract best hyperparameters\nworkflow_ids &lt;- grid_results |&gt;\n  pull(wflow_id)\n\ntuning_params_result &lt;- workflow_ids |&gt;\n  purrr::map(function(id) {\n    grid_results |&gt;\n      extract_workflow_set_result(id) |&gt;\n      select_best(metric = \"roc_auc\") |&gt; \n      mutate(wflow_id = id)\n  }) |&gt;\n  bind_rows() |&gt;\n  select(wflow_id, cost, rbf_sigma)\n\n\n# Finalize\nworkflow_set_final &lt;- workflow_ids |&gt;\n  purrr::map(function(id) {\n    grid_results |&gt;\n      extract_workflow(id) |&gt; \n      finalize_workflow(tuning_params_result |&gt; \n                          filter(wflow_id == id))\n  })\nnames(workflow_set_final) &lt;- workflow_ids\n\n# Fit\nworkflow_set_fit &lt;- workflow_ids |&gt;\n  purrr::map(function(id) {\n    workflow_set_final[[id]] |&gt;\n      last_fit(split = count_matrix_clr_split)\n  })\nnames(workflow_set_fit) &lt;- workflow_ids\n\n# Predict\nroc_auc_results &lt;- workflow_ids |&gt;\n  map(function(id) {\n    workflow_set_fit[[id]] |&gt;\n      collect_predictions() |&gt;\n      roc_curve(truth = Patient_status,\n                .pred_MS,\n                event_level = \"second\") |&gt;\n      mutate(wflow_id = id)\n  }) |&gt; bind_rows()\n\nThe ROC curves are visualized for the different models suggesting stand_svm is the better model.\n\nroc_auc_results |&gt;\n  ggplot(aes(x = 1 - specificity,\n             y = sensitivity,\n             col = wflow_id)) + \n  geom_path(lwd = 1) +\n  geom_abline(lty = 3) + \n  coord_equal() +\n  theme(text=element_text(size=13))",
    "crumbs": [
      "Modeling of Microbiome Data",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Non-Linear Modelling</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Bray, J. Roger, and J. T. Curtis. 1957. “An Ordination of the\nUpland Forest Communities of Southern Wisconsin.” Ecological\nMonographs 27 (October): 325–49. https://doi.org/10.2307/1942268.\n\n\nCox, Laura M., Amir Hadi Maghzi, Shirong Liu, Stephanie K. Tankou, Fyonn\nH. Dhang, Valerie Willocq, Anya Song, et al. 2021. “Gut Microbiome\nin Progressive Multiple Sclerosis.” Annals of Neurology\n89 (June): 1195–1211. https://doi.org/10.1002/ANA.26084.\n\n\nFaith, Daniel P. 1992. “Conservation Evaluation and Phylogenetic\nDiversity.” Biological Conservation 61 (January): 1–10.\nhttps://doi.org/10.1016/0006-3207(92)91201-3.\n\n\nPielou, E. C. 1966. “The Measurement of Diversity in Different\nTypes of Biological Collections.” Journal of Theoretical\nBiology 13: 131–44. https://doi.org/10.1016/0022-5193(66)90013-0.\n\n\nShannon, C. E. 1948. “A Mathematical Theory of\nCommunication.” Bell System Technical Journal 27:\n623–56. https://doi.org/10.1002/J.1538-7305.1948.TB00917.X.",
    "crumbs": [
      "References"
    ]
  }
]
[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BioStatistics",
    "section": "",
    "text": "Preface\nWrite some introduction to the book and its purpose.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "tidy00_book_intro.html",
    "href": "tidy00_book_intro.html",
    "title": "Tidy Modelling with R",
    "section": "",
    "text": "Some introduction to the book.",
    "crumbs": [
      "Tidy Modelling with R"
    ]
  },
  {
    "objectID": "tidy01_intro_package.html",
    "href": "tidy01_intro_package.html",
    "title": "1  Introduction to Tidy Modelling",
    "section": "",
    "text": "1.1 Introdution to dataset\nFor simplicity, the iris dataset is used since the structure is simple, and readers should be familiar with it. As a quick reminder, the dataset consists of five variables (columns) and 150 observations (rows). The first four variables are doubles describing the size of the flower, used as predictors. The last variable is a factor indicating the species of flower, used as the outcome. To make this a binary logistic classification problem, and not nominal, the versicolor species have been excluded.\ndim(iris)\n\n[1] 100   5\n\niris |&gt; \n  slice_head(n = 5) |&gt;\n  str()\n\ntibble [5 × 5] (S3: tbl_df/tbl/data.frame)\n $ Sepal.Length: num [1:5] 7 6.4 6.9 5.5 6.5\n $ Sepal.Width : num [1:5] 3.2 3.2 3.1 2.3 2.8\n $ Petal.Length: num [1:5] 4.7 4.5 4.9 4 4.6\n $ Petal.Width : num [1:5] 1.4 1.5 1.5 1.3 1.5\n $ Species     : Factor w/ 2 levels \"versicolor\",\"virginica\": 1 1 1 1 1",
    "crumbs": [
      "Tidy Modelling with R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Tidy Modelling</span>"
    ]
  },
  {
    "objectID": "tidy01_intro_package.html#training--and-test-set",
    "href": "tidy01_intro_package.html#training--and-test-set",
    "title": "1  Introduction to Tidy Modelling",
    "section": "1.2 Training- and test set",
    "text": "1.2 Training- and test set\nAs is customary when modeling, the dataset is split into a training- and testing set. We need to check for class imbalance in case some outcomes (here species) are more likely in the dataset. As seen from Figure 1.1, it is not the case.\n\niris |&gt; \n  ggplot(aes(x = Species,\n             fill = Species)) +\n  geom_bar()\n\n\n\n\n\n\n\nFigure 1.1: Count of each species in the iris dataset\n\n\n\n\n\nIf we saw a class imbalance, we would have used the below code for splitting the data. The prop argument indicates that 90% of the data is used for training and 20% for testing.\n\niris_split &lt;- initial_split(iris,\n                            prop = 0.90,\n                            strata = Species)\niris_train &lt;- training(iris_split)\niris_test &lt;- testing(iris_split)\n\nAs this is not the case, we use the below very similar code chunk.\n\niris_split &lt;- initial_split(iris, prop = 0.90)\niris_train &lt;- training(iris_split)\niris_test &lt;- testing(iris_split)",
    "crumbs": [
      "Tidy Modelling with R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Tidy Modelling</span>"
    ]
  },
  {
    "objectID": "tidy01_intro_package.html#fitting-a-model",
    "href": "tidy01_intro_package.html#fitting-a-model",
    "title": "1  Introduction to Tidy Modelling",
    "section": "1.3 Fitting a model",
    "text": "1.3 Fitting a model\nThe parsnip package standardize creating models. When learning a new technique, here parsnip, it is a good idea to apply well known theory. Therefore, a linear logistic regression model is used to predict the species of a flower given the dimensions of the sepal and the petal.\n\nlg_model &lt;- logistic_reg()\n\nDifferent packages contain different ways of applying a linear logistic regression model, and therefore different ways of supplying input, but the parsnip package standardize it. We choose which engine to use - i.e. which package.\n\n# Show available packages for the model\nshow_engines(\"logistic_reg\")\n\n# A tibble: 7 × 2\n  engine    mode          \n  &lt;chr&gt;     &lt;chr&gt;         \n1 glm       classification\n2 glmnet    classification\n3 LiblineaR classification\n4 spark     classification\n5 keras     classification\n6 stan      classification\n7 brulee    classification\n\nlg_model &lt;- lg_model |&gt; \n  set_engine(\"glm\")\n\nAs the goal is to predict flower species, it is necessary to specify how the model should be fit, e.g. what are the predictors and what is the outcome. This can be done via the formula syntax. From the p-values we see that some predictors are not statistically meaningful. This is ignored for now, but note that this metric is important.\n\n\n\nTable 1.1\n\n\nlg_fit &lt;- lg_model |&gt;\n  fit(Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width,\n      data = iris_train)\n\nlg_fit |&gt; tidy()\n\n# A tibble: 5 × 5\n  term         estimate std.error statistic p.value\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)    -41.6      24.6      -1.69  0.0912\n2 Sepal.Length    -2.35      2.33     -1.01  0.314 \n3 Sepal.Width     -6.94      4.49     -1.54  0.123 \n4 Petal.Length     9.55      4.86      1.97  0.0493\n5 Petal.Width     17.2       9.31      1.84  0.0653",
    "crumbs": [
      "Tidy Modelling with R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Tidy Modelling</span>"
    ]
  },
  {
    "objectID": "tidy01_intro_package.html#prediction",
    "href": "tidy01_intro_package.html#prediction",
    "title": "1  Introduction to Tidy Modelling",
    "section": "1.4 Prediction",
    "text": "1.4 Prediction\nMaking predictions is also standardized and requires the function predict(). The model seem to predict correctly in all cases.\n\niris_test |&gt; \n  select(Species) |&gt; \n  bind_cols(predict(lg_fit,\n                    new_data = iris_test))\n\n# A tibble: 10 × 2\n   Species    .pred_class\n   &lt;fct&gt;      &lt;fct&gt;      \n 1 versicolor versicolor \n 2 versicolor versicolor \n 3 versicolor versicolor \n 4 versicolor versicolor \n 5 versicolor versicolor \n 6 versicolor versicolor \n 7 virginica  virginica  \n 8 virginica  virginica  \n 9 virginica  virginica  \n10 virginica  virginica",
    "crumbs": [
      "Tidy Modelling with R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Tidy Modelling</span>"
    ]
  },
  {
    "objectID": "tidy02_mult_models.html",
    "href": "tidy02_mult_models.html",
    "title": "2  Creating multiple models",
    "section": "",
    "text": "2.1 Cross-validation\nThe reasonings for, and theoretical aspects of, cross-validation (CV) are expected to be already known. In the tidymodels universe, CV is setup as:\niris_folds &lt;- vfold_cv(iris_train,\n                       v = 10)\niris_folds\n\n#  10-fold cross-validation \n# A tibble: 10 × 2\n   splits         id    \n   &lt;list&gt;         &lt;chr&gt; \n 1 &lt;split [81/9]&gt; Fold01\n 2 &lt;split [81/9]&gt; Fold02\n 3 &lt;split [81/9]&gt; Fold03\n 4 &lt;split [81/9]&gt; Fold04\n 5 &lt;split [81/9]&gt; Fold05\n 6 &lt;split [81/9]&gt; Fold06\n 7 &lt;split [81/9]&gt; Fold07\n 8 &lt;split [81/9]&gt; Fold08\n 9 &lt;split [81/9]&gt; Fold09\n10 &lt;split [81/9]&gt; Fold10",
    "crumbs": [
      "Tidy Modelling with R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Creating multiple models</span>"
    ]
  },
  {
    "objectID": "tidy02_mult_models.html#recipes",
    "href": "tidy02_mult_models.html#recipes",
    "title": "2  Creating multiple models",
    "section": "2.2 Recipes",
    "text": "2.2 Recipes\nWhen it comes to modelling, creating and comparing multiple models is essential. This is easily achieved through the use of recipe() from the recipes package. A recipe is a collection of both a fomula and preprocessing steps. Here preprocessing steps is not applied, but could be: log-transforming data, creating dummy variables (e.g. one hot encoding) and sum up low occurring categories to handle class imbalance.\n\nSL_rec &lt;- recipe(Species ~ Sepal.Length,\n                 data = iris_train)\n\nSLW_rec &lt;- recipe(Species ~ Sepal.Length + Sepal.Width,\n                  data = iris_train)\n\nSLW_int_rec &lt;- recipe(Species ~ Sepal.Length + Sepal.Width,\n                      data = iris_train) |&gt;\n  step_interact(~ Sepal.Length:Sepal.Width)\n\nPL_rec &lt;- recipe(Species ~ Petal.Length,\n                 data = iris_train)\n\nPLW_rec &lt;- recipe(Species ~ Petal.Length + Petal.Width,\n                  data = iris_train)\n\nPLW_int_rec &lt;- recipe(Species ~ Petal.Length + Petal.Width,\n                      data = iris_train) |&gt;\n  step_interact(~ Petal.Length:Petal.Width)\n\nrecipe_list &lt;- list(SL = SL_rec,\n                    SLW = SLW_rec,\n                    SLW_int = SLW_int_rec,\n                    PL = PL_rec,\n                    PLW = PLW_rec,\n                    PLW_int = PLW_int_rec)\n\nlg_models &lt;- workflow_set(preproc = recipe_list,\n                          models = list(logistic = logistic_reg()),\n                          cross = FALSE)\n\nEach of the v folds are fitted with a purr-like workflow function:\n\n# To save the predicted values and used workflows\nkeep_pred &lt;- control_resamples(save_pred = TRUE, save_workflow = TRUE)\n\n\nlg_models &lt;- lg_models |&gt; \n  workflow_map(\"fit_resamples\",\n               resamples = iris_folds,\n               control = keep_pred,\n               seed = 1337)\n\n→ A | warning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\n\nThere were issues with some computations   A: x1\nThere were issues with some computations   A: x1\n\n\n\n\nlg_models\n\n# A workflow set/tibble: 6 × 4\n  wflow_id         info             option    result   \n  &lt;chr&gt;            &lt;list&gt;           &lt;list&gt;    &lt;list&gt;   \n1 SL_logistic      &lt;tibble [1 × 4]&gt; &lt;opts[2]&gt; &lt;rsmp[+]&gt;\n2 SLW_logistic     &lt;tibble [1 × 4]&gt; &lt;opts[2]&gt; &lt;rsmp[+]&gt;\n3 SLW_int_logistic &lt;tibble [1 × 4]&gt; &lt;opts[2]&gt; &lt;rsmp[+]&gt;\n4 PL_logistic      &lt;tibble [1 × 4]&gt; &lt;opts[2]&gt; &lt;rsmp[+]&gt;\n5 PLW_logistic     &lt;tibble [1 × 4]&gt; &lt;opts[2]&gt; &lt;rsmp[+]&gt;\n6 PLW_int_logistic &lt;tibble [1 × 4]&gt; &lt;opts[2]&gt; &lt;rsmp[+]&gt;",
    "crumbs": [
      "Tidy Modelling with R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Creating multiple models</span>"
    ]
  },
  {
    "objectID": "tidy02_mult_models.html#evaluation-metrics-across-models",
    "href": "tidy02_mult_models.html#evaluation-metrics-across-models",
    "title": "2  Creating multiple models",
    "section": "2.3 Evaluation metrics across models",
    "text": "2.3 Evaluation metrics across models\nTwo simple evaluation metrics for logistic regressions are the accuracy and Area Under the Curve (AUC), in this case area under the Receiver Operating Characteristic (ROC) curve. To view these two evaluation metrics:\n\ncollect_metrics(lg_models) |&gt; \n  select(-c(.config, preproc, .estimator)) |&gt; \n  filter(.metric == \"accuracy\") |&gt; \n  arrange(desc(mean))\n\n# A tibble: 6 × 6\n  wflow_id         model        .metric   mean     n std_err\n  &lt;chr&gt;            &lt;chr&gt;        &lt;chr&gt;    &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1 PLW_int_logistic logistic_reg accuracy 0.933    10  0.0246\n2 PL_logistic      logistic_reg accuracy 0.922    10  0.0237\n3 PLW_logistic     logistic_reg accuracy 0.922    10  0.0289\n4 SL_logistic      logistic_reg accuracy 0.733    10  0.0296\n5 SLW_logistic     logistic_reg accuracy 0.722    10  0.0299\n6 SLW_int_logistic logistic_reg accuracy 0.711    10  0.0339\n\ncollect_metrics(lg_models) |&gt; \n  select(-c(.config, preproc, .estimator)) |&gt; \n  filter(.metric == \"roc_auc\") |&gt; \n  arrange(desc(mean))\n\n# A tibble: 6 × 6\n  wflow_id         model        .metric  mean     n std_err\n  &lt;chr&gt;            &lt;chr&gt;        &lt;chr&gt;   &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1 PL_logistic      logistic_reg roc_auc 0.991    10 0.00458\n2 PLW_logistic     logistic_reg roc_auc 0.988    10 0.00825\n3 PLW_int_logistic logistic_reg roc_auc 0.988    10 0.00825\n4 SL_logistic      logistic_reg roc_auc 0.800    10 0.0442 \n5 SLW_logistic     logistic_reg roc_auc 0.787    10 0.0404 \n6 SLW_int_logistic logistic_reg roc_auc 0.776    10 0.0458 \n\n\nTo illustrate the two metrics:\n\nlg_models |&gt; \n  autoplot(metric = \"accuracy\") +\n  geom_label(aes(label = wflow_id)) +\n  theme(legend.position = \"none\") +\n  xlim(c(0.5, 6.5)) +\n  ggtitle(\"Accuracy stratified on model\")\n\n\n\n\n\n\n\nlg_models |&gt; \n  collect_predictions() |&gt; \n  group_by(wflow_id) |&gt; \n  roc_curve(truth = Species,\n            .pred_versicolor) |&gt; \n  autoplot() +\n  ggtitle(\"ROC curve statified on model\")",
    "crumbs": [
      "Tidy Modelling with R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Creating multiple models</span>"
    ]
  },
  {
    "objectID": "tidy02_mult_models.html#resample-to-resample-component-of-variation-consider-remove",
    "href": "tidy02_mult_models.html#resample-to-resample-component-of-variation-consider-remove",
    "title": "2  Creating multiple models",
    "section": "2.4 Resample-to-resample component of variation (consider remove)",
    "text": "2.4 Resample-to-resample component of variation (consider remove)\nAll the models are tested on the same v-folds. In some cases, different models tends to perform well on the same folds - this effect is called a resample-to-resample component of variation. We can numerically investigate these correlations by correlating each model estimates with eachother:\n\nlg_models |&gt; \n  collect_metrics(summarize = FALSE) |&gt; \n  filter(.metric == \"accuracy\") |&gt; \n  select(wflow_id, .estimate, id) |&gt; \n  pivot_wider(id_cols = \"id\",\n              names_from = \"wflow_id\",\n              values_from = \".estimate\") |&gt;\n  select(-id) |&gt; \n  corrr::correlate(quiet = TRUE)\n\n# A tibble: 6 × 7\n  term        SL_logistic SLW_logistic SLW_int_logistic PL_logistic PLW_logistic\n  &lt;chr&gt;             &lt;dbl&gt;        &lt;dbl&gt;            &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;\n1 SL_logistic     NA            0.930            0.627      -0.547        0.0320\n2 SLW_logist…      0.930       NA                0.812      -0.291       -0.0794\n3 SLW_int_lo…      0.627        0.812           NA           0.136        0.112 \n4 PL_logistic     -0.547       -0.291            0.136      NA           -0.180 \n5 PLW_logist…      0.0320      -0.0794           0.112      -0.180       NA     \n6 PLW_int_lo…     -0.0754      -0.187            0.0658     -0.0471       0.927 \n# ℹ 1 more variable: PLW_int_logistic &lt;dbl&gt;\n\n\nThe correlation illustrated:\n\nlg_models |&gt; \n  collect_metrics(summarize = FALSE) |&gt; \n  filter(.metric == \"accuracy\") |&gt; \n  mutate(wflow_id = reorder(wflow_id,\n                            .estimate)) |&gt; \n  ggplot(aes(x = wflow_id,\n             y = .estimate,\n             group = id,\n             color = id)) + \n  geom_line(alpha = .5,\n            linewidth = 1.25) + \n  theme(legend.position = \"none\")",
    "crumbs": [
      "Tidy Modelling with R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Creating multiple models</span>"
    ]
  },
  {
    "objectID": "tidy03_hyperparam.html",
    "href": "tidy03_hyperparam.html",
    "title": "3  Tuning hyperparameters and overfitting",
    "section": "",
    "text": "3.1 What is a hyperparameter?\nA classical and simple example of a hyperparameter is the number of neighbors (k) in a k-nearest neighbors (KNN) algorithm. This is a hyperparameter as it is not estimated during model fitting, but is specified a priori making it impossible to optimize during parameter estimation.",
    "crumbs": [
      "Tidy Modelling with R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Tuning hyperparameters and overfitting</span>"
    ]
  },
  {
    "objectID": "tidy03_hyperparam.html#set-up-tuning",
    "href": "tidy03_hyperparam.html#set-up-tuning",
    "title": "3  Tuning hyperparameters and overfitting",
    "section": "3.2 Set up tuning",
    "text": "3.2 Set up tuning\nIn the tidymodels universe, hyperparameters are marked for tuning in the specifications for a model. To examplify, both the number of nearest neighbors and a range of weight functions are tuned.\n\nknn_spec &lt;- nearest_neighbor(neighbors = tune(),\n                             weight_func = tune()) |&gt; \n  set_engine(engine = \"kknn\",\n             trace = 0) |&gt; \n  set_mode(\"classification\")\n\nSecondly, the recipe is set up. As no preprocessing is applied (e.g. log-transformation) it is quite simple.\n\nknn_rec &lt;- recipe(Species ~ ., # Use all other columns as predictors\n                  data = iris)\n\nThe specs and recipe is then combined into a workflow:\n\nknn_wflow &lt;- workflow() |&gt; \n  add_model(knn_spec) |&gt; \n  add_recipe(knn_rec)\n\nIt is possible to inspect which hyperparameters are being tuned, check which values that are tested and change those values. This is done through the use of the dials package.\n\n# Check hyperparameters\nknn_spec |&gt; extract_parameter_set_dials()\n\nCollection of 2 parameters for tuning\n\n  identifier        type    object\n   neighbors   neighbors nparam[+]\n weight_func weight_func dparam[+]\n\n# Check values tested\nknn_spec |&gt; extract_parameter_set_dials() |&gt; \n  extract_parameter_dials(\"weight_func\")\n\nDistance Weighting Function  (qualitative)\n\n\n10 possible values include:\n\n\n'rectangular', 'triangular', 'epanechnikov', 'biweight', 'triweight', 'cos', ... \n\n# Change values, save in new object\nknn_params &lt;- knn_spec |&gt; extract_parameter_set_dials() |&gt;\n  update(weight_func = weight_func(c(\"cos\", \"inv\", \"gaussian\")),\n         neighbors = neighbors(c(1, 15)))\n\n# Check that it is updated\nknn_params |&gt;\n  extract_parameter_dials(\"weight_func\")\n\nDistance Weighting Function  (qualitative)\n\n\n3 possible values include:\n\n\n'cos', 'inv' and 'gaussian' \n\nknn_params |&gt;\n  extract_parameter_dials(\"neighbors\")\n\n# Nearest Neighbors (quantitative)\nRange: [1, 15]\n\n\nDifferent grid_* functions exist to combine the hyperparameters, e.g. grid_random() and grid_regular(). As exemplified below, grid_regular() combines the parameters in all possible ways dependent on the number of levels chosen.\n\ngrid_regular(knn_params,\n             levels = 4)\n\n# A tibble: 12 × 2\n   neighbors weight_func\n       &lt;int&gt; &lt;chr&gt;      \n 1         1 cos        \n 2         5 cos        \n 3        10 cos        \n 4        15 cos        \n 5         1 inv        \n 6         5 inv        \n 7        10 inv        \n 8        15 inv        \n 9         1 gaussian   \n10         5 gaussian   \n11        10 gaussian   \n12        15 gaussian",
    "crumbs": [
      "Tidy Modelling with R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Tuning hyperparameters and overfitting</span>"
    ]
  },
  {
    "objectID": "tidy03_hyperparam.html#measure-performance-of-tuning",
    "href": "tidy03_hyperparam.html#measure-performance-of-tuning",
    "title": "3  Tuning hyperparameters and overfitting",
    "section": "3.3 Measure performance of tuning",
    "text": "3.3 Measure performance of tuning\nA metric is needed to measure the performance of the hyperparameters. The ROC curve is used. The regular grid is tuned:\n\n# Performance metric\nroc &lt;- metric_set(roc_auc)\n\n# Tuning\nknn_tune &lt;- knn_wflow |&gt; \n  tune_grid(iris_folds,\n            grid = knn_params |&gt; grid_regular(levels = 4),\n            metrics = roc)\nknn_tune\n\n# Tuning results\n# 10-fold cross-validation \n# A tibble: 10 × 4\n   splits          id     .metrics          .notes          \n   &lt;list&gt;          &lt;chr&gt;  &lt;list&gt;            &lt;list&gt;          \n 1 &lt;split [90/10]&gt; Fold01 &lt;tibble [12 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 2 &lt;split [90/10]&gt; Fold02 &lt;tibble [12 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 3 &lt;split [90/10]&gt; Fold03 &lt;tibble [12 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 4 &lt;split [90/10]&gt; Fold04 &lt;tibble [12 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 5 &lt;split [90/10]&gt; Fold05 &lt;tibble [12 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 6 &lt;split [90/10]&gt; Fold06 &lt;tibble [12 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 7 &lt;split [90/10]&gt; Fold07 &lt;tibble [12 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 8 &lt;split [90/10]&gt; Fold08 &lt;tibble [12 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 9 &lt;split [90/10]&gt; Fold09 &lt;tibble [12 × 6]&gt; &lt;tibble [0 × 3]&gt;\n10 &lt;split [90/10]&gt; Fold10 &lt;tibble [12 × 6]&gt; &lt;tibble [0 × 3]&gt;\n\n\nTo visualize the performance:\n\nknn_tune |&gt; \n  unnest(cols = .metrics) |&gt; \n  select(id, .metric, neighbors, weight_func, .estimate) |&gt;\n  group_by(neighbors, weight_func) |&gt; \n  mutate(estimate_avg = mean(.estimate)) |&gt;\n  ggplot(aes(x = neighbors,\n             y = estimate_avg)) +\n  geom_point() +\n  geom_line() +\n  scale_x_continuous(breaks = c(1, 5, 10, 15)) +\n  facet_wrap(~ weight_func)",
    "crumbs": [
      "Tidy Modelling with R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Tuning hyperparameters and overfitting</span>"
    ]
  },
  {
    "objectID": "tidy03_hyperparam.html#finalize-hyperparameter-selection",
    "href": "tidy03_hyperparam.html#finalize-hyperparameter-selection",
    "title": "3  Tuning hyperparameters and overfitting",
    "section": "3.4 Finalize hyperparameter selection",
    "text": "3.4 Finalize hyperparameter selection\nIt would seem there is no visual difference between the weight functions. For the number of neighbors, the performance is highest for 10 and 15 neighbors. Preferably, the simplest of the two models is chosen.\n\nfinal_hyperparams &lt;- tibble(weight_func = \"gaussian\",\n                            neighbors = 10)\n\nfinal_knn_wflow &lt;- knn_wflow |&gt; \n  finalize_workflow(final_hyperparams)\nfinal_knn_wflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: nearest_neighbor()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nK-Nearest Neighbor Model Specification (classification)\n\nMain Arguments:\n  neighbors = 10\n  weight_func = gaussian\n\nEngine-Specific Arguments:\n  trace = 0\n\nComputational engine: kknn \n\n\nThe model can now be fit to the data and used for prediction.\n\nfinal_knn_fit &lt;- final_knn_wflow |&gt; \n  fit(iris)\nfinal_knn_fit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: nearest_neighbor()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\nkknn::train.kknn(formula = ..y ~ ., data = data, ks = min_rows(10,     data, 5), kernel = ~\"gaussian\", trace = ~0)\n\nType of response variable: nominal\nMinimal misclassification: 0.07\nBest kernel: gaussian\nBest k: 10",
    "crumbs": [
      "Tidy Modelling with R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Tuning hyperparameters and overfitting</span>"
    ]
  },
  {
    "objectID": "stat00_intro.html",
    "href": "stat00_intro.html",
    "title": "Applied bio-statistical methods",
    "section": "",
    "text": "Each of the following chapters delve into an area of statistics. This ranges from biostatistics such as diversity measures and different statistical models such as Ordinary Linear Regression (OLS).\nIn the first part of the book, the iris data set was used to exemplify the usage of the tidymodels package. For the following chapters, a count matrix from a metagenomics study is used instead. The raw data can be obtained from Cox et al.(Cox et al. 2021). The structure of the data is quite similar to the iris data. In this case, each row is a sample from a person, each column is a genus and the values represent the abundance of said genus in said sample. Explaining how the count matrix is acquired from raw sequences is out of scope for this book. Scripts used for the pre-processing exists on the GitHub page from which this book is created, located in the src/ sub-folder.\nA snippet of the data set can be seen here:\n\ncount_matrix &lt;- readRDS(\"data/count_matrix/count_matrix.rds\")\ncount_matrix\n\n# A tibble: 456 × 124\n   Sample      Actinomyces Adlercreutzia Agathobacter Akkermansia Alistipes\n   &lt;chr&gt;             &lt;int&gt;         &lt;int&gt;        &lt;int&gt;       &lt;int&gt;     &lt;int&gt;\n 1 SRR14214860           0            11          190         486       272\n 2 SRR14214861           0            12            0           5      1158\n 3 SRR14214862           0             0            0           0         0\n 4 SRR14214863           4             0          505          94       361\n 5 SRR14214864           0            45          744           3       794\n 6 SRR14214865           0            29          924         933        45\n 7 SRR14214866           0            41         1187         308       145\n 8 SRR14214867           0            45          648         323       193\n 9 SRR14214868           0             0         6973           0       287\n10 SRR14214869           0             0          362         270        30\n# ℹ 446 more rows\n# ℹ 118 more variables: Anaerofustis &lt;int&gt;, Anaerostipes &lt;int&gt;,\n#   Anaerotruncus &lt;int&gt;, Angelakisella &lt;int&gt;, Bacteroides &lt;int&gt;,\n#   Barnesiella &lt;int&gt;, Bifidobacterium &lt;int&gt;, Bilophila &lt;int&gt;, Blautia &lt;int&gt;,\n#   Butyricicoccus &lt;int&gt;, Butyricimonas &lt;int&gt;, `CAG-352` &lt;int&gt;, `CAG-56` &lt;int&gt;,\n#   `Candidatus Soleaferrea` &lt;int&gt;, `Candidatus Stoquefichus` &lt;int&gt;,\n#   Catenibacillus &lt;int&gt;, Catenibacterium &lt;int&gt;, …\n\n\nThe shown snippet is used as test data when exemplifying.\n\ncount_matrix_test &lt;- count_matrix |&gt; \n  slice_head(n = 10) |&gt; \n  select(Sample, Actinomyces,\n         Adlercreutzia, Agathobacter, Akkermansia)\ncount_matrix_test\n\n# A tibble: 10 × 5\n   Sample      Actinomyces Adlercreutzia Agathobacter Akkermansia\n   &lt;chr&gt;             &lt;int&gt;         &lt;int&gt;        &lt;int&gt;       &lt;int&gt;\n 1 SRR14214860           0            11          190         486\n 2 SRR14214861           0            12            0           5\n 3 SRR14214862           0             0            0           0\n 4 SRR14214863           4             0          505          94\n 5 SRR14214864           0            45          744           3\n 6 SRR14214865           0            29          924         933\n 7 SRR14214866           0            41         1187         308\n 8 SRR14214867           0            45          648         323\n 9 SRR14214868           0             0         6973           0\n10 SRR14214869           0             0          362         270\n\n\n\n\n\n\nCox, Laura M., Amir Hadi Maghzi, Shirong Liu, Stephanie K. Tankou, Fyonn H. Dhang, Valerie Willocq, Anya Song, et al. 2021. “Gut Microbiome in Progressive Multiple Sclerosis.” Annals of Neurology 89 (June): 1195–1211. https://doi.org/10.1002/ANA.26084.",
    "crumbs": [
      "Applied bio-statistical methods"
    ]
  },
  {
    "objectID": "stat01_diversity.html",
    "href": "stat01_diversity.html",
    "title": "4  Diversity measures",
    "section": "",
    "text": "4.1 Alpha Diversity\nGenerally, alpha diversity measures the within-sample diversity. Multiple ways of calculating alpha diversity exists. A crude way is simply to count unique organisms per sample. One could also take the distribution into account or the phylogeny.\nThe R package vegan is widely used to help calculate diversity and is used here.",
    "crumbs": [
      "Applied bio-statistical methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Diversity measures</span>"
    ]
  },
  {
    "objectID": "stat01_diversity.html#alpha-diversity",
    "href": "stat01_diversity.html#alpha-diversity",
    "title": "4  Diversity measures",
    "section": "",
    "text": "4.1.1 Species Richness\nAs mentioned above, a crude way to measure diversity is to count the number of unique organisms in a given taxon for each sample. That is exactly what the species richness is. Through the vegan package, it can be calculated as follows:\n\nrichness &lt;- count_matrix_test |&gt;\n  column_to_rownames(var = \"Sample\") |&gt; \n  specnumber() |&gt;\n  as_tibble(rownames = \"Sample\") |&gt;\n  rename(Richness = value)\n\ncount_matrix_test |&gt;\n  left_join(richness,\n            by = \"Sample\")\n\n# A tibble: 10 × 6\n   Sample      Actinomyces Adlercreutzia Agathobacter Akkermansia Richness\n   &lt;chr&gt;             &lt;int&gt;         &lt;int&gt;        &lt;int&gt;       &lt;int&gt;    &lt;int&gt;\n 1 SRR14214860           0            11          190         486        3\n 2 SRR14214861           0            12            0           5        2\n 3 SRR14214862           0             0            0           0        0\n 4 SRR14214863           4             0          505          94        3\n 5 SRR14214864           0            45          744           3        3\n 6 SRR14214865           0            29          924         933        3\n 7 SRR14214866           0            41         1187         308        3\n 8 SRR14214867           0            45          648         323        3\n 9 SRR14214868           0             0         6973           0        1\n10 SRR14214869           0             0          362         270        2\n\n\n\n\n4.1.2 Shannon Index\nThe measure is usually referred to as Shannon Index or Shannon Entropy. It is able to take both the richness (described above) and the evenness into account. The evenness refers to how evenly the abundances are distributed among taxa for a species. The richness is considered when summing over all taxonomic units at a specified taxonomic rank. The index is calculated via the following formula:\n\\[\nH' = -\\sum_{i=1}^{R} p_i \\ln(p_i)\n\\]\n\\(R\\) is the number of observed species within a sample, i.e. the Richness. \\(p_{i}\\) is the proportion of the abundance belonging to the \\(i\\)’th species out of the total abundance for a specific sample(Shannon 1948).\nTo illustrate the meaning of the value of the Shannon Index a few examples are calculated. In the first scenario, three species and three samples exists. For the first sample, the proportion of abundances are spread evenly between the three species. For the second and third sample, the proportions are more skewered towards one species. These examples show how the evenness affect the Shannon Index.\n\n\n# A tibble: 3 × 4\n  sample   species_1 species_2 species_3\n  &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 sample_A       0.3       0.3       0.3\n2 sample_B       0.6       0.2       0.2\n3 sample_C       1         0         0  \n\n\nFor each of the three samples, the calculation would be as follows:\n\\[\nH'_{sample_{A}} = -(0.3 \\cdot ln{(0.3)} + 0.3 \\cdot ln{(0.3)} + 0.3 \\cdot ln{(0.3)}) = 1.08\n\\]\n\\[\nH'_{sample_{B}} = -(0.6 \\cdot ln{(0.6)} + 0.2 \\cdot ln{(0.2)} + 0.2 \\cdot ln{(0.2)}) = 0.95\n\\]\n\\[\nH'_{sample_{C}} = -(1 \\cdot ln{(1)}) = 0\n\\]\nWhen the samples are evenly distributed, we observe a higher Shannon Index.\nIn the following example, the number of species are increased to five. Three different samples with different distributions are still investigated.\n\n\n# A tibble: 3 × 6\n  sample   species_1 species_2 species_3 species_4 species_5\n  &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 sample_D       0.2       0.2       0.2       0.2       0.2\n2 sample_E       0.6       0.1       0.1       0.1       0.1\n3 sample_F       1         0         0         0         0  \n\n\n\\[\nH'_{sample_{D}} = 1.61\n\\]\n\\[\nH'_{sample_{E}} = 1.23\n\\]\n\\[\nH'_{sample_{F}} = 0\n\\]\nAs before, the Shannon Index decreases with a more skewered distribution. Both \\(sample_{A}\\) and \\(sample_{D}\\) have even distributions, but the Shannon Index for \\(sample_{D}\\) higher since the richness is higher, i.e. more species with an even distribution.\nThe diversity in a sample is said to be high when the number of species are high, and the distribution is even. As shown above, that would yield a high Shannon Index. So, to conclude, a high Shannon Index suggests high diversity.\nFinally, Shannon Index calculated for the test set. The index does seem to be higher when the genera are more evenly distributed as show in the above.\n\nshannon &lt;- count_matrix_test |&gt;\n  column_to_rownames(var = \"Sample\") |&gt;\n  diversity(index = \"shannon\") |&gt; \n  as_tibble(rownames = \"Sample\") |&gt; \n  rename(Shannon = value)\n\ncount_matrix_test |&gt;\n  left_join(shannon,\n            by = \"Sample\")\n\n# A tibble: 10 × 6\n   Sample      Actinomyces Adlercreutzia Agathobacter Akkermansia Shannon\n   &lt;chr&gt;             &lt;int&gt;         &lt;int&gt;        &lt;int&gt;       &lt;int&gt;   &lt;dbl&gt;\n 1 SRR14214860           0            11          190         486   0.667\n 2 SRR14214861           0            12            0           5   0.606\n 3 SRR14214862           0             0            0           0   0    \n 4 SRR14214863           4             0          505          94   0.472\n 5 SRR14214864           0            45          744           3   0.243\n 6 SRR14214865           0            29          924         933   0.762\n 7 SRR14214866           0            41         1187         308   0.618\n 8 SRR14214867           0            45          648         323   0.789\n 9 SRR14214868           0             0         6973           0   0    \n10 SRR14214869           0             0          362         270   0.683\n\n\n\n\n4.1.3 Faith’s phylogenetic diversity (Faith’s PD)\nFaith’s PD consider phylogenetic distances within a sample. The PD for a sample is calculated as the total sum of branch lengths from a phylogenetic tree, where the tree only contains the organisms found in a given sample (Faith 1992). Imagine a scenario with four organisms, organism a and b are closely related, and so are organism c and d (Figure 4.1).\n\n\n\n\n\n\nFigure 4.1: Phylogenetic tree\n\n\n\nIn the case where all four organisms were found in the same sample, the PD would be calculated as:\n\\[\nPD = 15 + 6 + 4 + 3 + 1 + 2 + 4 = 35\n\\]\nIf an organism a was lost, the PD would be decreased with 4 units to a value of 31. A higher PD is thus indicative of a higher diversity.",
    "crumbs": [
      "Applied bio-statistical methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Diversity measures</span>"
    ]
  },
  {
    "objectID": "stat01_diversity.html#beta-diversity",
    "href": "stat01_diversity.html#beta-diversity",
    "title": "4  Diversity measures",
    "section": "4.2 Beta Diversity",
    "text": "4.2 Beta Diversity\nBeta diversity describes the between-sample diversity. The diversity is calculated by comparing the diversity in one sample to the diversity in another sample. The diversity can be calculated in different ways, and the choice of method is dependent on the data and the question at hand. The most common methods include Bray-Curtis dissimilarity, UniFrac, and Jaccard similarity index (sometimes called Jaccard similarity coefficient).\nA crude way to calculate beta diversity is to simply count the number of organisms which exists in one sample, but not the other. The mathematical expression is:\n\\[\n\\beta =  (\\alpha_{sample_{1}} - c) + (\\alpha_{sample_{2}} - c)\n\\]\nWhere \\(\\alpha_{sample_{1}}\\) and \\(\\alpha_{sample_{2}}\\) are the number of unique organisms in sample 1 and 2 (the richness), respectively. \\(c\\) is the number of organisms which exists in both samples.\nAs an example, consider the following two samples (the top two rows of the test set):\n\ncount_matrix_test |&gt;\n  slice_head(n = 2)\n\n# A tibble: 2 × 5\n  Sample      Actinomyces Adlercreutzia Agathobacter Akkermansia\n  &lt;chr&gt;             &lt;int&gt;         &lt;int&gt;        &lt;int&gt;       &lt;int&gt;\n1 SRR14214860           0            11          190         486\n2 SRR14214861           0            12            0           5\n\n\nSample 1 (SRR14214860 ) has a richness of 3 and sample 2 (SRR14214861) has a richness of 2. They share 2 genera. The beta diversity would be calculated as:\n\\[\n\\beta = (3 - 2) + (2 - 2) = 1\n\\]\n\n4.2.1 Jaccard similarity index\nThe Jaccard index is another way to calculate beta diversity where only the presence/absence of an organism is taken into account. It is calculated as the number of shared organisms (intersection) divided by the number of unique organisms (union) in the two samples. The equation is as follows::\n\\[\nJ(A, B) = \\frac{|A \\cap B|}{|A \\cup B|}\n\\]\nFrom the equation we see why it is called a similarity index. The union (denominator) is just the number of observed organisms, and the intersection (numerator) is the number of shared organisms. The Jaccard index then represent the fraction of the observed organisms which were observed in both samples, i.e. the similarity of the two samples. The value ranges from 0 to 1, where 0 indicates that the two samples do not share any organisms, and 1 indicates that the two samples are identical.\nWhen comparing the two first samples in the subset, it is apparent that they share 2 genera, and a total of 3 genera is present between the two samples. The calculation by hand gives: \\[\nJ(SRR14214860, SRR14214861) = \\frac{2}{3} \\approx 0.67\n\\]\nThe two samples are only somewhat similar, as the Jaccard index is close to 1.\nTo calculate the Jaccard index by using the vegan package, the argument binary is set to TRUE to indicate that the data is binary, i.e. presence/absence of organisms instead of abundance. It is also important to note, that the vegan package actually calculates the dissimilarities, \\(1 - \\frac{|A \\cap B|}{|A \\cup B|}\\) so a higher value indicates a higher dissimilarity. The Jaccard index is calculated for the test data as follows:\n\ncount_matrix_test |&gt;\n  slice_head(n = 2) |&gt;\n  column_to_rownames(var = \"Sample\")|&gt;\n  vegdist(method = \"jaccard\",\n          binary = TRUE)\n\n            SRR14214860\nSRR14214861   0.3333333\n\n\nThe result is in agreement with the manual calculation since the vegan package calculates the dissimilarity. When inspecting the two samples, they also appear somewhat similar, as the majority of the organisms are present in both sample (even though talking about a majority is a bit misleading when only four organisms are compared). The abundances of the organisms are quite different, and to take that into account, another method is needed.\n\n\n4.2.2 Bray-Curtis dissimilarity\nThis measure is a widely used method to calculate beta diversity. Bray-Curtis dissimilarity takes the composition and abundance of the two compared samples into account. It is also important to note that it is a dissimilarity measure, so a higher value indicates a higher dissimilarity. The value ranges from 0 to 1. A value of 0 indicates that the two samples are identical, while a value of 1 indicates that the two samples do not share any organisms. The metric is calculated as:\n\\[\nBC_{jk} = \\frac{\\sum_{i=1}^{p} |S_{ij} - S_{ik}|}{\\sum_{i=1}^{p} (S_{ij} + S_{ik})}\n\\]\nWhere \\(S_{ij}\\) and \\(S_{ik}\\) are the abundances of the \\(i\\)’th taxon in sample \\(j\\) and \\(k\\), respectively. \\(p\\) is the number of taxa. With the test data, the Bray-Curtis dissimilarity is calculated as follows:\n\\[\nBC_{SRR14214860, SRR14214861} = \\frac{|0 - 0| + |11 - 12| + |190 - 0| + |486 - 5|}\n                                     {(0 + 0) + (11 + 12) + (190 + 0) + (486 + 5)}\n                                     = 0.955\n\\]\nBy using the vegan package the Bray-Curtis dissimilarity can be calculated as follows:\n\ncount_matrix_test |&gt;\n  slice_head(n = 2) |&gt;\n  column_to_rownames(var = \"Sample\")|&gt;\n  vegdist(method = \"bray\")\n\n            SRR14214860\nSRR14214861   0.9545455\n\n\nThe results are in agreement. The value is close to 1, indicating a high dissimilarity between the two samples. Interestingly, this is a different find compared to the Jaccard index, which indicated a higher similarity between the two samples. The abundances of the organisms in the two samples are quite different, and the Bray-Curtis dissimilarity takes that into account.\n\n\n4.2.3 UniFrac\nThe method is based on the phylogenetic tree of the organisms and thereby take the phylogenetic distances into account. UniFrac is shorthand notation for Unique Fraction and is calculated as the sum of unique branch lengths in the phylogenetic tree over the sum of shared branch lengths:\n\\[\nU = \\frac{sum \\; of \\; unique \\; branches' \\; length}{sum \\; of \\; all \\; branches'  \\; length}\n\\]\nThe possible values range from 0 to 1. In the case where the two compared samples share all organisms, the value of the numerator would sum to 0, and the UniFrac would be 0. If the two samples do not share any organisms, the value of the numerator would sum to the total length of the phylogenetic tree (the denominator), and the UniFrac would be 1.\nImagine a phylogenetic tree made from two samples, red and blue. Four different organisms were found in total (Figure 4.2). A red colour indicate an organism found in the red sample, a blue colour indicate an organism found in the blue sample and a purple colour indicate an organism was found in both samples.\n\n\n\n\n\n\nFigure 4.2: Phylogenetic tree with cohorts\n\n\n\nFollowing the equation above, the UniFrac distance would be calculated as:\n\\[\nU = \\frac{4 + 2 + 4}{15 + 6 + 4 + 3 + 1 + 2 + 4} = 0.286\n\\]\nThe value indicates that the blue and red samples are quite similar.\nThe method is divided into two different types, unweighted and weighted. It is the unweighted method which is described above. The weighted method takes the abundance of the organisms into account. The weighted method biases towards the most abundant organisms, while the unweighted method biases more towards the rare organisms. A third version exists, called Generalized UniFrac. This metric has another parameter, usually called \\(\\alpha\\), which can be used to control the balance between the unweighted and weighted methods.\n\n\n\n\nFaith, Daniel P. 1992. “Conservation Evaluation and Phylogenetic Diversity.” Biological Conservation 61 (January): 1–10. https://doi.org/10.1016/0006-3207(92)91201-3.\n\n\nShannon, C. E. 1948. “A Mathematical Theory of Communication.” Bell System Technical Journal 27: 623–56. https://doi.org/10.1002/J.1538-7305.1948.TB00917.X.",
    "crumbs": [
      "Applied bio-statistical methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Diversity measures</span>"
    ]
  },
  {
    "objectID": "stat02_wilcoxon.html",
    "href": "stat02_wilcoxon.html",
    "title": "5  Wilcoxon test",
    "section": "",
    "text": "A plethora of statistical tests exists. The choice of test depends on the type of data. For example, if the data is normally distributed, the t-test can be used to compare the means of two groups. If the data is not normally distributed, the Wilcoxon test can be used instead. Below the Shannon Index for the full data set have been calculated and plotted stratified by the patient status, Healthy vs Multiple Sclerosis (MS). As can be seen from the plots, the Shannon Index does not seem to follow a normal distribution.\n\nshannon &lt;- count_matrix |&gt;\n  column_to_rownames(var = \"Sample\") |&gt;\n  diversity(index = \"shannon\") |&gt; \n  as_tibble(rownames = \"Sample\") |&gt; \n  rename(Shannon = value) |&gt; \n  left_join(meta,\n            by = \"Sample\")\n\nshannon |&gt;\nggplot(aes(x = Shannon,\n           fill = Patient_status)) +\n  geom_density(alpha = 0.7) +\n  facet_wrap(~ Patient_status)\n\n\n\n\n\n\n\n\nThe Wilcoxon test is a non-parametric test, which means it do not assume that the samples have been taken from a specific distribution, e.g. the normal distribution. Two different Wilcoxon tests exists, one for comparing dependent samples and one for comparing independent samples. Our samples would be dependent if they were taken from the same patient at different times. Since we are comparing different patients, our samples are independent. Hence, the specific Wilcoxon test we will use is the Wilcoxon Rank Sum test, also called the Mann-Whitney U test.\nThe test is rank-based, which means that all the data is ranked and are then used to calculate the test statistic, in this case the U statistic. After ranking the data, the sum of the ranks for the two groups are calculated. U is then calculated as:\n\\[\nU = min(U_1, \\; U_2) = min(R_1 - \\frac{n_1(n_1+1)} {2}, \\; R_2 - \\frac{n_2(n_2+1)} {2})\n\\]\nWhere \\(R_1\\) and \\(R_2\\) is the sum of the ranks for the first and second group, respectively. Likewise, \\(n_1\\) and \\(n_2\\) is the number of samples in the first and second group, respectively. The U statistic is then used to find the p-value, which is used to determine if the difference between the two groups is statistically significant. The p-value is the probability of observing a U statistic as extreme as the one observed, given that the null hypothesis of no difference between the distributions of \\(U_1\\) and \\(U_2\\) is true. Due to the Central Limit Theorem, the distributions of \\(U_1\\) and \\(U_2\\), and thereby \\(U\\), can be assumed to follow a normal distribution when the sample size is larger than approximately 20. The Wilcoxon Rank Sum test for the Shannon Index is shown below. As the p-value is much lower than the usual significance level of 0.05, we can conclude that the Shannon Index is significantly different between the two groups. Here, the two groups are the healthy and MS patients.\n\nwilcox.test(Shannon ~ Patient_status,\n            data = shannon) |&gt; \n  broom::tidy()\n\n# A tibble: 1 × 4\n  statistic p.value method                                           alternative\n      &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                                            &lt;chr&gt;      \n1      7016 0.00440 Wilcoxon rank sum test with continuity correcti… two.sided  \n\n\n\n5.0.0.1 OLD\nTry to manually calculate U statistic (successful) and p-value (no success)\n\n# Select 3 healthy samples\nhealthy_samples &lt;- count_matrix |&gt;\n  column_to_rownames(var = \"Sample\") |&gt;\n  diversity(index = \"shannon\") |&gt; \n  as_tibble(rownames = \"Sample\") |&gt; \n  rename(Shannon = value) |&gt; \n  left_join(meta,\n            by = \"Sample\") |&gt; \n  select(Sample, Patient_status, Shannon) |&gt;\n  filter(Patient_status == \"Healthy\") |&gt;\n  slice_head(n = 25)\n\n# Select 7 sick samples\nsick_samples &lt;- count_matrix |&gt;\n  column_to_rownames(var = \"Sample\") |&gt;\n  diversity(index = \"shannon\") |&gt; \n  as_tibble(rownames = \"Sample\") |&gt; \n  rename(Shannon = value) |&gt; \n  left_join(meta,\n            by = \"Sample\") |&gt; \n  select(Sample, Patient_status, Shannon) |&gt;\n  filter(Patient_status == \"MS\") |&gt;\n  slice_head(n = 25)\n\n# Combine the selected samples\nselected_samples &lt;- bind_rows(healthy_samples, sick_samples) |&gt; \n  arrange(Shannon) |&gt; \n  tibble::rowid_to_column(\"Rank\") # no ties between values\n\n\n# Calculate the U statistic\nU_statistic &lt;- selected_samples |&gt;\n  group_by(Patient_status) |&gt;\n  mutate(R = sum(Rank),\n         U = R - (n()*(n()+1)/2)) |&gt; \n  pull(U) |&gt;\n  min()\n\n\nwilcox.test(Shannon ~ Patient_status,\n            data = selected_samples, paired= FALSE) |&gt; \n  broom::tidy()\n\n# A tibble: 1 × 4\n  statistic p.value method                       alternative\n      &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                        &lt;chr&gt;      \n1       263   0.345 Wilcoxon rank sum exact test two.sided",
    "crumbs": [
      "Applied bio-statistical methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Wilcoxon test</span>"
    ]
  },
  {
    "objectID": "stat03_mult_testing.html",
    "href": "stat03_mult_testing.html",
    "title": "6  Multiple testing",
    "section": "",
    "text": "CALCULATE WILCOXON FOR SHANNON INDEX WITH DIFFERENT COHORTS AND ADJUST FOR MULTIPLE TESTING\nGood video for multiple testing: https://www.youtube.com/watch?v=RUX94txw4Qo",
    "crumbs": [
      "Applied bio-statistical methods",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Multiple testing</span>"
    ]
  },
  {
    "objectID": "stat04_OLS.html",
    "href": "stat04_OLS.html",
    "title": "7  Ordinary Least Squares Regression",
    "section": "",
    "text": "A wide range of models for predicting data exists. One of the most common models is the linear regression model. In this case, the model is called the Ordinary Least Squares (OLS) which is a type of linear regression model.\nThe general idea is to fit a line to the data, such that the sum of the squared residuals is minimized, hence the name least squares. Residuals are the difference between the actual value and the predicted value of the model.\nAnother famously used data set is the mtcars. Below is an example of a scatter plot of the miles pr gallon (mpg) vs the weight of the car (wt). By fitting a linear regression model, the goal is to predict the miles pr gallon based on the weight of the car. For an initial guess, the mean of the miles pr gallon is used and shown as the red dashed line. The residuals are the distance between the actual value (dots) and the predicted value (red dashed line). The sum of the residuals is then used as a measure of how well the model fits the data. As some predicted values are below the actual value, and some values are above, these can counter balance each other, such that the sum of the residuals is 0. To avoid this, the residuals are squared, such that all residuals are positive. Further, by squaring the residuals the bigger residuals have a larger effect on the sum of the residuals squared. Since the goal is to minimize the sum of the residuals squared, the bigger residuals are more important to reduce.\n\nmpg_mean &lt;- mtcars |&gt; \n  pull(mpg) |&gt; \n  mean()\n\nmtcars |&gt; \n  ggplot(aes(x = wt,\n             y = mpg)) +\n  geom_point(size = 3,\n             colour = \"steelblue\") +\n  geom_hline(yintercept = mpg_mean,\n             colour = \"firebrick\",\n             linetype = \"dashed\") +\n  labs(title = \"Miles pr Gallon vs Weight\",\n       x = \"Weight (1000 lbs)\",\n       y = \"Miles pr Gallon\",\n       colour = \"Miles per gallon\")\n\n\n\n\n\n\n\n\nIn this simple case, the model is fitting the line to the data by changing the slope and intercept of the line to find the slope and intercept that minimizes the sum of the squared residuals. This results in a three dimensional space, where the slope and intercept are plotted along the x and y axis, and the sum of the squared residuals are plotted along the z-axis. Some combination of slope and intercept results in a minimum in the 3D plane. This combination can be found by taking the derivative of the sum of the squared residuals with respect to the slope and intercept, and set it equal to zero.\nThe model can be generalized to more than one variable, such that the model can predict the outcome based on multiple variables. Usually, the simple linear case is written as \\(y = ax + b\\) where \\(a\\) is the slope and \\(b\\) is the intercept. When using multiple variables, it is generalizes to \\(y = \\beta_0 + \\beta_{1}x_1 + \\beta_{2}x_2 + ... + \\beta_{n}x_n\\). In matrix form it can be written as \\(y = X\\beta\\) where \\(X\\) is a matrix with \\(n\\) rows (number of samples) and \\(p\\) columns (number of variables). \\(\\beta\\) is a vector of length \\(p\\) which includes a parameter for each variable.\nAs mentioned earlier, the goal is to minimize the sum of the squared residuals. On matrix form, the residual sum of squares (RSS) is written as:\n\\[\nRSS = ||y-X\\beta||_{2}^{2}\n\\]\nWhere the subscript 2 denotes the L2 norm, i.e. Euclidean distance. The term \\(X\\beta\\) is the predicted value of the model as we saw from \\(y = X\\beta\\) in the above. RSS is also written as:\n\\[\nRSS = \\sum_{i=1}^{n} (y_i - X_{i}\\beta)^2 =  \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n\\]\nWhere \\(\\hat{y}_i\\) is the predicted value of the model for the \\(i\\)’th sample. Since the goal is to minimize the RSS, the \\(\\beta\\)s should be chosen such that the RSS is minimized:\n\\[\n\\beta_{OLS} = \\arg\\min_{\\beta} RSS = \\arg\\min_{\\beta} ||y-X\\beta||_{2}^{2}\n\\]\nAs mentioned in the above, the \\(\\beta\\)s can be found by taking the derivative of the RSS with respect to \\(\\beta\\) and set it equal to zero. Since this is generalized to multiple variables, the gradient is taken with respect to the vector \\(\\beta\\) instead. The equation is then:\n\\[\n\\nabla_\\beta||y-X\\beta||_{2}^{2} = 0\n\\]",
    "crumbs": [
      "Applied bio-statistical methods",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Ordinary Least Squares Regression</span>"
    ]
  },
  {
    "objectID": "stat05_log_reg.html",
    "href": "stat05_log_reg.html",
    "title": "8  Logistic Regression",
    "section": "",
    "text": "Predict TRUE or FALSE, not a continuous value. The output is a probability from 0 to 1.\nOLS uses sum of squared residuals (SSR). Logistic regression uses maximum likelihood.\nWatch https://www.youtube.com/watch?v=XepXtl9YKwc and https://www.youtube.com/watch?v=p3T-_LMrvBc (second video for some math included) for explanation of maximum likelihood.",
    "crumbs": [
      "Applied bio-statistical methods",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "stat06_enet.html",
    "href": "stat06_enet.html",
    "title": "9  Elastic Net",
    "section": "",
    "text": "9.1 Ridge Regression\nEssentially OLS. Introduce bias to reduce variance. OLS minimizes the sum of squared residuals (SSR), Ridge minimizes the SSR and lambda X slope^2. lambda X slope^2 introduces a penalty varied by lambda. Slope is the coefficient of the variable, so when lambda increases, the penalty increases and the slope decreases, meaning the coefficient of the variable decreases.\nHigher values for variables means more sensitive to small changes in data, it is those high values we want to reduce. As our model is less sensitive to the small changes, we have introduced bias in our model. Find values for lambda by testing many values with CV.\nIt also gives an extra incentive to reduce especially high variable coefficients. Makes this model more robust to outliers? Outliers could result in large coefficients and Ridge regression would reduce these.\nMention what happens with lambda = 0, and with very high lambda values. Can only shrink slope asymptotically to 0.\nWhy can Ridge not set variables to 0?",
    "crumbs": [
      "Applied bio-statistical methods",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Elastic Net</span>"
    ]
  },
  {
    "objectID": "stat06_enet.html#ridge-regression",
    "href": "stat06_enet.html#ridge-regression",
    "title": "9  Elastic Net",
    "section": "",
    "text": "9.1.1 Logistic Regression\nCan be used for Logistic Regression. Instead optimizes the sum of likelihoods and lambda X slope^2 since logistic regression uses maximum likelihood.",
    "crumbs": [
      "Applied bio-statistical methods",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Elastic Net</span>"
    ]
  },
  {
    "objectID": "stat06_enet.html#lasso-regression",
    "href": "stat06_enet.html#lasso-regression",
    "title": "9  Elastic Net",
    "section": "9.2 Lasso Regression",
    "text": "9.2 Lasso Regression\nAlso introduces bias for lowering variance.\nInstead of squaring the slope as Ridge, take absolute value.\nMention what happens with lambda = 0, and with very high lambda values. Can shrink slope to 0.\nWhy can Lasso set variables to 0?\n\n9.2.1 Logistic Regression",
    "crumbs": [
      "Applied bio-statistical methods",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Elastic Net</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Cox, Laura M., Amir Hadi Maghzi, Shirong Liu, Stephanie K. Tankou, Fyonn\nH. Dhang, Valerie Willocq, Anya Song, et al. 2021. “Gut Microbiome\nin Progressive Multiple Sclerosis.” Annals of Neurology\n89 (June): 1195–1211. https://doi.org/10.1002/ANA.26084.\n\n\nFaith, Daniel P. 1992. “Conservation Evaluation and Phylogenetic\nDiversity.” Biological Conservation 61 (January): 1–10.\nhttps://doi.org/10.1016/0006-3207(92)91201-3.\n\n\nShannon, C. E. 1948. “A Mathematical Theory of\nCommunication.” Bell System Technical Journal 27:\n623–56. https://doi.org/10.1002/J.1538-7305.1948.TB00917.X.",
    "crumbs": [
      "References"
    ]
  }
]
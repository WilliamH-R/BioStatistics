Set seed and load packages.
```{r}
#| output: FALSE
#| code-fold: true
#| code-summary: "Show the code"

set.seed(1337)

library("tidymodels")
tidymodels::tidymodels_prefer()
```

Load data.
```{r}
#| output: FALSE
#| code-fold: true
#| code-summary: "Show the code"

mtcars <- mtcars |> 
  as_tibble()
```


# Ordinary Least Squares Regression

A wide range of models for predicting data exists. One of the most common models is the linear regression model. In this case, the model is called the Ordinary Least Squares (OLS) which is a type of linear regression model.

The general idea is to fit a line to the data, such that the sum of the squared residuals is minimized, hence the name least squares. Residuals are the difference between the actual value and the predicted value of the model.

Another famously used data set is the `mtcars`. Below is an example of a scatter plot of the miles pr gallon (mpg) vs the weight of the car (wt). By fitting a linear regression model, the goal is to predict the miles pr gallon based on the weight of the car. For an initial guess, the mean of the miles pr gallon is used and shown as the red dashed line. The residuals are the distance between the actual value (dots) and the predicted value (red dashed line). The sum of the residuals is then used as a measure of how well the model fits the data. As some predicted values are below the actual value, and some values are above, these can counter balance each other, such that the sum of the residuals is 0. To avoid this, the residuals are squared, such that all residuals are positive. Further, by squaring the residuals the bigger residuals have a larger effect on the sum of the residuals squared. Since the goal is to minimize the sum of the residuals squared, the bigger residuals are more important to reduce.

```{r}
mpg_mean <- mtcars |> 
  pull(mpg) |> 
  mean()

mtcars |> 
  ggplot(aes(x = wt,
             y = mpg)) +
  geom_point(size = 3,
             colour = "steelblue") +
  geom_hline(yintercept = mpg_mean,
             colour = "firebrick",
             linetype = "dashed") +
  labs(title = "Miles pr Gallon vs Weight",
       x = "Weight (1000 lbs)",
       y = "Miles pr Gallon",
       colour = "Miles per gallon")
```

In this simple case, the model is fitting the line to the data by changing the slope and intercept of the line. The combination which minimizes the sum of the squared residuals (SSR) is chosen. This results in a three dimensional space, where the slope and intercept are plotted along the x and y axis, and the sum of the squared residuals are plotted along the z-axis. Some combination of slope and intercept results in a minimum in the 3D plane. This combination can be found by taking the derivative of the sum of the squared residuals with respect to the slope and intercept, and set it equal to zero.

The model can be generalized to more than one variable, such that the model can predict the outcome based on multiple variables. Usually, the simple linear case is written as $y = ax + b$ where $a$ is the slope and $b$ is the intercept. When using multiple variables, it is generalized to $y = \beta_0 + \beta_{1}x_1 + \beta_{2}x_2 + ... + \beta_{n}x_n$. In matrix form it can be written as $y = X\beta$ where $X$ is a matrix with $n$ rows (number of samples) and $p$ columns (number of variables). $\beta$ is a vector of length $p$ which includes a parameter for each variable.

As mentioned earlier, the goal is to minimize the sum of the squared residuals. On matrix form, SSR is written as:

$$
SSR = ||y-X\beta||_{2}^{2}
$$

Where the subscript 2 denotes the L2 norm, i.e. Euclidean distance. The term $X\beta$ is the predicted value of the model as we saw from $y = X\beta$ in the above. SSR is also written as:

$$
SSR = \sum_{i=1}^{n} (y_i - X_{i}\beta)^2 =  \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
$$

Where $\hat{y}_i$ is the predicted value of the model for the $i$'th sample. Since the goal is to minimize the SSR, the $\beta$s should be chosen such that the SSR is minimized:

$$
\beta_{OLS} = \min_{\beta} SSR = \min_{\beta} ||y-X\beta||_{2}^{2}
$$

As mentioned in the above, the $\beta$s can be found by taking the derivative of the SSR with respect to $\beta$ and set it equal to zero. Since this is generalized to multiple variables, the gradient is taken with respect to the vector $\beta$ instead. The equation is then:

$$
\nabla_\beta||y-X\beta||_{2}^{2} = 0
$$
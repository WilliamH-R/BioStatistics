---
output: html_document
editor_options: 
  chunk_output_type: inline
  
params:
  percentage_train: 0.80
  n_folds: 10
  grid_size: 200
  penalty_lower: -10
  penalty_higher: 1
---
Set seed and load packages.
```{r}
#| output: FALSE
#| code-fold: true
#| code-summary: "Show the code"

set.seed(1337)

library("tidymodels")
tidymodels::tidymodels_prefer()
library("vegan")
```

Load data.
```{r}
#| output: FALSE
#| code-fold: true
#| code-summary: "Show the code"

count_matrix_clr <- readr::read_rds("https://github.com/WilliamH-R/BioStatistics/raw/main/data/count_matrix/count_matrix_clr.rds") |> 
  select(-"NA")

meta <- read.csv(file = "data/metadata.txt") |> 
  as_tibble() |>
  select(Run, chem_administration, ETHNICITY, geo_loc_name,
         Host_age, host_body_mass_index, Host_disease, host_phenotype, host_sex) |> 
  rename(Sample = Run,
         Treatment = chem_administration,
         Ethnicity = ETHNICITY,
         Location = geo_loc_name,
         Age = Host_age,
         BMI = host_body_mass_index,
         Disease_severity = Host_disease,
         EDSS = host_phenotype,
         Sex = host_sex) |>
  mutate(Patient_status = case_when(Disease_severity == "1HealthyControl" ~ "Healthy",
                                    TRUE ~ "MS"),
         EDSS = as.factor(EDSS),
         EDSS = case_when(is.na(EDSS) & Disease_severity == "1HealthyControl" ~ "-1",
                          is.na(EDSS) & Disease_severity != "1HealthyControl" ~ "Unknown",
                          TRUE ~ EDSS),
         EDSS = as.factor(EDSS))
```

# Modelling

## Data wrangling
The data is joined with the metadata as the `Patient_status` is used as the outcome variable. Only the columns that are needed for the model are kept. Notice `inner_join()` is used such that only rows for which both the count matrix and metadata are available are kept.

```{r}
count_matrix_clr <- count_matrix_clr |>
  inner_join(meta,
             by = "Sample") |>
  select(-c(Sample, Treatment, Ethnicity, Location,
            Age, BMI, Disease_severity, EDSS, Sex)) |>
  relocate(Patient_status)
```


## Cross-Validation
The data is split into a training and testing set using cross-validation to avoid overfitting. This is especially needed, as some hyperparameters needs to be tuned.

The data is split:
```{r}
count_matrix_clr_split <- initial_split(count_matrix_clr,
                                        prop = params$percentage_train,
                                        strata = Patient_status)
count_matrix_clr_train <- training(count_matrix_clr_split)
count_matrix_clr_test <- testing(count_matrix_clr_split)
```

The CV object is created:
```{r}
count_matrix_clr_folds <- vfold_cv(count_matrix_clr_train,
                                   v = params$n_folds)
count_matrix_clr_folds
```

## Recipes
The recipes contain two pieces of important information. The first is the formula that describes the relationship between the variables and the outcome. In this case, which genera to use for predicting disease. The second is the pre-processing steps that are applied to the data before the model is built. This could include steps such as scaling, log-transform or imputing missing values. Here, the data is already clr-transformed.

Two recipes are created. One without any pre-processing steps, and one where Principal Component Analysis (PCA) is applied to the data. In both cases, all variables is used to try and predict the outcome `Patient_status`.

```{r}
stand_recipe <- recipe(Patient_status ~ .,
                       data = count_matrix_clr_train)

pca_recipe <- stand_recipe |> 
  step_pca(all_predictors(),
           num_comp = 7)
```

## Model Specifications
To model the data, a specification is needed. This includes the model to use, the engine to use, the mode of prediction and choosing hyperparameters. Since the outcome is binary, a logistic regression model is used. The model is later tuned using a grid search over the hyperparameters `penalty` and `mixture`. Models with exclusively Ridge and Lasso regularization are built, as well as a model with a mixture of both.

```{r}
log_spec <- logistic_reg(penalty = tune(),
                         mixture = tune()) |>
  set_engine("glmnet") |>
  set_mode("classification")

log_ridge_spec <- logistic_reg(penalty = tune(),
                               mixture = 0) |>
  set_engine("glmnet") |>
  set_mode("classification")

log_lasso_spec <- logistic_reg(penalty = tune(),
                               mixture = 1) |>
  set_engine("glmnet") |>
  set_mode("classification")
```

## Change Hyperparameter Values
The hyperparameters `penalty` and `mixture` are extracted from the model specification. This is done to see the possible values that can be used in the grid search.

For the mixture, values between $0.05$ and $1$ is used which seem to be decent values.

```{r}
log_spec |> 
  extract_parameter_set_dials() |> 
  extract_parameter_dials("mixture")
```

For the penalty, values between $10^-10$ and $1$ are used pr default. The range is increased to $5*10^1$. The updated range is saved in a new object, but the information is not added to the models until the workflow is created further down. As seen from the output, the information do not pertain to a specific model, but simply contain ranges for different hyperparameters.

```{r}
log_param_ranges <- log_spec |> 
  extract_parameter_set_dials() |>
  update(penalty = penalty(c(params$penalty_lower,
                             params$penalty_higher)))

log_penalty_param_ranges <- log_lasso_spec |> 
  extract_parameter_set_dials() |>
  update(penalty = penalty(c(params$penalty_lower,
                             params$penalty_higher)))

log_param_ranges

log_penalty_param_ranges
```

## Create the Workflow Set
The recipes are combined with the model specifications to create a workflow set. The workflow set is used to fit the models to the data and evaluate the models. Notice here it is called a workflow set, as it contains multiple workflows. Each row of the workflow set is a workflow of its own which further contain the recipe and model.

A workflow set for all combinations of recipes and models is created:

```{r}
workflow_set <- workflow_set(
  preproc = list(stand = stand_recipe,
                 pca = pca_recipe),
  models = list(log = log_spec,
                log_ridge = log_ridge_spec,
                log_lasso = log_lasso_spec)
)

workflow_set
```

The parameter objects which contain the hyperparameter ranges are added to the workflow:

```{r}
workflow_set <- workflow_set |>
  option_add(id = "stand_log",
             param_info = log_param_ranges) |>
  option_add(id = "pca_log",
             param_info = log_param_ranges) |>
  option_add(id = "stand_log_ridge",
             param_info = log_penalty_param_ranges) |>
  option_add(id = "stand_log_lasso",
             param_info = log_penalty_param_ranges) |>
  option_add(id = "pca_log_ridge",
             param_info = log_penalty_param_ranges) |>
  option_add(id = "pca_log_lasso",
             param_info = log_penalty_param_ranges)
workflow_set
```

## Tune hyperparameters

All workflows in the workflow set contain hyperparameters. These are tuned with a search grid using the function `tune_grid()`. As `workflow_set` contain multiple workflows, the `tune_grid()` function can be mapped over the workflows to tune all hyperparameters using `workflow_map()`, a `purrr`-like map function.

Several settings for tuning the grid exists. This includes e.g. whether or not to parallelize, what output to save and how verbose the output should be, i.e. how much should be printed to std.out. The settings are set to:

```{r}
grid_settings <-
   control_grid(
      save_pred = TRUE,
      parallel_over = "everything",
      save_workflow = TRUE,
      extract = function(x) x
   )
```

The grid search is performed for a grid of size 10. This means that 10 different combinations of the hyperparameters are tried for each model.

```{r}
grid_results <- workflow_set |>
  workflow_map(
    fn = "tune_grid",
    seed = 1337,
    resamples = count_matrix_clr_folds,
    grid = params$grid_size,
    control = grid_settings
  )
```

`tidymodels` comes with a lot of convenience functions. One of these is `autoplot()` that can, among other things, plot the results of the grid search. The best result of each workflow is selected based on the AUC. 

```{r}
autoplot(
  grid_results,
  rank_metric = "roc_auc",
  metric = "roc_auc",
  type = "wflow_id"
) +
  geom_point(aes(y = mean)) +
  lims(y = c(0.4, 1)) +
  theme(legend.position = "none") +
  facet_wrap(~ wflow_id,
             ncol = 2)
```

The issue with wrappers, e.g. the convenience function `autoplot()`, is that they do not always provide the flexibility needed.

```{r}
grid_results |>
  collect_metrics() |>
  filter(.metric == "roc_auc") |> 
  group_by(wflow_id) |>
  arrange(desc(mean)) |>
  mutate(rank = row_number()) |>
  ungroup() |>
  ggplot(aes(x = rank,
             y = mean,
             col = wflow_id)) +
  geom_point() +
  geom_errorbar(aes(ymin = mean - std_err,
                    ymax = mean + std_err)) +
  theme(legend.position = "none") +
  labs(x = "Rank",
       y = "AUC") +
  facet_wrap(~ wflow_id,
             ncol = 2, scales = "free_y")
```

The hyperparameters resulting in the best AUC is extracted for each workflow. Note that the value of mixture is an order of magnitude different for `stand_log` and `pca_log`. In the PCA space, the model emphasizes Lasso over Ridge. This is likely due to the PCA reducing the number of variables, which in turn seems to reduce the need for setting coefficients to 0 (Lasso regularization).


Note that the value of mixture is close to zero for both Elastic Net models, which means that the model emphasizes Ridge over Lasso. Note also that the penalty is much lower for the workflow with PCA pre-processing. This is likely due to the PCA reducing the number of variables, which in turn seems to reduce the need for regularization.

```{r}
workflow_ids <- grid_results |>
  pull(wflow_id)


tuning_params_result <- workflow_ids |>
  purrr::map(~{grid_results |>
      extract_workflow_set_result(.x) |>
      select_best(metric = "roc_auc") |> 
      mutate(wflow_id = .x)
    }) |>
  bind_rows() |>
  mutate(mixture = case_when(stringr::str_detect(string = wflow_id,
                                                 pattern = "lasso") ~ 1,
                             stringr::str_detect(string = wflow_id,
                                                 pattern = "ridge") ~ 0,
                             TRUE ~ mixture)) |>
  select(wflow_id, penalty, mixture)
  
tuning_params_result
```

## Finalize workflow
As the hyperparameters have been tuned, the final workflow can be created. This is done by adding the hyperparameters to the workflow. When the hyperparameters have been added, the workflows can be fitted.

```{r}
workflow_set_final <- workflow_ids |>
  purrr::map(~{
    grid_results |>
      extract_workflow(.x) |> 
      finalize_workflow(tuning_params_result |> 
                          filter(wflow_id == .x))
    })
names(workflow_set_final) <- workflow_ids
```

To exemplify, the `pca_log` workflow is shown from before finalizing and after finalizing. As can be seen, the hyperparameters are added to the workflow such that they have a value instead of `tune()`.

```{r}
grid_results |>
  extract_workflow("pca_log")

workflow_set_final$pca_log
```


## Fit and Predict for the Final Workflow
With the hyperparameters set, the workflows can be fit to the data, and the models can be evaluated. The `last_fit()` function is used to fit the model to the data. The `split` argument is set to the cross-validation object created earlier. This is done to avoid overfitting.

```{r}
workflow_set_fit <- workflow_ids |>
  purrr::map(~{
    workflow_set_final[[.x]] |>
      last_fit(split = count_matrix_clr_split)
  })
names(workflow_set_fit) <- workflow_ids
```

## Predict and visualize performance
For each of the fitted models, the predictions are collected and the performance is evaluated. One option is to look at the confusion matrices, e.g.: 

```{r}
workflow_set_fit$stand_log |>
  collect_predictions()|> 
  conf_mat(truth = Patient_status,
           estimate = .pred_class)
```

Another more visual approach is to plot the ROC-curve. This is done for all models in the workflow set. First, the specificity and sensitivity is calculated for each model with a convenience function `roc_curve()`

```{r}
roc_auc_results <- workflow_ids |>
  map( ~ {
    workflow_set_fit[[.x]] |>
      collect_predictions() |>
      roc_curve(truth = Patient_status,
                .pred_MS,
                event_level = "second") |>
      mutate(wflow_id = .x)
  }) |> bind_rows()

roc_auc_results
```

Finally, the ROC-curve is plotted stratified by the workflow ID. The ROC-curve is a plot of the sensitivity against (1 - specificity). Sensitivity is the proportion of true positives out of all positives, i.e. true positive rate. In this case, the proportion of correctly classified MS patients out of all MS patients. Specificity is the proportion of true negatives out of all negatives. In this case, the proportion of correctly classified healthy patients out of all healthy patients. Since the x-axis plots (1 - specificity), it is actually the proportion of false positives out of all negatives, i.e. the false positive rate.

```{r}
roc_auc_results |>
  ggplot(aes(x = 1 - specificity,
             y = sensitivity,
             col = wflow_id)) + 
  geom_path(lwd = 1,
            alpha = 0.6) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  scale_color_viridis_d()
```

## Fine Tuning Hyperparameters
The hyperparameters are tuned using a grid search. The grid search is performed for a grid of size `r params$grid_size`. The best result of each workflow is selected based on the AUC and is chosen strictly on the best numerical value. It is very possible, that a model with a slightly lower AUC is preferable due to simplicity. To investigate this, plots of the hyperparameters plotted against the AUC is created.

It can be easily illustrated with the `autoplot()` function. The plot is not very informative with multiple hyperparameters, as it is not shown how they affect the AUC in combination.

```{r}
autoplot(grid_results,
         id = "stand_log",
         metric = "roc_auc")
```

For the two ENet models, the hyperparameters are extracted manually and plotted against the AUC. For the Ridge and Lasso models, the hyperparameters performance is plotted using the `autoplot` function.

### Standard Preprocessing, Lasso
By inspecting the plot, it would seem the optimal `penalty` is indeed at the optimum `roc_auc`.

```{r}
autoplot(grid_results,
         id = "stand_log_lasso",
         metric = "roc_auc")
```

The manually chosen hyperparameter is collected.

```{r}
stand_log_lasso_penalty <- grid_results |>
  extract_workflow_set_result("stand_log_lasso") |>
  show_best(metric = "roc_auc",
            n = 1) |>
  pull(penalty)

stand_log_lasso_mixture <- 1
```

### Standard Preprocessing, Ridge
By inspecting the plot, it would seem the optimal `penalty` is indeed at the optimum `roc_auc`.

```{r}
autoplot(grid_results,
         id = "stand_log_ridge",
         metric = "roc_auc")
```

The manually chosen hyperparameter is collected.

```{r}
stand_log_ridge_penalty <- grid_results |>
  extract_workflow_set_result("stand_log_ridge") |>
  show_best(metric = "roc_auc",
            n = 1) |>
  pull(penalty)

stand_log_ridge_mixture <- 0
```

### Standard Preprocessing, Elastic Net

For the ENet models, two parameters are tuned: `penalty` and `mixture`. Instead of plotting against AUC, the hyperparameters are plotted against each other, and the color represents the AUC. The best AUC are the more purple colors. The plot shows that the best AUC is achieved with a penalty close to zero.

```{r}
stand_log_plot <- grid_results |>
  extract_workflow_set_result("stand_log") |>
  show_best(metric = "roc_auc",
            n = params$grid_size) |>
  ggplot(aes(x = penalty,
             y = mixture,
             col = mean)) +
  geom_point() +
  scale_color_viridis_c(direction = -1) +
  labs(title = "Standard Preprocessing, Elastic Net",
       x = "Penalty",
       y = "Mixture",
       col = "AUC")
stand_log_plot
```

To better see the effect of the hyperparameters on the AUC, the x-axis is limited as the higher values are observed close to $penalty = 0$.

```{r}
p <- stand_log_plot +
  lims(x = c(0, 1))

plotly::ggplotly(p)
```

```{r}
stand_log_penalty <- 0.06319747

stand_log_mixture <- 0.05603853
```

### PCA Preprocessing, Lasso
By inspecting the plot, it would seem the optimal `penalty` is indeed at the optimum `roc_auc`.

```{r}
autoplot(grid_results,
         id = "pca_log_lasso",
         metric = "roc_auc")
```

The manually chosen hyperparameter is collected.

```{r}
pca_log_lasso_penalty <- grid_results |>
  extract_workflow_set_result("pca_log_lasso") |>
  show_best(metric = "roc_auc",
            n = 1) |>
  pull(penalty)

pca_log_lasso_mixture <- 1
```

### PCA Preprocessing, Ridge
By inspecting the plot, the ROC values do not vary much with the penalty. The optimal `penalty` is chosen to be the maximum `penalty`.

```{r}
autoplot(grid_results,
         id = "pca_log_ridge",
         metric = "roc_auc")
```

The manually chosen hyperparameter is collected.

```{r}
pca_log_ridge_penalty <- grid_results |>
  extract_workflow_set_result("pca_log_ridge") |>
  show_best(metric = "roc_auc",
            n = params$grid_size) |>
  pull(penalty) |> 
  max()

pca_log_ridge_mixture <- 0
```

### PCA Preprocessing, Elastic Net

Again, the hyperparameter values are plotted against each other, and the colour signifies the AUC. The best AUC are the more purple colors.

```{r}
pca_log_plot <- grid_results |>
  extract_workflow_set_result("pca_log") |>
  show_best(metric = "roc_auc",
            n = params$grid_size) |>
  ggplot(aes(x = penalty,
             y = mixture,
             col = mean)) +
  geom_point() +
  scale_color_viridis_c(direction = -1) +
  labs(title = "PCA Preprocessing, Elastic Net",
       x = "Penalty",
       y = "Mixture",
       col = "AUC")
pca_log_plot
```

To better see the effect of the hyperparameters on the AUC, the x-axis is limited as the higher values are observed close to $penalty = 0$.

```{r}
p <- pca_log_plot +
  lims(x = c(0, 1))

plotly::ggplotly(p)
```

By inspecting the plotly object, the parameters are chosen.

```{r}
pca_log_penalty <- 0.3379935

pca_log_mixture <- 0.12525226
```


### Gather Manually Chosen Hyperparameters

```{r}
tuning_params_result_manual <- tribble(~wflow_id, ~penalty, ~mixture,
                                       "stand_log_lasso", stand_log_lasso_penalty, 1,
                                       "stand_log_ridge", stand_log_ridge_penalty, 0,
                                       "stand_log", stand_log_penalty, stand_log_mixture,
                                       "pca_log_lasso", pca_log_lasso_penalty, 1,
                                       "pca_log_ridge", pca_log_ridge_penalty, 0,
                                       "pca_log", pca_log_penalty, pca_log_mixture)
```

## Finalize, Fit and Predict with Manually Chosen Hyperparameters

The finalization, fitting and predicting is done as before, but with the manually chosen hyperparameters.

```{r}
# Finalize
workflow_set_final_manual <- workflow_ids |>
  purrr::map(~{
    grid_results |>
      extract_workflow(.x) |> 
      finalize_workflow(tuning_params_result_manual |> 
                          filter(wflow_id == .x))
    })
names(workflow_set_final_manual) <- workflow_ids

# Fit
workflow_set_fit_manual <- workflow_ids |>
  purrr::map(~{
    workflow_set_final_manual[[.x]] |>
      last_fit(split = count_matrix_clr_split)
  })
names(workflow_set_fit_manual) <- workflow_ids

# Predict
roc_auc_results_manual <- workflow_ids |>
  map( ~ {
    workflow_set_fit_manual[[.x]] |>
      collect_predictions() |>
      roc_curve(truth = Patient_status,
                .pred_MS,
                event_level = "second") |>
      mutate(wflow_id = .x)
  }) |> bind_rows()

# Visualize performance
roc_auc_results_manual |>
  ggplot(aes(x = 1 - specificity,
             y = sensitivity,
             col = wflow_id)) + 
  geom_path(lwd = 1,
            alpha = 0.6) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  scale_color_viridis_d()
```

# Session Info

```{r}
sessioninfo::session_info()
```
---
output: html_document
editor_options: 
  chunk_output_type: inline
  
params:
  percentage_train: 0.80
  n_folds: 10
  grid_size: 100
  penalty_lower: -10
  penalty_higher: 1
---
Set seed and load packages.
```{r}
#| output: false
#| code-fold: true
#| code-summary: "Show the code"

set.seed(1337)

library("tidymodels")
tidymodels::tidymodels_prefer()
library("vegan")
library("embed")
```

Load data.
```{r}
#| output: false
#| code-fold: true
#| code-summary: "Show the code"

count_matrix_clr <- readr::read_rds("https://github.com/WilliamH-R/BioStatistics/raw/main/data/count_matrix/count_matrix_clr.rds") |> 
  select(-"NA")

meta <- read.csv(file = "data/metadata.txt") |> 
  as_tibble() |>
  select(Run, chem_administration, ETHNICITY, geo_loc_name,
         Host_age, host_body_mass_index, Host_disease, host_phenotype, host_sex) |> 
  rename(Sample = Run,
         Treatment = chem_administration,
         Ethnicity = ETHNICITY,
         Location = geo_loc_name,
         Age = Host_age,
         BMI = host_body_mass_index,
         Disease_severity = Host_disease,
         EDSS = host_phenotype,
         Sex = host_sex) |>
  mutate(Patient_status = case_when(Disease_severity == "1HealthyControl" ~ "Healthy",
                                    TRUE ~ "MS"),
         EDSS = as.factor(EDSS),
         EDSS = case_when(is.na(EDSS) & Disease_severity == "1HealthyControl" ~ "-1",
                          is.na(EDSS) & Disease_severity != "1HealthyControl" ~ "Unknown",
                          TRUE ~ EDSS),
         EDSS = as.factor(EDSS))
```

Chosen parameters.

```{r}
params
```


# Linear Modelling {#sec-lin}

Logistic Regression is a linear model since the decision boundary is linear. Linear models tend to be faster due to the less complex math, and are thereby also better for scalability (large data sets). Further, linear models are easier to interpret, as the coefficients can be directly interpreted as the effect of the feature on the response. Linear models also contain some built in regularization as they cannot memorize the data set which can help avoiding overfitting.

The issues with linear models occur when the data do not contain linear relationships, in which case the model will not be able to capture the underlying structure.

In this chapter, different logistic regression models are built to predict the disease status of individuals. The models are built using different preprocessing steps and different hyperparameter combinations.

## Data Wrangling
The data is joined with the metadata as the `Patient_status` is used as the response. Only the columns that are needed for the model are kept. Notice `inner_join()` is used such that only rows for which both the count matrix and metadata are available are kept.

```{r}
count_matrix_clr <- count_matrix_clr |>
  inner_join(meta,
             by = "Sample") |>
  select(-c(Sample, Treatment, Ethnicity, Location,
            Age, BMI, Disease_severity, EDSS, Sex)) |>
  relocate(Patient_status)
```


## Cross-Validation
The data is split into a training and testing set using cross-validation to avoid overfitting. This is especially needed as hyperparameters are to be tuned.

The data is split:
```{r}
count_matrix_clr_split <- initial_split(count_matrix_clr,
                                        prop = params$percentage_train,
                                        strata = Patient_status)
count_matrix_clr_train <- training(count_matrix_clr_split)
count_matrix_clr_test <- testing(count_matrix_clr_split)
```

The CV object is created:
```{r}
count_matrix_clr_folds <- vfold_cv(count_matrix_clr_train,
                                   v = params$n_folds)
```

## Recipes
The recipes contains two pieces of important information. The first is the formula that describes the relationship between the features and the response. In this case, which genera to use for predicting disease. The second is the pre-processing steps that are applied to the data before the model is built. This could include steps such as scaling, log-transform or imputing missing values. Here, the data is already clr-transformed.

Two recipes are created. One without any pre-processing steps and one where PCA is applied for dimensionality reduction. In both cases, all features are used to try and predict the response `Patient_status`.

```{r}
stand_recipe <- recipe(Patient_status ~ .,
                       data = count_matrix_clr_train)

pca_recipe <- stand_recipe |> 
  step_pca(all_predictors(),
           num_comp = 7)
```

## Model Specifications
To model the data, a specification is needed. This includes the model to use, the engine to use, mode of prediction and choosing hyperparameters. Since the response is binary, a logistic regression model is used. The model is later tuned using a grid search over the hyperparameters `penalty` and `mixture`. Models with exclusively Ridge and Lasso regularization are built, as well as a model with a mixture of both.

```{r}
log_spec <- logistic_reg(penalty = tune(),
                         mixture = tune()) |>
  set_engine("glmnet") |>
  set_mode("classification")

log_ridge_spec <- logistic_reg(penalty = tune(),
                               mixture = 0) |>
  set_engine("glmnet") |>
  set_mode("classification")

log_lasso_spec <- logistic_reg(penalty = tune(),
                               mixture = 1) |>
  set_engine("glmnet") |>
  set_mode("classification")
```

## Change Hyperparameter Values
The hyperparameters `penalty` and `mixture` are extracted from the model specification to see the possible values that can be used in the grid search.

For the mixture, values between $0.05$ and $1$ is used which seem to be reasonable values.

```{r}
log_spec |> 
  extract_parameter_set_dials() |> 
  extract_parameter_dials("mixture")
```

For the penalty, values between $10^{-10}$ and $1$ are used per default. The range is increased to $5\cdot10^1$. The updated range is saved in a new object, but the information is not added to the models until the workflow is created further down. As seen from the output, the information do not pertain to a specific model, but simply contain ranges for different hyperparameters.

```{r}
log_param_ranges <- log_spec |> 
  extract_parameter_set_dials() |>
  update(penalty = penalty(c(params$penalty_lower,
                             params$penalty_higher)))

log_penalty_param_ranges <- log_lasso_spec |> 
  extract_parameter_set_dials() |>
  update(penalty = penalty(c(params$penalty_lower,
                             params$penalty_higher)))

log_param_ranges

log_penalty_param_ranges
```

## Create the Workflow Set
The recipes are combined with the model specifications to create a workflow set. The workflow set is used to fit the models to the data and evaluate the models. Notice it is called a workflow set, as it contains multiple workflows. Each row of the workflow set is a workflow of its own which further contain the recipe and model.

A workflow set for all combinations of recipes and models is created:

```{r}
workflow_set <- workflow_set(
  preproc = list(stand = stand_recipe,
                 pca = pca_recipe#,
                 #umap = umap_recipe
                 ),
  models = list(log = log_spec,
                log_ridge = log_ridge_spec,
                log_lasso = log_lasso_spec)
)

workflow_set
```

The parameter objects which contain the hyperparameter ranges are added to the workflow (note the `option` column is changed):

```{r}
workflow_set <- workflow_set |>
  option_add(id = "stand_log",
             param_info = log_param_ranges) |>
  option_add(id = "pca_log",
             param_info = log_param_ranges) |>
  option_add(id = "stand_log_ridge",
             param_info = log_penalty_param_ranges) |>
  option_add(id = "stand_log_lasso",
             param_info = log_penalty_param_ranges) |>
  option_add(id = "pca_log_ridge",
             param_info = log_penalty_param_ranges) |>
  option_add(id = "pca_log_lasso",
             param_info = log_penalty_param_ranges)
workflow_set
```

## Tune Hyperparameters

All workflows in the workflow set contains hyperparameters. These are tuned with a search grid using the function `tune_grid()`. As `workflow_set` contain multiple workflows, the `tune_grid()` function can be mapped over the workflows to tune all hyperparameters using `workflow_map()`, a `purrr`-like map function.

Several settings for tuning the grid exists. This includes e.g. whether or not to parallelize, what output to save and how verbose the output should be, i.e. how much should be printed to std.out. The settings are set to:

```{r}
grid_settings <-
   control_grid(
      save_pred = TRUE,
      parallel_over = "everything",
      save_workflow = TRUE,
      extract = function(x) x
   )
```

The grid search is performed for a grid of size `r params$grid_size`, which means `r params$grid_size` different combinations of the hyperparameters are tried for each model.

```{r}
grid_results <- workflow_set |>
  workflow_map(
    fn = "tune_grid",
    seed = 1337,
    resamples = count_matrix_clr_folds,
    grid = params$grid_size,
    control = grid_settings
  )
```

`tidymodels` comes with a lot of convenience functions. One of these is `autoplot()` that can, among other things, plot the results of the grid search. The best result of each workflow is selected based on the AUC. 

```{r}
autoplot(
  grid_results,
  rank_metric = "roc_auc",
  metric = "roc_auc",
  type = "wflow_id"
) +
  geom_point(aes(y = mean)) +
  lims(y = c(0.4, 1)) +
  theme(legend.position = "none") +
  facet_wrap(~ wflow_id,
             ncol = 2) +
  lims(y = c(0.5, 1))
```

The issue with wrappers, e.g. the convenience function `autoplot()`, is that they do not always provide the flexibility needed. A cleaner plot is created by extracting the data and manually plotting it. The plot shows the AUC for each workflow, with error bars showing the standard error.

```{r}
grid_results |>
  collect_metrics() |>
  filter(.metric == "roc_auc") |> 
  group_by(wflow_id) |>
  arrange(desc(mean)) |>
  mutate(rank = row_number()) |>
  ungroup() |>
  ggplot(aes(x = rank,
             y = mean,
             col = wflow_id)) +
  geom_point() +
  geom_errorbar(aes(ymin = mean - std_err,
                    ymax = mean + std_err)) +
  theme(legend.position = "none") +
  labs(x = "Rank",
       y = "AUC") +
  facet_wrap(~ wflow_id,
             ncol = 2,
             scales = "free_y")
```

A couple of interesting insights can be gained from looking at the optimal hyperparameters (see table below). When comparing the two ENet models (`stand_log` vs `pca_log`), the mixture is higher for `pca_log`, i.e. preferring Lasso over Ridge. This is opposite expectations as PCA should reduce the number of features, which in turn should reduce the need for Lasso regularization. However, `penalty` is several orders of magnitude lower for `pca_log` meaning the Lasso regularization is less strict. When comparing the full Ridge and Lasso models between the two preprocessing steps, the `penalty` is lower when PCA preprocessing is performed. This is expected as PCA reduces the number of features reducing the need for regularization.

```{r}
workflow_ids <- grid_results |>
  pull(wflow_id)


tuning_params_result <- workflow_ids |>
  purrr::map(function(id) {
    grid_results |>
      extract_workflow_set_result(id) |>
      select_best(metric = "roc_auc") |> 
      mutate(wflow_id = id)
    }) |>
  bind_rows() |>
  mutate(mixture = case_when(stringr::str_detect(string = wflow_id,
                                                 pattern = "lasso") ~ 1,
                             stringr::str_detect(string = wflow_id,
                                                 pattern = "ridge") ~ 0,
                             TRUE ~ mixture)) |>
  select(wflow_id, penalty, mixture)
  
tuning_params_result
```

## Finalize Workflow
As the hyperparameters have been tuned, the final workflow can be created which is done by adding the hyperparameters to the workflow.

```{r}
workflow_set_final <- workflow_ids |>
  purrr::map(function(id) {
    grid_results |>
      extract_workflow(id) |> 
      finalize_workflow(tuning_params_result |> 
                          filter(wflow_id == id))
    })
names(workflow_set_final) <- workflow_ids
```

To exemplify, the `pca_log` workflow is shown from before finalizing and after finalizing. As can be seen, the hyperparameters are added to the workflow such that they have a value instead of `tune()`.

```{r}
# Before finalizing
grid_results |>
  extract_workflow("pca_log")

# After finalizing
workflow_set_final$pca_log
```

## Fit and Predict for the Final Workflow

With the hyperparameters set, the workflows can be fitted to the data, and the models can be evaluated. The `last_fit()` function is used to fit the model to the data. The `split` argument is set to the cross-validation object created earlier fitting to the entire training data.

```{r}
workflow_set_fit <- workflow_ids |>
  purrr::map(function(id) {
    workflow_set_final[[id]] |>
      last_fit(split = count_matrix_clr_split)
  })
names(workflow_set_fit) <- workflow_ids
```

## Visualize Performance

Different options exist for visualizing the performance of the models. One option is to look at the confusion matrices. The example below shows the confusion matrix for the `stand_log` model. From the table it can be seen that the model always predict MS, regardless of the true status.

```{r}
workflow_set_fit$stand_log |>
  collect_predictions()|> 
  conf_mat(truth = Patient_status,
           estimate = .pred_class)
```

Another more visual approach is to plot the ROC-curve for each model. First, the specificity and sensitivity is calculated for each model with a convenience function `roc_curve()`:

```{r}
roc_auc_results <- workflow_ids |>
  map(function(id) {
    workflow_set_fit[[id]] |>
      collect_predictions() |>
      roc_curve(truth = Patient_status,
                .pred_MS,
                event_level = "second") |>
      mutate(wflow_id = id)
  }) |> bind_rows()

roc_auc_results
```

Finally, the ROC-curve is plotted stratified by the workflow ID. The ROC-curve is a plot of the sensitivity against `1 - specificity`. Sensitivity is the proportion of true positives out of all positives, i.e. true positive rate. In this case, the proportion of correctly classified MS patients out of all MS patients. Specificity is the proportion of true negatives out of all negatives. In this case, the proportion of correctly classified healthy patients out of all healthy patients. Since the x-axis plots `1 - specificity`, it is rather the proportion of false positives out of all negatives, i.e. the false positive rate (also called false discovery rate).

Determining the best model given the ROC-curve is not trivial as it depends on how many false positives one is willing to accept. If the model is to decide on a risky treatment, one might want to avoid false positives at all costs, e.g. a threshold of 0.01, which would choose `stand_log_lasso` as it has the highest sensitivity at that threshold. Increasing the threshold to 0.25 would instead choose `stand_log`.

```{r}
roc_auc_results |>
  ggplot(aes(x = 1 - specificity,
             y = sensitivity,
             col = wflow_id)) + 
  geom_path(lwd = 1,
            alpha = 0.5) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  scale_color_viridis_d()
```

Otherwise, the general rule is to choose the model with the highest AUC since it is the model that performs best across all thresholds. `stand_log_ridge` performs slightly better than the other models.

```{r}
workflow_ids |>
  purrr::map(function(id) {
    grid_results |>
      extract_workflow_set_result(id) |>
      show_best(metric = "roc_auc",
                n = 1) |> 
      mutate(wflow_id = id)
    }) |>
  bind_rows() |>
  ggplot(aes(x = wflow_id,
             y = mean,
             fill = wflow_id)) +
  geom_col() +
  geom_label(aes(label = round(mean, 3)),
             position = position_stack(vjust = 0.9),
             show.legend = FALSE) +
  labs(title = "AUC",
       x = "Workflow ID",
       y = "AUC by Workflows") +
  theme(axis.text.x = element_blank(),
        axis.ticks.x = element_blank())
```

## Fine Tuning Hyperparameters

The hyperparameters were tuned using a grid search of size `r params$grid_size`. The best result of each workflow is chosen solely on the best value of AUC. It is possible, that a slightly worse performing model is preferred due to it being simpler. Models with a larger `penalty` are simpler as the coefficients are closer to zero, or less coefficients are present. A "one standard error rule" can be used to choose the simplest model that is within one standard error of the best model.

```{r}
#| code-fold: true
#| code-summary: "Show the code"

# All results
ridge_results <- grid_results |>
  extract_workflow_set_result("stand_log_ridge") |>
  show_best(metric = "roc_auc",
            n = params$grid_size)

# Numerically best
n_best <- ridge_results |>
  filter(mean == max(mean)) |>
  filter(penalty == min(penalty))

# 1-std error rule best
one_std_err_best <- grid_results |>
  extract_workflow_set_result("stand_log_ridge") |>
  select_by_one_std_err(p = desc(penalty),
                        metric = "roc_auc") |>
  left_join(ridge_results |>
              select(.config,
                     mean),
            by = ".config")

ridge_results |>
  ggplot(aes(x = penalty,
             y = mean)) +
  geom_point(aes(col = "Data Points")) +
  geom_point(aes(col = "Numerical Best"),
             data = n_best) +
  geom_point(aes(col = "One std err Best"),
             data = one_std_err_best) +
  geom_errorbar(aes(ymin = mean - std_err,
                    ymax = mean + std_err),
                col = "steelblue") +
  scale_x_log10() +
  scale_color_manual(values = c("Data Points" = "steelblue",
                                "Numerical Best" = "red",
                                "One std err Best" = "green")) +
  labs(title = "Standard Preprocessing, Ridge",
       x = "Penalty",
       y = "AUC") +
  lims(y = c(0, 0.75))
```

The practice is applied to all the models by extracting, finalizing, fitting, predicting and then visualizing as done in the above.

```{r}
#| code-fold: true
#| code-summary: "Show the code"

# Extract hyperparameters
tuning_params_result_fine <- workflow_ids |>
  purrr::map(function(id) {
    grid_results |>
      extract_workflow_set_result(id) |>
      select_by_one_std_err(p = desc(penalty),
                            metric = "roc_auc") |> 
      mutate(wflow_id = id)
    }) |>
  bind_rows() |>
  mutate(mixture = case_when(stringr::str_detect(string = wflow_id,
                                                 pattern = "lasso") ~ 1,
                             stringr::str_detect(string = wflow_id,
                                                 pattern = "ridge") ~ 0,
                             TRUE ~ mixture)) |>
  select(wflow_id, penalty, mixture)

# Finalize
workflow_set_final_fine <- workflow_ids |>
  purrr::map(function(id) {
    grid_results |>
      extract_workflow(id) |> 
      finalize_workflow(tuning_params_result_fine |> 
                          filter(wflow_id == id))
    })
names(workflow_set_final_fine) <- workflow_ids

# Fit
workflow_set_fit_fine <- workflow_ids |>
  purrr::map(function(id) {
    workflow_set_final_fine[[id]] |>
      last_fit(split = count_matrix_clr_split)
  })
names(workflow_set_fit_fine) <- workflow_ids

# Predict
roc_auc_results_fine <- workflow_ids |>
  map(function(id) {
    workflow_set_fit_fine[[id]] |>
      collect_predictions() |>
      roc_curve(truth = Patient_status,
                .pred_MS,
                event_level = "second") |>
      mutate(wflow_id = id)
  }) |> bind_rows()

# Visualize performance
roc_auc_results_fine |>
  ggplot(aes(x = 1 - specificity,
             y = sensitivity,
             col = wflow_id)) + 
  geom_path(lwd = 1,
            alpha = 0.6) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  scale_color_viridis_d()
```

Finding the best performing model based on the AUC is nonsensical after applying the one standard error rule, as a worse performing model was deliberately chosen to increase simplicity of the model.

```{r}
#| code-fold: true
#| code-summary: "Show the code"

workflow_ids |>
  purrr::map(function(id) {
    # Get hyper params of best
    std_error_best <- grid_results |>
      extract_workflow_set_result(id) |>
      select_by_one_std_err(p = desc(penalty),
                            metric = "roc_auc") |> 
      mutate(wflow_id = id)
    
    # Get AUC of best
    auc <- grid_results |>
      extract_workflow_set_result(id) |>
      show_best(metric = "roc_auc",
                n = params$grid_size)
    
    # Get join keys (depend on model)
    shared_col_names <- intersect(colnames(std_error_best),
                                  colnames(auc))
    
    std_error_best |>
      left_join(auc,
                by = shared_col_names) |>
      select(wflow_id, mean)
    }) |> bind_rows() |>
  ggplot(aes(x = wflow_id,
             y = mean,
             fill = wflow_id)) +
  geom_col() +
  geom_label(aes(label = round(mean, 3)),
             position = position_stack(vjust = 0.9),
             show.legend = FALSE) +
  labs(title = "AUC",
       x = "Workflow ID",
       y = "AUC by Workflows") +
  theme(axis.text.x = element_blank(),
        axis.ticks.x = element_blank())
```

# Session Info

```{r}
sessioninfo::session_info()
```